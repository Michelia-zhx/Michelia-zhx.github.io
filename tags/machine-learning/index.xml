<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on Michelia&#39;Log</title>
    <link>https://michelia-zhx.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Michelia&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="https://michelia-zhx.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bias-Variance Decomposition</title>
      <link>https://michelia-zhx.github.io/posts/2022-03-03-bias-variance-decomposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-03-03-bias-variance-decomposition/</guid>
      <description>Bias–variance tradeoff The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: - The</description>
    </item>
    
    <item>
      <title>Paper Notes - Attention</title>
      <link>https://michelia-zhx.github.io/posts/2022-04-02-attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-04-02-attention/</guid>
      <description>1. Attention 机制 1a. 背景知识 我们最为熟悉的NMT模型便是经典的Seq2Seq, 这篇文章从一个Seq2Seq模型开始介绍, 然后进一步看如何将Attent</description>
    </item>
    
    <item>
      <title>Paper Notes - Self-Attention</title>
      <link>https://michelia-zhx.github.io/posts/2022-04-03-self_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-04-03-self_attention/</guid>
      <description>Self-Attention 机制和代码 BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, SciBERT, BioBERT, MobileBERT, TinyBERT 和 CamemBERT 的共同点是 self-attention 机制. Self-attention 机制不仅是使某种架构被称为&amp;quot;BERT&amp;quot;的原因, 更准确地, 是基于</description>
    </item>
    
  </channel>
</rss>
