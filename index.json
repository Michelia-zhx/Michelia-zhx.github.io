[{"content":"","permalink":"https://michelia-zhx.github.io/about/","summary":"about","title":"About"},{"content":"Bias–variance tradeoff The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: - The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). - The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).\nThe bias–variance decomposition is a way of analyzing a learning algorithm\u0026rsquo;s expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\nBias–variance decomposition of mean squared error 对于数据集$D = {(x_1,y_1),\u0026hellip;,(x_n,y_n)}$, 假设标签$y = f(x) + \\epsilon$, 其中$\\epsilon$为噪声, 期望为 $0$, 方差为$\\sigma^2$. 希望$\\hat{f}(x;D)$能尽量拟合$f(x)$.\n$$E_{D,\\epsilon}\\left[(y-\\hat{f}(x;D))^2\\right] = \\left(\\text{Bias}_D[\\hat{f}(x;D)])^2\\right) + \\text{Var}_D[\\hat{f}(x;D)] + \\text{intrinsic noise}$$\n其中 $$\\text{Bias}_D[\\hat{f}(x;D)] = E_D[\\hat{f}(x;D)] - f(x)$$\n$$\\text{Var}_D[\\hat{f}(x;D)] = E_D\\left[\\left(E_D(\\hat{f}(x;D)) - \\hat{f}(x;D)\\right)^2\\right]$$\nBias–variance decomposition of Kullback-Leibler divergence $$K(q, \\hat{p}) = \\int\\rm{d}yq(y)\\log\\left[\\dfrac{q(y)}{\\hat{p}(y)}\\right]$$\n$$\\text{variance} = \\min_{a: \\int\\rm{d}ya(y)=1}EK(a,\\hat{p}) = EK(\\overline{p}, \\hat{p}), \\overline{p}(y) = \\dfrac{1}{Z}\\exp[E\\log\\hat{p}(y)]$$\n$$\\text{bias} = K(q,\\overline{p}) = EK(q,\\overline{p}) + \\log Z$$\n$$\\text{error} = EK(q,\\hat{p}) = K(q,\\overline{p}) + EK(q,\\hat{p})$$\n$$-E\\log\\hat{p}(t) = -\\log\\overline{p}(t) + EK(\\overline{p}, \\hat{p})$$\n$$error = -E\\left[\\int\\rm{d}q(t)\\log\\hat{p}(t)\\right] = -\\int\\rm{d}q(t)\\log q(t) + K(q,\\overline{p}) + EK(\\overline{p}, \\hat{p})$$\nReference   Fortmann-Roe, Scott (2012). \u0026ldquo;Understanding the Bias–Variance Tradeoff\u0026rdquo;\n  Heskes T. Bias/variance decompositions for likelihood-based estimators[J]. Neural Computation, 1998, 10(6): 1425-1433.\n  ","permalink":"https://michelia-zhx.github.io/posts/2022-03-03-bias-variance-decomposition/","summary":"Bias–variance tradeoff The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: - The","title":"Bias-Variance Decomposition"},{"content":"1. 安装 Hugo 苹果用户有安装 HomeBrew 工具的话可以直接输入 brew install hugo 进行安装.\n下载完成后检查是否安装成功，输入: hugo version, 若出现版本信息则表示安装成功.\n2. 新建站点 输入 hugo new site {your site's name}, 即可生成以 {your site's name} 命名的站点文件夹. 具有 archetypes, content, data, layouts, static, themes 文件夹和 config.yml 配置文件.\n3. 下载主题 先去 Hugo 主题官网 找到自己喜欢的主题, 然后点击下载会跳转到主题的github, 把终端的路径调整到博客文件夹的themes目录下, 输入 git clone https://github.com/adityatelange/hugo-PaperMod.git. 也可以在 github 下载对应 repo 的压缩包, 解压到themes目录下.\n使用该主题的方法就是在站点配置文件输入主题的名字:\n1  theme: PaperMod # 主题名字，和themes文件夹下的一致   4. 配置文件 我的 config.yml 内容如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202  baseURL: https://michelia-zhx.github.io languageCode: zh-cn # en-us title: Michelia\u0026#39;Log theme: PaperMod # 主题名字，和themes文件夹下的一致 enableInlineShortcodes: true enableEmoji: true # 允许使用 Emoji 表情，建议 true enableRobotsTXT: true # 允许爬虫抓取到搜索引擎，建议 true hasCJKLanguage: true # 自动检测是否包含 中文日文韩文 如果文章中使用了很多中文引号的话可以开启 buildDrafts: false buildFuture: false buildExpired: false # googleAnalytics: UA-123-45 # 谷歌统计 # Copyright: Michelia paginate: 5 # 首页每页显示的文章数 minify: disableXML: true # minifyOutput: true permalinks: post: \u0026#34;/:title/\u0026#34; # post: \u0026#34;/:year/:month/:day/:title/\u0026#34; defaultContentLanguage: en # 最顶部首先展示的语言页面 defaultContentLanguageInSubdir: true languages: en: languageName: \u0026#34;English\u0026#34; # contentDir: content/english weight: 1 avatarURL: \u0026#34;images/Cheese.ico\u0026#34; profileMode: enabled: true title: 🧀 Welcome to Michelia\u0026#39;Log imageUrl: \u0026#34;images/goujuan.png\u0026#34; imageTitle: imageWidth: 130 imageHeight: 130 menu: main: - identifier: home name: 🧀Home url: / weight: 1 - identifier: posts name: 🥑Posts url: posts weight: 2 - identifier: archives name: 🍑Archives url: archives weight: 4 - identifier: tags name: 🍍Tags url: tags weight: 5 - identifier: about name: 🍉About url: about weight: 6 - identifier: search name: 🍳Search url: search weight: 7 outputs: home: - HTML - RSS - JSON params: env: production # to enable google analytics, opengraph, twitter-cards and schema. author: Michelia-zhx # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors defaultTheme: auto # defaultTheme: light or dark  disableThemeToggle: false DateFormat: \u0026#34;2006-01-02\u0026#34; ShowShareButtons: true ShowReadingTime: true # disableSpecialistPost: true displayFullLangName: true ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: true hideFooter: false # 隐藏页脚 ShowWordCounts: true VisitCount: true ShowLastMod: true #显示文章更新时间 ShowToc: true # 显示目录 TocOpen: true # 自动展开目录 comments: true socialIcons: - name: github url: \u0026#34;https://github.com/Michelia-zhx\u0026#34; - name: twitter url: \u0026#34;images/twitter.png\u0026#34; - name: facebook url: \u0026#34;https://www.facebook.com/\u0026#34; - name: instagram url: \u0026#34;images/instagram.png\u0026#34; - name: QQ url: \u0026#34;images/qq.png\u0026#34; - name: WeChat url: \u0026#34;images/wechat.png\u0026#34; - name: email url: \u0026#34;mailto: zhanghanxiao000@gmail.com\u0026#34; - name: RSS url: \u0026#34;index.xml\u0026#34; # editPost: # URL: \u0026#34;https://github.com/adityatelange/hugo-PaperMod/tree/exampleSite/content\u0026#34; # Text: \u0026#34;Suggest Changes\u0026#34; # edit text # appendFilePath: true # to append file path to Edit link # label: # text: \u0026#34;Home\u0026#34; # icon: icon.png # iconHeight: 35 # analytics: # google: # SiteVerificationTag: \u0026#34;XYZabc\u0026#34; assets: favicon: \u0026#34;images/Cheese.ico\u0026#34; favicon16x16: \u0026#34;images/Cheese.ico\u0026#34; favicon32x32: \u0026#34;images/Cheese.ico\u0026#34; apple_touch_icon: \u0026#34;images/Cheese.ico\u0026#34; safari_pinned_tab: \u0026#34;images/Cheese.ico\u0026#34; # cover: # hidden: true # hide everywhere but not in structured data # hiddenInList: true # hide on list pages and home # hiddenInSingle: true # hide on single page fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 1 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;] twikoo: version: 1.4.11 taxonomies: category: categories tag: tags series: series markup: goldmark: renderer: unsafe: true # HUGO 默认转义 Markdown 文件中的 HTML 代码，如需开启的话 highlight: codeFences: true guessSyntax: false hl_Lines: \u0026#34;\u0026#34; lineNoStart: 1 lineNos: true lineNumbersInTable: true noClasses: true style: monokai tabWidth: 4 privacy: vimeo: disabled: false simple: true twitter: disabled: false enableDNT: true simple: true instagram: disabled: false simple: true youtube: disabled: false privacyEnhanced: true services: instagram: disableInlineCSS: true twitter: disableInlineCSS: true   5. 启动博客 在站点目录下输入 hugo server -D 就可以在本地预览了, 本地预览网址为 localhost:1313, 输入 hugo 就可以生成 public 文件夹，这个文件夹可以部署到云服务器或者托管到 github 上.\n注意: 输入 hugo 的生成方式只会往 public 文件夹里添加内容, 但是不会删除外部已经不存在而 public 里面还存在的文件.\n6. 写博客 输入 hugo new 文章名称.md 就会在 content 目录下的到 \u0026ldquo;文章名称.md\u0026rdquo; 形式的文件, 所有文章都是放在 content 这个文件夹里.\n文章内部的头部配置信息:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  --- title: Hugo 博客搭建 | PaperMod 主题 author: \u0026#34;Michelia-zhx\u0026#34; # 文章作者 date: 2022-03-01 15:20 # 文章编写日期 lastmod: 2022-03-01 15:20 # 文章修改日期 tags: [ # 文章所属标签 \u0026#34;Hugo\u0026#34;, \u0026#34;Blog\u0026#34; ] keywords: [ # 文章关键词 \u0026#34;Hugo\u0026#34;, \u0026#34;static\u0026#34;, \u0026#34;generator\u0026#34;, ] ---   这里有一些功能每个主题不一定有, 需要根据具体情况做一些适配, 仅供参考.\n7. 数学公式 在 ./themes/PaperMod/layouts/partials/footer.html末尾添加以下内容 (因为每个页面都有 footer 部分):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  \u0026lt;script type=\u0026#34;text/javascript\u0026#34; async src=\u0026#34;https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0026#34;\u0026gt; MathJax.Hub.Config({ tex2jax: { inlineMath: [[\u0026#39;$\u0026#39;,\u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;,\u0026#39;\\\\)\u0026#39;]], displayMath: [[\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026#39;\\[\\[\u0026#39;,\u0026#39;\\]\\]\u0026#39;]], processEscapes: true, processEnvironments: true, skipTags: [\u0026#39;script\u0026#39;, \u0026#39;noscript\u0026#39;, \u0026#39;style\u0026#39;, \u0026#39;textarea\u0026#39;, \u0026#39;pre\u0026#39;], TeX: { equationNumbers: { autoNumber: \u0026#34;AMS\u0026#34; }, extensions: [\u0026#34;AMSmath.js\u0026#34;, \u0026#34;AMSsymbols.js\u0026#34;] } } }); MathJax.Hub.Queue(function() { // Fix \u0026lt;code\u0026gt; tags after MathJax finishes running. This is a  // hack to overcome a shortcoming of Markdown. Discussion at  // https://github.com/mojombo/jekyll/issues/199  var all = MathJax.Hub.getAllJax(), i; for(i = 0; i \u0026lt; all.length; i += 1) { all[i].SourceElement().parentNode.className += \u0026#39; has-jax\u0026#39;; } }); \u0026lt;/script\u0026gt; \u0026lt;style\u0026gt; code.has-jax { font: inherit; font-size: 100%; background: inherit; border: inherit; color: #515151; } \u0026lt;/style\u0026gt;   8. 插入图片 举例: 将图片放在 ./static/images/multi-teacher_kd/1.png, 文章内使用 \u0026lt;img src=\u0026quot;/images/multi-teacher_kd/1.png\u0026quot; width = \u0026quot;500\u0026quot; div align=center /\u0026gt;, 建站时 public 文件夹内 images 文件夹下会生成 multi-teacher_kd文件夹, 存放 1.png, 对应的 html 文件内也会通过 \u0026lt;img src=\u0026quot;/images/multi-teacher_kd/11.png\u0026quot; width = \u0026quot;500\u0026quot; div align=center /\u0026gt;\u0026lt;/li\u0026gt;插入图片.\n","permalink":"https://michelia-zhx.github.io/posts/2022-03-01-%E5%BB%BA%E7%AB%99/","summary":"1. 安装 Hugo 苹果用户有安装 HomeBrew 工具的话可以直接输入 brew install hugo 进行安装. 下载完成后检查是否安装成功，输入: hugo version, 若出现版本信息则表示安装成功. 2. 新建站点 输","title":"Hugo 博客搭建 | PaperMod 主题"},{"content":"Chap 1 计算机网络和因特网 1.1 什么是因特网 1.1.1 因特网的具体构成描述  各种各样的设备都被连接到因特网,这些设备都被称为主机或端系统. 端系统通过通信链路和分组交换机连接到一起. 分组: 一台端系统向另一台端系统发送数据时, 发送端系统将数据分组, 并为每段加上首部字节 分组交换机: 从入通信链路接受分组, 从出通信链路转发分组. 路由器和链路层交换机 一个分组从发送端到接收端所经历的一系列通信链路和分组交换机称为通过该网络的路径 (route 或 path) 端系统通过因特网服务提供商 (Internet Servise Provider, ISP) 接入因特网 端系统, 分组交换机和其他因特网部件, 都要运行控制因特网中信息接收和发送的一系列协议. TCP (Transmission Control Protocol, 传输控制协议), IP (Internet Protocol, 网际协议). IP 协议定义了在路由器和端系统中发送和接受的分组的格式. 因特网的主要协议统称为 TCP/IP.  1.1.2 服务描述  因特网也可以被描述为: 为应用程序提供服务的基础设施. 与因特网相连的端系统提供了一个应用程序编程接口 (Application Programming Interface, API), 规定了运行在一个端系统上的软件请求因特网基础设施向运行在另一个端系统上的特定目的的软件交付数据的方式. 一个协议定义了在两个或多个通信实体之间交换的报文格式和次序, 以及在报文传输和/或接受或其他事件方面做采取的动作.  1.2 网络边缘 1.2.1 客户机和服务器程序  客户器程序是运行在一个端系统上的程序, 它发出请求, 并从运行在另一个端系统上的服务器程序接受服务.  客户机-服务器因特网应用程序是分布式应用程序   P2P 应用程序: 其中的端系统互相作用并执行客户机和服务器功能的程序.  1.2.2 接入网  接入网: 将端系统连接到其边缘路由器的物理链路 (边缘路由器是端系统到任何其他远程端系统的路径上的第一台路由器)  网络接入大致分为三种类型: 住宅接入, 公司接入, 无线接入.   住宅接入: 将家庭端系统与边缘路由器相连接  通过普通模拟电话线用拨号调制解调器与住宅 ISP 相连, 家用调制解调器将 PC 输出的数字信号转换为模拟形式, 以便在模拟电话线上传输 宽带住宅区接入有两种常见类型: 数字用户线 (digital subscriber line, DSL) 和混合光纤同轴电缆 (hybrid fiber-coaxial cable, HFC)   公司接入: 局域网 (LAN) 通常被用于连接端用户与边缘路由器 无线接入: 在无线局域网中, 无线用户与位于几十米内的基站之间传输/接收分组. 在广域无线接入网中, 分组经用于蜂窝电话的相同无线基础设施进行发送.  1.3 网络核心  通过网络链路和交换机移动数据有两种基本方法: 电路交换和分组交换  电路交换: 沿着系统通信路径, 为端系统之间通信所提供的资源在通信会话期间会被预留 \u0026ndash; 电话网络 分组交换: 这些资源不会被预留, 会话的报文按需使用这些资源, 导致可能不得不等待 (排队) \u0026ndash; 因特网 作者用去饭店吃饭的例子解释了以上两条: 需要预约的饭店, 和不需要预约但是不一定有位置的饭店.   电路交换和其中的多路复用  频分多路复用 (FDM) 时分多路复用 (TDM)   分组交换:  报文 (message): 包含协议设计者需要的全部信息 分组: 源主机将长报文划分为较小的数据块 分组交换机: 交换机主要有路由器和链路层交换机两类 存储转发传输机制: 在交换机能够开始向输出链路传输该分组的第一个比特之前, 必须接收到整个分组 \u0026ndash;\u0026gt; 存储转发时延. 输出缓存 (输出队列): 如果到达的分组需要跨越链路传输, 但发现该链路正在忙于传输其他分组 \u0026ndash;\u0026gt; 排队时延; 到达的分组发现该缓存被等待传输的分组充满了 \u0026ndash;\u0026gt; 分组丢失/丢包 按需共享资源有时被称为资源的统计多路复用 分组到达路由器后, 每个路由器有一个转发表, 用于将分组头部的目的地址映射到输出链路.   ISP 和因特网主干  第一层 ISP:  直接与其他每一个第一层 ISP 相连 与大量的第二层 ISP 和其他客户网络相连 \u0026ndash; 客户, 提供商 覆盖国际区域   因特网由几十个第一层 ISP 和第二层 ISP 与数以千计的较低层 ISP 组成. ISP 覆盖的区域不同, 较低层的 ISP 与较高层的 ISP 相连, 较高层 ISP 彼此互联. 用户和内容提供商是较低层 ISP 的客户, 较低层的 ISP 是较高层的 ISP 的客户.    1.4 分组交换中的时延、丢包和吞吐量  时延的类型  处理时延: 检查分组首部以决定去向, 检查比特级差错 排队时延: 分组在链路上等待传输 传输时延: $L$表示分组的长度, $R\\ bps$表示从路由器 A 到路由器 B 的链路传输速率. $L/R$是将分组的所有比特推向链路所需要的时间 传播时延: 该链路的起点到路由器 B 传播所需要的时间 $t_{nodal} = t_{proc} + t_{queue} + t_{trans} + t_{prop}$   排队时延和丢包  流量强度: $a$表示分组到达队列的平均速率, 则比特到达队列的平均速率为$La$, $\\dfrac{La}{R}$定义为流量强度. (设计系统时流量强度不能大于 1). 流量强度接近于 0 时, 平均排队时延将接近于 0; 流量强度接近于 1 时, 平均排队时延将迅速增加; $t_{queue} = \\dfrac{I}{1-I}\\dfrac{L}{R}$, $I=\\dfrac{La}{R}$表示流量强度, $\\dfrac{L}{R}$是一个分组的传输时间   吞吐量: 瞬时吞吐量 (bps) 和平均吞吐量.  对于简单的两链路网络, 假设$R_S$为服务器和路由器之间的链路速率, $R_C$为路由器和和客户机之间的链路速率. 吞吐量是$\\min{R_S, R_C}$, 即瓶颈链路的传输速率.    1.5 协议层次和它们的服务类型  分层的体系结构: 某层对其上面的层提供相同的服务, 并且使用来自下面层次的服务 (服务类型). 当某层的实现变化时, 该系统的其余部分可以保持不变. 层 n 的协议通常分布在构成网络的端系统, 分组交换机和其他组件中. 各层的所有协议被称为协议栈: 物理层, 链路层, 网络层, 运输层, 应用层.  应用层: HTTP, SMTP, FTP\u0026hellip; 协议分布在端系统上, 一个端系统中的应用程序使用协议与另一个端系统中的应用程序交换信息分组 (报文). 运输层: TCP, UDP. 运输层分组称为报文段. 网络层: 将称为数据报的网络层分组从一台主机移动到另一台主机. 链路层: 通过一系列路由器在源和目的地之间发送分组. 链路层分组称为帧. 物理层: 将整个帧从一个网络元素移动到邻近的网络元素. 在每一层, 分组具有两种类型的字段: 首部字段和有效载荷字段, 有效载荷通常来自上一分组.    ","permalink":"https://michelia-zhx.github.io/posts/2022-02-26-computer_network_chap1/","summary":"Chap 1 计算机网络和因特网 1.1 什么是因特网 1.1.1 因特网的具体构成描述 各种各样的设备都被连接到因特网,这些设备都被称为主机或端系统. 端系统通过通信链路和","title":"Notes - Computer Networking -- Chap 1"},{"content":"Chap 2 应用层 2.1 应用层原理  网络应用的体系结构:  客户-服务器 (C/S) 模式 (可扩展性差, 性能随用户数量增加会有断崖式下降) 对等体 (P2P) 体系结构 混合体   进程通信: 同一主机内, 使用进程间通信机制通信 (操作系统定义); 不同主机, 通过交换报文来通信  对进程进行编址 addressing:  进程为了接受报文, 必须有一个标识 SAP: IP 地址, 所采用的传输层协议是 TCP 还是 UDP, TCP/UDP 的端口号; 本质上, 一对主机的进程之间的通信由 2 个端节点构成.     传输层提供的服务 - 层间信息的代表: 如果 socket api 每次传输报文都要携带双方地址和内容, 会过于繁琐.  用个代号标示通信的双方或单方: socket; 就想 OS 打开文件返回的句柄一样 \u0026ndash; 对句柄的操作就是对文件的操作; TCP socket: 相当于本地 IP 本地 TCP 端口, 对方 IP 对方 TCP 端口的本地标识 (用于指明应用进程会话的本地标识); UDP socket: 两个进程之间的通信需要之前无需建立连接 (每个报文都是独立传输的). 用一个整数表示本应用实体的标示: 本 IP, 本端口. 但是传输报文时, 必须提供对方 IP, port; 接受报文时, 传输层需要上传对方的 IP, port.   应用层需要传输层提供的服务? 如何描述传输层的服务?  数据丢失率 延迟 吞吐 安全性   TCP 服务  可靠的传输服务 流量控制: 发送方不会淹没接受方 拥塞控制: 当网络出现拥塞时, 能抑制发送方 不能提供的服务: 时间保证、最小吞吐保证和安全 面向连接: 要求在客户端进程和服务器进程之间建立连接   UDP 服务:  不可靠的数据传输 不提供的服务: 可靠, 流量控制, 拥塞控制, 时间带宽保证, 建立连接 存在的必要性:  能够区分不同进程 无需建立连接 不做可靠性的工作 没有流量控制, 拥塞控制, 应用能够按照设定的速度发送数据      2.2 Web and HTTP  Web 页: 由一些对象组成. 含有一个基本的 html 文件, 其中包含若干对象的引用 (以 url 的形式表示: 协议名+用户:口令+主机名+路径名+端口) HTTP: 超文本传输协议  Web 的应用层协议 客户/服务器模式 使用 TCP  客户发起一个与服务器的 TCP 连接, 端口号为 80 服务器接受客户的 TCP 连接 (服务器有 waiting socket 和 connection socket, 每建立一个连接, 弹出一个 connection socket, 同时 waiting socket 始终存在) 在浏览器与 Web 服务器交换 HTTP 报文 TCP 连接关闭   无状态服务器: 服务器不需要维护客户端的状态 建立连接需要一次往返 (建立连接请求 + 建立连接确认), http 请求对象, 返回对象 (响应报文)    非持久 (最多只有一个对象在 TCP 连接上发送, 下载多个对象需要多个 TCP 连接 \u0026ndash; HTTP 1.0) 和持久 (多个对象在一个 TCP 连接上传输 \u0026ndash; HTTP 1.1) HTTP  非流水方式的持久 HTTP: 客户端只有在收到前一个响应后才能产生一个请求, 每个引用对象花费一个 RTT 流水方式的持久 HTTP: 客户端遇到一个引用对象就立即产生一个请求, 所有引用 (小) 对象只花费一个 RTT 是可能的   HTTP 请求报文  两种类型: 请求, 响应 ASCII: 请求行 (GET, POST, HEAD 命令), 首部行, 换行回车符 (表示报文结束) 提交表单输入  Post 方式 URL 方式     HTTP 响应报文  状态行 (协议版本, 状态码和相应状态信息), 首部行, 数据 (如请求的 HTML 文件)   用户 - 服务器的状态: cookies  组成部分:  在 HTTP 响应报文中有一个 cookie 的首部行 在 HTTP 请求报文含有一个 cookie 的首部行 在用户端系统中保留有一个 cookie 文件, 由用户的浏览器管理 在 Web 站点有一个后端数据库      Web 缓存 (代理服务器 proxy server) \u0026ndash; 不访问原始服务器, 就满足客户需求  用户设置浏览器: 通过缓存访问 Web 浏览器将所有的 HTTP 请求发给缓存 缓存既是客户端又是服务器 用很小的缓存就可以解决大多数需求 ( 28 分布, 访问的趋同性) 安装本地缓存效果远好于扩宽链路 条件 GET 方法: 如果缓存中的对象拷贝是最新的, 就不要发送对象    2.3 FTP: 文件传输协议  向远程主机上传文件或从远程主机接收文件 客户/服务器模式  客户端: 发起传输的一方 服务器: 远程主机   控制连接与数据连接分开  FTP客户端与FTP服务器服务器通过端口21联系, 并使用TCP为传输协议 客户端通过控制连接获得身份确认 客户端通过控制连接发送命令浏览远程目录 收到一个文件传输命令时, 服务器打开一个到客户端的数据连接 一个文件传输完成后, 服务器关闭连接   特点  服务器打开第二个 TCP 数据连接用来传输另一个文件 控制连接: 带外 (\u0026ldquo;out of band\u0026rdquo;) 传送 FTP服务器维护用户的状态信息: 当前路径, 用户帐户与控制连接对应 \u0026ndash; 有状态    2.4 Email  三个组成部分:  用户代理 (\u0026ldquo;邮件阅读器\u0026rdquo;, 阅读, 编辑, Foxmail, Outlook) 邮件服务器 简单邮件传输协议: SMTP    SMTP [RFC 2821]  使用TCP在客户端和服务器之间传送报文, 端口号为25 直接传输: 从发送方服务器到接收方服务器 传输的3个阶段  握手 传输报文 关闭   命令/响应交互  命令: ASCII文本 响应: 状态码和状态信息   报文必须为7位ASCII码    2.5 DNS (Domain Name System)  IP 地址标识主机, 路由器. DNS 负责域名 - IP 地址的转换. DNS的主要思想:  分层的、基于域的命名机制 若干分布式的数据库完成名字到IP地址的转换 运行在UDP之上端口号为53的应用服务 核心的 Internet 功能, 但以应用层协议实现  在网络边缘处理复杂性     DNS主要目的:  实现主机名-IP地址的转换(name/IP translate) 其它目的:  主机别名到规范名字的转换: Host aliasing 邮件服务器别名到邮件服务器的正规名字的转换: Mail server aliasing 负载均衡: Load Distribution     DNS 域名结构  一个层面命名设备会有很多重名 DNS 采用层次树状结构的命名方法 Internet 根被划分为几百个顶级域  通用的 (generic): .com, .edu, .gov, .int, .mil, .net, .org, .firm, .hsop, .web, .arts, .rec; 国家的 (contries): .cn, .us, .nl, .jp;   每个(子)域下面可划分为若干子域 (subdomains) 树叶是主机   区域 (zone)  区域的划分有区域管理者自己决定 将DNS名字空间划分为互不相交的区域, 每个区域都是树的一部分 名字服务器  每个区域都有一个名字服务器: 维护着它所管辖区域的权威信息 (authoritative record) 名字服务器允许被放置在区域之外, 以保障可靠性     顶级域 (TLD) 服务器: 负责顶级域名: 如com, org, net, edu和gov; 和所有国家级的顶级域名, 如cn, uk, fr, ca, jp) 区域名字服务器维护资源记录  资源记录 (resource records)  作用: 维护域名 - IP地址 (其它) 的映射关系 位置: Name Server 的分布式数据库中   RR格式: (domain_name, ttl, type, class, Value)  Domain_name: 域名 Ttl - time to live : 生存时间 (权威, 缓冲记录) Class 类别: 对于Internet, 值为IN Value 值: 可以是数字 域名或ASCII串 Type 类别: 资源记录的类型      DNS大致工作流程  应用调用解析器 (resolver) 解析器作为客户向 Name Server 发出查询报文 (封装在UDP段中) Name Server 回响应报文(name/ip)    本地名字服务器 (Local Name Server)  并不严格属于层次结构 每个ISP (居民区的ISP、公司、大学, 都有一个本地DNS服务器)  也称为“本地名字服务器”   当一个主机发起一个DNS查询时, 查询发送到其本地DNS服务器  起着代理的作用, 将查询转发到层次结构中     两种查询:   2.6 P2P 应用  没有(极少)一直运行的服务器; 任意端系统直接通信; 利用 peer 的服务能力; peer 节点间歇上网, 每次 IP 地址都有可能变化. 服务器上载 N 份文件, N 个客户端下载的时间下限  C/S 模式下, $\\max{\\dfrac{F}{d_{min}}, N\\dfrac{F}{u_s}}$. P2P 模式下, $\\max{\\dfrac{F}{d_{min}}, N\\dfrac{F}{u_s+\\sum Nu_c}}$.   两大问题: 如何定位所需资源; 如何处理用户的加入与离开. overlay 覆盖网 (应用层逻辑网络) 非结构化 P2P  集中化目录 (有一个节点维护全局目录信息) (单点故障, 性能瓶颈, 版权侵犯) 完全分布式 (没有节点维护全局目录信息) 洪泛 (flooding) 式查询  混合体 (组长维护局部目录信息)    DHTC (Distributed Hash Table) (结构化) P2P  Hash DHT方案 环形DHT 以及覆盖网络 Peer波动    2.7 CDN   多媒体: 视频\n 视频：固定速度显示的图像序列 网络视频特点：  高码率: \u0026gt;10x于音频,高的网络带宽需求 可以被压缩 90%以上的网络流量是视频   数字化图像：像素的阵列  每个像素被若干bits表示   编码：使用图像内和图像间的冗余来降低编码的比特数  空间冗余(图像内)  空间编码例子: 不是发送 N 个相同的颜色 (全部是紫色) 值, 仅仅发送2各值: 颜色 (紫色) 和重复的个数 (N)   时间冗余 (相邻的图像间)  时间编码例子: 不是发送第 i+1 帧的全部编码, 而仅仅发送和帧 i 差别的地方     CBR: (constant bit rate): 以固定速率编码 VBR: (variable bit rate): 视频编码速率随时间的变化而变化    多媒体流化服务: DASH (Dynamic, Adaptive Streaming over HTTP)\n 服务器:  将视频文件分割成多个块 每个块独立存储, 编码于不同码率 (8-10种) 告示文件 (manifest file): 提供不同块的URL   客户端:  先获取告示文件 周期性地测量服务器到客户端的带宽 查询告示文件,在一个时刻请求一个块, HTTP头部指定字节范围  如果带宽足够, 选择最大码率的视频块 会话中的不同时刻, 可以切换请求不同的编码块 (取决于当时的可用带宽)        CDNs (Content Distribution Networks)\n 挑战: 服务器如何通过网络向上百万用户同时流化视频内容 (上百万视频内容)? 选择1: 单个的、大的超级服务中心“megaserver”  服务器到客户端路径上跳数较多, 瓶颈链路的带宽小导致停顿 “二八规律”决定了网络同时充斥着同一个视频的多个拷贝, 效率低 (付费高、带宽浪费、效果差) 单点故障点, 性能瓶颈 周边网络的拥塞   选项2: 通过CDN, 全网部署缓存节点, 存储服务内容, 就近为用户提供服务, 提高用户体验  enter deep: 将CDN服务器深入到许多接入网  更接近用户，数量多，离用户近，管理困难 Akamai, 1700个位置   bring home: 部署在少数 (10个左右) 关键位置，如将服务器簇安装于POP附近 (离若干1stISP POP较近)  采用租用线路将服务器簇连接起来 Limelight         NetFlix 案例   2.8 套接字编程 2种传输层服务的socket类型:\n TCP: 可靠的, 字节流的服务 UDP: 不可靠 (数据UDP数据报) 服务  TCP 套接字编程 服务器首先运行，等待连接建立:\n 服务器进程必须先处于运行状态  创建欢迎socket 和本地端口捆绑 在欢迎socket上阻塞式等待接收用户的连接    客户端主动和服务器建立连接:\n创建客户端本地套接字 (隐式捆绑到本地port)  指定服务器进程的IP地址和端口号, 与服务器进程连接   当与客户端连接请求到来时  服务器接受来自用户端的请求, 解除阻塞式等待, 返回一个新的socket (与欢迎socket不一样), 与客户端通信 允许服务器与多个客户端通信 使用源IP和源端口来区分不同的客户端   连接 API 调用有效时，客户端 IP 与服务器建立了 TCP 连接  1 2 3 4 5 6 7 8 9 10  数据结构 sockaddr_in // IP地址和port捆绑关系的数据结构（标示进程的端节点）  struct sockaddr_in{ short sin_family; // 地址簇  u_short sin_port; // port  struct in_addr, sin_addr; // IP address, unsigned long  char sin_zero; // 对齐 };   1 2 3 4 5 6 7 8 9 10 11 12 13  数据结构 host_ent // 域名和 IP 地址的数据结构 struct host_ent{ char *h_name; // 域名  char **h_aliases; // 域名别名  int h_addrtypes; int h_length; // 地址长度  char **h_addr_list; // IP 地址  # define h_addr h_addr_list[0]; }; // 作为调用域名解析函数时的参数 // 返回后，将IP地址拷贝到sockaddr_in的IP地址部分   例子: C 客户端 (TCP)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  /* client.c */ void main(int argc, char *argv[]) { struct sockaddr_in sad; /* structure to hold an IP address of server */ int clientSocket; /* socket descriptor */ struct hostent *ptrh; /* pointer to a host table entry */ char Sentence[128]; char modifiedSentence[128]; host = argv[1]; port = atoi(argv[2]); /* Create client socket, connect to server */ clientSocket = socket(PF_INET, SOCK_STREAM, 0); memset((char *)\u0026amp;sad,0,sizeof(sad)); /* clear sockaddr structure */ sad.sin_family = AF_INET; /* set family to Internet */ sad.sin_port = htons((u_short)port); ptrh = gethostbyname(host); /* Convert host name to IP address */ memcpy(\u0026amp;sad.sin_addr, ptrh-\u0026gt;h_addr, ptrh-\u0026gt;h_length); //将IP地址拷贝到sad.sin_addr  connect(clientSocket, (struct sockaddr *)\u0026amp;sad, sizeof(sad)); /* Get input stream from user */ gets(Sentence); /* Send line to server */ n=write(clientSocket, Sentence, strlen(Sentence)+1); /* Read line from server */ n=read(clientSocket, modifiedSentence, sizeof(modifiedSentence)); printf(\u0026#34;FROM SERVER: %s\\n\u0026#34;, modifiedSentence); /* Close connection */ close(clientSocket); }   例子: C服务器 (TCP)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  /* server.c */ void main(int argc, char *argv[]){ struct sockaddr_in sad; /* structure to hold an IP address of server*/ struct sockaddr_in cad; /*client */ int welcomeSocket, connectionSocket; /* socket descriptor */ struct hostent *ptrh; /* pointer to a host table entry */ char clientSentence[128]; char capitalizedSentence[128]; port = atoi(argv[1]); /* Create welcoming socket at port \u0026amp; Bind a local address */ welcomeSocket = socket(PF_INET, SOCK_STREAM, 0); memset((char *)\u0026amp;sad,0,sizeof(sad)); /* clear sockaddr structure */ sad.sin_family = AF_INET; /* set family to Internet */ sad.sin_addr.s_addr = INADDR_ANY; /* set the local IP address */ sad.sin_port = htons((u_short)port); /* set the port number */ bind(welcomeSocket, (struct sockaddr *)\u0026amp;sad, sizeof(sad)); /* Specify the maximum number of clients that can be queued */ listen(welcomeSocket, 10) while(1) { /* Wait, on welcoming socket for contact by a client */ connectionSocket=accept(welcomeSocket, (struct sockaddr *)\u0026amp;cad, \u0026amp;alen); n=read(connectionSocket, clientSentence, sizeof(clientSentence)); /* capitalize Sentence and store the result in capitalizedSentence*/ /* Write out the result to socket */ n=write(connectionSocket, capitalizedSentence, strlen(capitalizedSentence)+1); /* End of while loop, loop back and wait for another client connection */ close(connectionSocket); } }   UDP 套接字编程 UDP: 在客户端和服务器之间没有连接\n 没有握手 发送端在每一个报文中明确地指定目标的IP地址和端口号 服务器必须从收到的分组中提取出发送端的IP地址和端口号  UDP: 传送的数据可能乱序, 也可能丢失\nClient/server socket 交互: UDP 样例: C客户端 (UDP)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  /* client.c */ void main(int argc, char *argv[]){ struct sockaddr_in sad; /* structure to hold an IP address */ int clientSocket; /* socket descriptor */ struct hostent *ptrh; /* pointer to a host table entry */ char Sentence[128]; char modifiedSentence[128]; host = argv[1]; port = atoi(argv[2]); /* 创建客户端socket, 没有连接到服务器 */ clientSocket = socket(PF_INET, SOCK_DGRAM, 0); /* determine the server\u0026#39;s address */ memset((char *)\u0026amp;sad, 0, sizeof(sad)); /* clear sockaddr structure */ sad.sin_family = AF_INET; /* set family to Internet */ sad.sin_port = htons((u_short)port); ptrh = gethostbyname(host); /* Convert host name to IP address */ memcpy(\u0026amp;sad.sin_addr, ptrh-\u0026gt;h_addr, ptrh-\u0026gt;h_length); gets(Sentence); addr_len =sizeof(struct sockaddr); n = sendto(clientSocket, Sentence, strlen(Sentence)+1, (struct sockaddr *) \u0026amp;sad, addr_len); n = recvfrom(clientSocket, modifiedSentence, sizeof(modifiedSentence), (struct sockaddr *) \u0026amp;sad, \u0026amp;addr_len); printf(\u0026#34;FROM SERVER: %s\\n\u0026#34;, modifiedSentence); close(clientSocket); }   样例: C服务器(UDP)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  /* server.c */ void main(int argc, char *argv[]){ struct sockaddr_in sad; /* structure to hold an IP address */ struct sockaddr_in cad; int serverSocket; /* socket descriptor */ struct hostent *ptrh; /* pointer to a host table entry */ char clientSentence[128]; char capitalizedSentence[128]; port = atoi(argv[1]); /* Create welcoming socket at port \u0026amp; Bind a local address */ serverSocket = socket(PF_INET, SOCK_DGRAM, 0); memset((char *)\u0026amp;sad,0,sizeof(sad)); /* clear sockaddr structure */ sad.sin_family = AF_INET; /* set family to Internet */ sad.sin_addr.s_addr = INADDR_ANY; /* set the local IP address */ sad.sin_port = htons((u_short)port);/* set the port number */ bind(serverSocket, (struct sockaddr *)\u0026amp;sad, sizeof(sad)); while(1) { n = recvfrom(serverSocket, clientSentence, sizeof(clientSentence), 0, (struct sockaddr *) \u0026amp;cad, \u0026amp;addr_len ); /* capitalize Sentence and store the result in capitalizedSentence*/ n = sendto(serverSocket , capitalizedSentence, strlen(capitalizedSentence)+1, (struct sockaddr *) \u0026amp;cad, \u0026amp;addr_len); } }   2.10 小结   应用程序体系结构\n 客户-服务器 P2P 混合    应用程序需要的服务品质描述:\n 可靠性、带宽、延时、安全    Internet传输层服务模式\n 可靠的、面向连接的服务: TCP 不可靠的数据报: UDP 流行的应用层协议: HTTP FTP SMTP, POP, IMAP DNS    Socket编程\n  应用层协议报文类型：请求/响应报文：\n 客户端请求信息或服务 服务器以数据、状态码进行响应    报文格式：\n 首部：关于数据信息的字段 数据：被交换的信息    控制报文vs. 数据报文\n 带内、带外    集中式 vs. 分散式\n  无状态 vs. 维护状态\n  可靠的 vs. 不可靠的报文传输\n  在网络边缘处理复杂性\n  ","permalink":"https://michelia-zhx.github.io/posts/2022-03-08-computer_network_chep2/","summary":"Chap 2 应用层 2.1 应用层原理 网络应用的体系结构: 客户-服务器 (C/S) 模式 (可扩展性差, 性能随用户数量增加会有断崖式下降) 对等体 (P2P) 体系结构 混合体 进程通信:","title":"Notes - Computer Networking -- Chap 2"},{"content":"Chap 3 传输层 3.1 概述和传输层服务 3.2 多路复用与解复用 3.3 无连接传输: UDP 3.4 可靠数据传输的原理 3.5 面向连接的传输: TCP 3.6 拥塞控制原理 3.7 TCP 拥塞机制 ","permalink":"https://michelia-zhx.github.io/posts/2022-03-28-computer_network_chep3/","summary":"Chap 3 传输层 3.1 概述和传输层服务 3.2 多路复用与解复用 3.3 无连接传输: UDP 3.4 可靠数据传输的原理 3.5 面向连接的传输: TCP 3.6 拥塞控制原理 3.7 TCP 拥塞机制","title":"Notes - Computer Networking -- Chap 3"},{"content":"Active Learning 也称为查询学习或者最优实验设计. 主动学习通过设计合理的查询函数, 不断从未标注的数据中挑选出数据标注后放入训练集. 有效的主动学习数据选择策略可以有效地降低训练的代价并同时提高模型的识别能力.\n一. 主动学习的场景   Membership Query Sythesis 生成一个询问, 并请求这个样本的标签, 这个样本可能是未标注数据中的任意一个, 甚至是从头生成的, 反正这些数据一般不是简单的服从一个自然分布的随机.\n  Stream-Based (Sequential) Selective Sampling 基于流的有选择性地选择目标, 这种通常是假设会有大量廉价的无标签数据. 它将采样一个无标签的数据, 然后决定是要询问它的标签还是忽略它 (如果数据是均匀分布, 那么和上一种情况一样), 通常基于下面的两个度量:\n 更大的信息量: 选择这些具有高信息量的数据 不确定性原则: 选择落在这种不确定域之中的数据    Pool Based Sampling 基于池的采样, 假设能够一次性获得大量未标注的数据, 并进行同时处理. 可以对数据池中的数据进行信息量排序, 直接采最有信息量的数据进行分析.\n  基于流的场景下, 数据是顺序来到的, 不会有全局的视野, 而基于池的则是更加常见的做法可以一次性对所有数据的信息量进行分析. 但是有的时候因为数据的生成情况或者计算带宽内存等的限制, 人们还是不得已还是要使用基于流的场景.\n二. Query Strategy Frameworks 用$x^*_A$表示某种采样算法$A$下最有信息量的样本.\nUncertainty Sampling  用熵作度量: $x^*_{ENT} = \\rm{argmax}_x -\\sum_i P(y_i|x;\\theta)\\log P(y_i|x;\\theta)$ 用置信度作度量: $x^_{LC} = \\rm{argmin}_x P(y^|x; \\theta)$, $y^* = \\rm{argmin}y P{\\theta}(y|x)$, 选取最大置信度最小的样本 用最小间隔作度量: $x^_{SM} = \\rm{argmax}x P{\\theta}(y^1|x) - P{\\theta}(y^_2|x), y^_1, y^*_2$ 为可能性最高的两个样本.  Query-By-Committee (QBC)  选择一定数量的模型构成委员会 $\\mathcal{C}={\\theta^{(1)},\u0026hellip;,\\theta^{(C)}}$, 对未标注的数据进行处理, 挑选出所有未标记数据中各个模型意见最不一致的样本. 不一致的度量:  用投票熵作度量: $$x^*_{VE} = \\rm{argmax}_x-\\sum_i\\dfrac{V(y_i)}{C}\\log\\dfrac{V(y_i)}{C}$$ 用KL散度(衡量两个分布的差异)作度量: $$x^*{KL} = \\argmax_x\\dfrac{1}{C}\\sum{c=1}^CD(P_{\\theta^{(c)}}|P_{\\mathcal{C}})$$, 其中 $$D(P_{\\theta^{(c)}}|P_{\\mathcal{C}}) = \\sum_iP(y_i|x;\\theta^{(c)})\\log\\dfrac{P(y_i|x;\\theta^{(c)})}{P(y_i|x;\\mathcal{C})}$$    Expected Model Change  去采的样本应当具备条件: 当它被赋予标记, 应当最大程度优化模型 —— 用训练梯度作为这种优化的衡量. $$x^*_{EGL} = \\rm{argmax}_x\\sum_iP(y_i|x;\\theta)|\\nabla \\mathcal{l}(\\mathcal{L}\\cup\\langle x, y_i\\rangle;\\theta)|$$  Variance Reduction and Fisher Information Ratio   最小化学习器的未来误差 (偏置-方差分解): $$E_T[(o-y)^2|x] = E[(y-E[y|x])^2] + (E_{\\mathcal{L}}[o] - E[y|x])^2 + E_{\\mathcal{L}}[(o-E_{\\mathcal{L}}[o])^2]$$\n  右边三项分别为噪音、偏置的平方、方差. 只适用于回归任务.\n  离散分类器使用Fisher Information: $$\\mathcal{I}(\\theta) = -\\int_xP(x)\\int_yP(y|x;\\theta)\\dfrac{\\partial^2}{\\partial\\theta^2}\\log P(y|x;\\theta)$$\n  最佳检索样本应最小化Fisher Information Ratio: $$x^*_{FIR} = \\argmin_xtr(\\mathcal{I}x(\\theta)^{-1}\\mathcal{I}{\\mathcal{U}}(\\theta))$$\n  $\\mathcal{I}x(\\theta)$不仅说明模型对样本$x$的不确定性有多大, 而且说明了是哪个参数造成了这种不确定性. $\\mathcal{I}{\\mathcal{U}}(\\theta)$表明了在整个数据集上的不确定性.\n  Estimated Error Reduction Density-Weight Methods 三. 主动学习分析 理论分析 四. Problem Setting Varients (变体) 结构化输出的主动学习  序列化模型 (CRF 或 HMM) 产生的输出. 树状输出.  批处理模式下的主动学习  适合并行处理. 每次选取最优的N个样本不一定能达到很好的效果, 因为没有考虑这些样本间信息的重合度.  主动学习的代价 多种访问类型 ","permalink":"https://michelia-zhx.github.io/posts/2021-08-31-active_learning/","summary":"Active Learning 也称为查询学习或者最优实验设计. 主动学习通过设计合理的查询函数, 不断从未标注的数据中挑选出数据标注后放入训练集. 有效的主动学习数据选择策","title":"Paper Notes - Active Learning"},{"content":"1. Attention 机制 1a. 背景知识 我们最为熟悉的NMT模型便是经典的Seq2Seq, 这篇文章从一个Seq2Seq模型开始介绍, 然后进一步看如何将Attention应用到NMT中.\n在Seq2Seq模型中, 一般使用两个RNN, 一个作为编码器, 一个作为解码器：编码器的作用是将输入数据编码成一个特征向量, 然后解码器将这个特征向量解码成预测结果.\n这个模型的问题是只将编码器的最后一个节点的结果进行了输出, 但是对于一个序列长度特别长的特征来说, 这种方式无疑将会遗忘大量的前面时间片的特征:\n既然如此, 与其输入最后一个时间片的结果, 不如将每个时间片的输出都提供给解码器. 那么解码器如何使用这些特征就是我们这里介绍的Attention的作用.\n在这里, Attention是一个介于编码器和解码器之间的一个接口, 用于将编码器的编码结果以一种更有效的方式传递给解码器. 一个特别简单且有效的方式就是让解码器知道哪些特征重要, 哪些特征不重要, 即让解码器明白如何进行当前时间片的预测结果和输入编码的对齐. Attention模型学习了编码器和解码器的对齐方式, 因此也被叫做对齐模型 (Alignment Model).\nAttention有两种类型, 一种是作用到编码器的全部时间片, 这种Attention叫做全局 (Global) Attention, 另外一种只作用到时间片的一个子集, 叫做局部 (Local) Attention.\n1b. Attention可以分为4步:   生成编码节点 将输入数据依次输入到RNN中, 得到编码器每个时间片的隐层状态的编码结果 (绿色), 并将编码器的最后一个输出作为解码器的第一个输入隐层状态 (红色, decoder hidden state):\n  为每个编码器的隐层状态计算一个得分 使用当前编码器的当前时间片的隐层状态和解码器的隐层状态计算一个得分, 得分的计算方式有多种:\n  使用softmax对得分进行归一化 将softmax作用到step 2得到的score之上, 得到和为1的分数:\n  使用score对隐层状态进行加权 将score以及隐层状态进行点乘操作, 得到加权之后的特征, 这个特征也叫做对齐特征 (Alignment Vector) 或者注意力特征 (Attention Vector):\n  加和特征向量 这一步是将加权之后的特征进行加和, 得到最终的编码器的特征向量:\n  将特征向量应用的解码器 最后一步是将含有Attention的编码器编码的结果提供给解码器进行解码, 注意每个时间片的Attention的结果会随着decoder hidden state的改变而更改:\n  2. 经典 Attention 模型 2a. Bahdanau et. al (2015)   编码器是双向GRU, 解码器是单向GRU, 解码器的初始化输入是反向GRU的输出;\n  Attention操作选择的是additive/concat;\n  解码器的输入特征是上一个时间片的预测结果和解码器的编码结果拼接而成的;\n  BLEU值为26.75\n  2b. Luong et. al (2015)   编码器和解码器都是两层的LSTM;\n  解码器的初始化隐层状态分别是两个解码器的最后一个时间片的输出;\n  在论文中他们尝试了(i) additive/concat, (ii) dot product, (iii) location-based, 以及(iv) \u0026lsquo;general\u0026rsquo;;\n  将解码器得到的结果和编码器进行拼接, 送入一个FFNN中得到最终的结果;\n  BLEU值为25.9.\n  2c. Google\u0026rsquo;s Neural Machine Translation (GNMT)   编码器是一个8层的LSTM。第一个层是双向的LSTM，把它们的特征拼接够提供给第二层，在后面的每一层LSTM都使用残差进行连接；\n  解码器是使用的8层单向LSTM并使用残差结构进行连接；\n  score function和 2a 相同，为addition/concat；\n  拼接方式也和 2a 相同；\n  英法翻译的BLEU为38.95，英德翻译的BLEU为24.17。\n  ","permalink":"https://michelia-zhx.github.io/posts/2022-04-02-attention/","summary":"1. Attention 机制 1a. 背景知识 我们最为熟悉的NMT模型便是经典的Seq2Seq, 这篇文章从一个Seq2Seq模型开始介绍, 然后进一步看如何将Attent","title":"Paper Notes - Attention"},{"content":" Learning from Multiple Teacher Networks http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf\n  loss:  teachers的softmax输出取平均和student的交叉熵 中间层表示的相对相异度(仅适用于MTKD), 三元组$(q_i,q_i^+,q_i^-)$, 其中$q$是中间层的输出, 偏序关系$q_i^+ \u0026gt; q_i^-$由两者和$q_i$的距离$d$决定, 参数$w_s$决定选取哪些层. 在不同teacher中, 中间层输出的偏序关系可能不同, 因此用投票法决定中间层输出应当的偏序关系, 设计和student对应层输出的loss, 以此鼓励student拥有和teacher中间层类似的相对相似(相异)关系. student和groudtruth的交叉熵   实验设置: 基于CIFAR-10, CIFAR-100, MNIST, SVHN的实验  CIFAR-10, 比较student不同层数和参数量(11/250K, 11/862K, 13/1.6M, 19/2.5M)时的表现(compression rate, acceleration rate and classification accuracy)(和Fitnets比较) CIFAR-10, student均为11层, 比较当student的参数为250K和862K时, teacher数量为1, 3, 5时, Teacher, RDL, FitNets, KD和他们的准确率 CIFAR-10, CIFAR-100, 比较不同方法(Teacher(5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法(19层))在两个数据集上的准确率 MNIST, 比较不同方法(Teacher(4层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法(7层))的准确率 SVHN, 比较不同方法(Teacher(5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法(19层))的准确率   @inproceedings{you2017learning, author={You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng}, booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, title={Learning from multiple teacher networks}, pages={1285\u0026ndash;1294}, year={2017} }   Semi-Supervised Knowledge Transfer for Deep Learning from Private Training Data. ICLR 2017 https://openreview.net/pdf?id=HkwoSDPgg\n  学习算法应当保护用户私人数据, 但模型会记住数据, 也会受到攻击(black/white box attack). 将样本分成n份, 分别训练teacher model, 将这些teachers聚合:  如果大多数teacher有相同的输出, 则输出不依赖于分别训练teacher的不相交集 如果有某两类票数相近, 则分歧可能会泄露私人信息(我不理解) 在投票中引入随机噪声   student是半监督, 一部分是利用private data通过teacher得到的label, 之后使用无标记的public data. 利用GAN训练student, discriminator增加1类(m+由生成器生成), 训练后只使用discriminator. 实验设置:    Dataset Teacher Student Student Public Data testing Data     MNIST 2 conv + 1 relu GANS(6 fc layers) test[:1000] test[1000:]   SVHN 2 conv + 2 relu GANS(7 conv + 2 NIN) test[:1000] test[1000:]   UCI Adult RF(100 trees) RF(100 trees) test[:500] test[500:]   UCI Diabetes RF(100 trees) RF(100 trees) test[:500] test[500:]     @article{Papernot2017SemisupervisedKT, title={Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data}, author={Nicolas Papernot and Mart{'i}n Abadi and {'U}lfar Erlingsson and Ian J. Goodfellow and Kunal Talwar}, journal={arXiv preprint}, pages = {}, year={2016} }   Knowledge Adaptation: Teaching to Adapt. ICLR, 2017 https://openreview.net/pdf?id=rJRhzzKxl\n  teachers的domain和student $\\mathcal{D}{T_i}, \\mathcal{D}S$不完全一致, 因此student对teacher的信任度取决于两者表示空间的相似度 $\\mathcal{L} = \\mathcal{H}(\\sum{i=1}sim(\\mathcal{D}{T_i}, \\mathcal{D}S)\\cdot D{T_i}, D_S)$ 定义MCD, MCD越大表示越远离分类边界, 输入teacher得到的结果置信度越高. 因此选取MCD最大(即置信度最高)的n个样本, 在teacher中得到的结果作为伪标记以训练student可以使无监督学习的性能得到提升. 实验设置: 基于Amazon product reviews sentiment analysis dataset. 包含Book, DVD, Electronics, Kitchen四类.  比较teacher, 由以相同类别样本训练的teacher训练出来的student, 由以所有样本训练的teacher训练出来的student, 结合以上两种teacher训练出来的student, 以及许多其他模型(SCL, SFA, SCL-com, SFA-com, SST, IDDIWP, DWHC, DAM, CP-MDA, SDAMS-SVM, SDAMS-Log)在四种类别上的性能. 比较分别以其中三类为domain的teacher训练以第四类为domain的student(B$\\rightarrow$D,E$\\rightarrow$D,K$\\rightarrow$D依次轮换)在不同方法下的性能   @article{ruder2017knowledge, title={Knowledge adaptation: Teaching to adapt}, author={Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G}, journal={arXiv preprint arXiv:1702.02052}, pages = {}, year={2017} }   Deep Model Compression: Distilling Knowledge from Noisy Teachers. Sau, Bharat Bhusan et al. arXiv:1610.09650 https://arxiv.org/pdf/1610.09650.pdf\n  在teacher的输出(student的目标)上加扰动, 等价于基于噪声的正则化. 可用于模型压缩. 实验设置: 基于MNIST, SVHN, CIFAR-10.  MNIST: teacher \u0026ndash; a modified network of LeNet([C5(S1P0)@20-MP2(S2)]- [C5(S1P0)@50-MP2(S2)]- FC500- FC10); student \u0026ndash; FC800-FC800-FC10 SVHN: Network-in-Network([C5(S1P2)@192]- [C1(S1P0)@160]- [C1(S1P0)@96-MP3(S2)]- D0.5- [C5(S1P2)@192]- [C1(S1P0)@192]- [C1(S1P0)@192- AP3(S2)]- D0.5- [C3(S1P1)@192]- [C1(S1P0)@192]- [C1(S1P0)@10]- AP8(S1)); student: LeNet([C5(S1P2)@32-MP3(S2)]- [C5(S1P2)@64-MP3(S2)]- FC1024-FC10) CIFAR-10: teacher: same as SVHN; student: a modified version of the LeNet([C5(S1P2)@64-MP2(S2)]- [C5(S1P2)@128- MP2(S2)]-FC1024-FC10).   @article{sau2016deep, title={Deep model compression: Distilling knowledge from noisy teachers}, author={Sau, Bharat Bhusan and Balasubramanian, Vineeth N}, journal={CoRR}, pages = {}, year={2016} }   Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Tarvainen, Antti and Valpola, Harri. NeurIPS 2017 https://papers.nips.cc/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf\n  算法:  构建一个普通的监督模型; copy一份监督学习模型, 原模型叫做student, 新的叫teacher; 每步使用同样的minibatch输入到student与teacher模型中, 但在输入数据前分别加入随机增强或者噪音; 加入student与teacher输出的一致性损失函数; 优化器只更新student的权重; 每步之后, 采用student权重的EMA更新teacher权重;   核心思想是: 模型既充当学生, 又充当老师. 作为老师, 用来产生学生学习时的目标; 作为学生, 则利用教师模型产生的目标来进行学习. 而教师模型的参数是由历史上(前几个step)几个学生模型的参数经过加权平均得到. 可以看成是П-model中的两次计算中模型换成了两个不同的模型, 一个叫teacher, 一个叫student; 另外, 也可以看成作Temporal ensembling的改进版, 在Temporal ensembling中, 采用的是每epoch的指数移动平均值来聚合历史数内容, 而Mean teacher则是在每训练步进行对Student的权重进指数移动平均. 实验设置: 基于SVHN和CIFAR-10  All the methods in the comparison use a similar 13-layer ConvNet architecture.   @inproceedings{10.5555/3294771.3294885, title = {Mean Teachers Are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results}, author = {Tarvainen, Antti and Valpola, Harri}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {1195–1204}, year = {2017} }   Born-Again Neural Networks. Furlanello, Tommaso et al. ICML 2018 https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf\n  基于新模型的输入和原模型的输入间的交叉熵, 使用KD项修改替代和正则化原来的loss Selves Born-Again Networks集成的学习顺序: $\\mathcal{L}(f(x, \\arg\\min_{\\theta_{k-1}}\\mathcal{L}(f(x, \\theta_{k-1}))),f(x,\\theta_k))$, 将上一个student学到的知识作为监督信息, 教导下一个学生. 实验设置:  CIFAR-10: Wide-ResNet with different depth and width (28-1, 28-2, 28-5, 28-10) and DenseNet of different depth and growth factor (112-33, 90-60, 80-80, 80-120) CIFAR-100: 与上同.   @inproceedings{Furlanello2018BornAN, title={Born Again Neural Networks}, author={Tommaso Furlanello and ZaKnowledge Adaptation: Teaching to Adaptchary Chase Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar}, booktitle={ICML}, year={2018} }   Deep Mutual Learning. Zhang, Ying et al. CVPR 2018 https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf\n  两个(多个?)students相互学习, 对于每个student, 损失为和groundtruth的交叉熵以及相对于另一个student的softmax输出的KL散度: $\\mathcal{L}{\\theta_1} = \\mathcal{L}{C_1} + D_{KL}(p_2|p_1), \\theta_1\\leftarrow\\theta_1+\\gamma_t\\dfrac{\\partial\\mathcal{\\theta_1}}{\\partial\\theta_1}$; $\\mathcal{L}{\\theta_2} = \\mathcal{L}{C_2} + D_{KL}(p_1|p_2), \\theta_2\\leftarrow\\theta_2+\\gamma_t\\dfrac{\\partial\\mathcal{\\theta_2}}{\\partial\\theta_2}$ 优点:  随着学生网络的增加其效率也得到提高 它可以应用在各种各样的网络中, 包括大小不同的网络 即使是非常大的网络采用相互学习策略, 其性能也能够得到提升   实验设置:  数据集: ImageNet, CIFAR-10, CIFAR-100, Market-1501 Networks:    ResNet-32 MobileNet InceptionV1 WRN-28-10     0.5M 3.3M 7.8M 36.5M       @inproceedings{8578552, title = {Deep Mutual Learning}, author = {Y. Zhang and T. Xiang and T. M. Hospedales and H. Lu}, booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages = {4320-4328}, year = {2018} }   Data Distillation: Towards Omni-Supervised Learning. Radosavovic, Ilija et al. CVPR 2018 https://openaccess.thecvf.com/content_cvpr_2018/papers/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.pdf\n  Model Distillation vs. Data Distillation: 前者ensemble同一样本在不同模型的输出, 后者ensemble同一样本经不同转换后在同一模型的输出. 方法:  用手动标注的数据训练模型A 用模型A去训练数据增广 (本文中为 scaling and horizontal flipping) 的未标注数据 将未标注数据的预测结果通过 ensembling 多个预测结果, 转化为 labels 在手动标注和自动标注的数据集重新训练模型   实验: 在COCO Keypoint Detection, Object Detection 验证. teacher和student都是Mask R-CNN keypoint detection variant @inproceedings{inproceedings, title = {Data Distillation: Towards Omni-Supervised Learning}, author = {Radosavovic, Ilija and Dollar, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming}, year = {2018}, doi = {10.1109/CVPR.2018.00433} pages = {4119-4128} }   Multilingual Neural Machine Translation with Knowledge Distillation. ICLR 2019 https://openreview.net/pdf?id=S1gUsoR9YX\n  方法很简单, 就是先针对每对语言训练单独的翻译模型作为teacher, 再用multi-teacher KD训练student, loss就是student和label的交叉熵以及和teacher的softmax输出的交叉熵. 实验设置: 数据集: IWSLT, WMT, Ted Talk; student和teacher均使用Transformer    Task model hidden size $d_{model}$ feed-forward hidden size $d_{ff}$ number of layer     IWSLT and Ted talk tasks 256 1024 2   WMT task 512 2048 6     @article{Tan2019MultilingualNM, title={Multilingual Neural Machine Translation with Knowledge Distillation}, author={Xu Tan and Yi Ren and Di He and Tao Qin and Zhou Zhao and Tie-Yan Liu}, journal={ICLR}, year={2019}, volume={abs/1902.10461} }   Unifying Heterogeneous Classifiers with Distillation. Vongkulbhisal et al. CVPR 2019 https://openaccess.thecvf.com/content_CVPR_2019/papers/Vongkulbhisal_Unifying_Heterogeneous_Classifiers_With_Distillation_CVPR_2019_paper.pdf\n  N个不同的模型$\\mathcal{C} = {C_i}_{i=1}^N$具有不同的结构和目标类别, $C_i$被训练以分别预测$p_i(Y=l_j)$, 并整合出样本在所有类中的概率$q(Y=i_j)$. 最后利用$q$训练student. 作者提出了基于交叉熵最小化和矩阵分解的方法，从未标记的样本中估计所有类别的soft-labels. 实验设置:  数据集: ImageNet, LSUN, Places365 $C_i$从AlexNet, VGG16, ResNet18, ResNet34中随机选择   @article{Vongkulbhisal2019UnifyingHC, title={Unifying Heterogeneous Classifiers With Distillation}, author={Jayakorn Vongkulbhisal and Phongtharin Vinayavekhin and Marco Visentini Scarzanella}, journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2019}, pages={3170-3179} }   Distilled Person Re-Identification: Towards a More Scalable System. Wu, Ancong et al. CVPR 2019 https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf\n  解决三个问题: 降低标签成本(减少标签的需求量); 降低跨数据库成本(利用一些先验知识); 降低测试成本(使用轻量级网络) 假设taregt domain包含10个类的图片, 先用多个个source domain分别训练多个teacher model, source domain之后并不会被用到(利用一些先验知识\u0026ndash;降低跨数据库成本); target domain可以只包含10个labelled sample(10类均有), 其余均为unlabeled sample, 对于N个unlabelled input, 定义相似度矩阵$A$, 其中第i行第j列表示第i个图像和第j个图像在同一个模型下输出的相似度. 为了将知识从teacher迁移到student, 需要最小化teacher的相似度矩阵$A_T$和student的相似度矩阵$A_S$的距离(这句话是学习single teacher). 分别利用teacher计算target domain中每一个x的特征向量, 并分别计算相似度矩阵$A$, 使用$L_{ver}$更新每一个老师模型的权重$a$(可以理解为，权重越大，该老师模型对应的source和target越相似) 计算出每一个老师模型和学生模型得到的相似矩阵的差异，并使用上述的权重加权，从而得到$L_{ta}$. 使用$L_{ta}$对学生模型进行更新, 循环训练. 实验设置: 数据集 \u0026ndash; Market-1501, DukeMTMC. 分别使用MSMT17, CUHK03, ViPER, DukeMTMC, Market-1501训练5个teacher model$T_1, T_2, T_3, T_4, T_5$; teacher \u0026ndash; an advanced Re-ID model PCB, student \u0026ndash; a lightweight mod- el MobileNetV2. @InProceedings{Wu_2019_CVPR, author = {Wu, Ancong and Zheng, Wei-Shi and Guo, Xiaowei and Lai, Jian-Huang}, title = {Distilled Person Re-Identification: Towards a More Scalable System}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2019} }   Diversity with Cooperation: Ensemble Methods for Few-Shot Classification. Dvornik, Nikita et al. ICCV 2019 https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf\n  meta learning \u0026ndash; learning to learn 模型间的关系有三种 - 合作(预测的结果无论是属于正确结果的概率还是属于错误结果的概率都是比较一致的), 独立(两个模型预测的结果之间不存在明显的关系), 多样性(除了正确结果，预测为其他结果的概率差异明显). 文章通过出了交叉熵损失外设计不同的损失函数$\\psi(y_i,f_{\\theta_j}(x_i),f_{\\theta_l}(x_i))$诱导模型的关系向不同方向发展, 例如基于cos或KL散度. 实验设置:  数据集: mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds (CUB) 2002011. ensemble of ResNet18 and WideResNet28   @INPROCEEDINGS{9010380, title={Diversity With Cooperation: Ensemble Methods for Few-Shot Classification},author={Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia}, booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, pages={3722-3730},\nyear={2019} }   Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System. Yang, Ze et al. WSDM 2020 https://arxiv.org/pdf/1910.08381.pdf\n  一种用于网络问答系统的两阶段多教师知识蒸馏(简称 TMKD)方法, 首先用一个通用的问答提炼任务对student进行预训练(貌似也是使用Multi-teacher), 并在下游任务(如 Web Q\u0026amp;A 任务, MNLI, SNLI, 来自 GLUE 的 RTE 任务)上使用Multi-Teacher KD进一步微调这个预训练的student. “early calibration” effect缓解了单个teacher造成的过拟合偏差. 实验设置:  数据集 - DeepQA, CommQA-Unlabeled, CommQA-Labeled, MNLI, SNLI, QNLI, RTE. Baseline: teacher - BERT-3, BERT_{large}, BERT_{large}Ensemble; student(Traditional Distillation Model) - Bi-LSTM(1-o-1, 1_{avg}-o-1, m-o-m), BERT3(1-o-1, 1_{avg}-o-1, m-o-m), student(TMKD) - Bi-LSTM(TMKD), TMKD_{base}, TMKD_{large}(后两者都是BERT-3 models).   @inproceedings{inproceedings, author = {Ze, Yang and Shou, Linjun and Gong, Ming and Lin, Wutao and Jiang, Daxin}, title = {Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System}, publisher = {Association for Computing Machinery}, doi = {10.1145/3336191.3371792}， pages = {690-698}, year = {2020} }   FEED: Feature-level Ensemble for Knowledge Distillation. Park, SeongUk and Kwak, Nojun. AAAI 2020 https://openreview.net/pdf?id=BJxYEsAqY7\n  实验方法就是让student直接学teacher的feature. 实验设置: 数据集: CIFAR-100; 选取模型: student \u0026ndash; ResNet-56, ResNet-110, WRN28-10, ResNext29-16x64d; 没说teacher是谁. @article{Park2019FEEDFE, title={FEED: Feature-level Ensemble for Knowledge Distillation}, author={Seonguk Park and Nojun Kwak}, journal={ECAI}, year={2019}, volume={abs/1909.10754} }   Stochasticity and Skip Connection Improve Knowledge Transfer. Lee, Kwangjin et al. ICLR 2020 https://openreview.net/pdf?id=HklA93NYwS\n  利用单个教师网络生成多个教师网络(加入stochastic blocks和skip connections)并训练学生网络, 分块并含有skip connections的网络可以看成树状网络, 从input到output有多条路径. 实验设置: 数据集 \u0026ndash; CIFAR-100 和 tiny imagenet, 并将这种方法应用到KD, AT(attention tranfer), ML. 实验中涉及到的teacher有ResNet 32, ResNet 110, WRN 28-10, MobileNet, WRN 40-4; 涉及到的student有VGG 13, ResNet 20, ResNet 32, WRN 40-4. @INPROCEEDINGS{9287227, author={Nguyen, Luong Trung and Lee, Kwangjin and Shim, Byonghyo}, title={Stochasticity and Skip Connection Improve Knowledge Transfer}, booktitle={2020 28th European Signal Processing Conference (EUSIPCO)}, pages={1537-1541}, year={2021} }   Hydra: Preserving Ensemble Diversity for Model Distillation. Tran, Linh et al. arXiv:2001.04694 http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-026.pdf\n  普通multi-teacher KD对teacher的预测值取平均, 这样会丧失多teacher结果包含的不确定性信息(?), 本文将student拆分成body和多个head, 每个head对应一个teacher, 以保留多teacher输出的多样性 假设有M个teacher, 首先训练一个head直至其收敛至teacher的平均, 再添加其他M-1个head, M个head一起训练, 实验证明如果没有第一个head会很难收敛. 作者定义了一个模型不确定性, 由数据不确定性和总不确定性组成(我不理解为什么是这个顺序). 实验设置:  数据集: a spiral toy dataset(用于可视化并解释模型不确定性), MNIST(测试时用了它的测试集和Fashion-MNIST), CIFAR-10(测试时用了它的测试集, cyclic translated test set, 80 different corrupted test sets 和 SVHN). 模型: toy dataset - 两层MLP, 每层100个结点; MNIST - MLP; CIFAR-10 - ResNet-20 V1. 在回归问题中, 所有数据集均使用MLP.   @article{DBLP:journals/corr/abs-2001-04694, author = {Linh Tran, Bastiaan S. Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V. Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Sebastian Nowozin, Rodolphe Jenatton}, title = {Hydra: Preserving Ensemble Diversity for Model Distillation}, journal = {CoRR}, year = {2020} }   Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition. Gao, Yan et al. arXiv:2005.09310 https://arxiv.org/pdf/2005.09310v1.pdf\n  @article{DBLP:journals/corr/abs-2005-09310, author = {Yan Gao, Titouan Parcollet, Nicholas D. Lane}, title = {Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition}, journal = {CoRR}, year = {2020} }   Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection. Chen, Cong et al. IEEE 2020 [code]\n  Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation. MICCAI 2020\n  Knowledge Distillation for Multi-task Learning. Li, WeiHong \u0026amp; Bilen, Hakan. arXiv:2007.06889 [project]\n  Adaptive Multi-Teacher Multi-level Knowledge Distillation. Liu, Yuang et al. Neurocomputing 2020 [code] https://arxiv.org/pdf/2103.04062.pdf\n  loss: $\\mathcal{L} = \\mathcal{L}{KD}+\\alpha\\mathcal{L}{angle}+\\beta\\mathcal{L}_{HT}$  $\\mathcal{L}{KD}$: 对于每个样本, student需要赋予teachers的输出不同的权重, student中fc之前的表示经过maxpooling后和每个teacherfc前的表示分别做点积作为权重, teacher的加权和作为weighted target, 将weighted target与student的soft-target间的KL散度和student输出与groungtruth的交叉熵作为$\\mathcal{L}{KD}$. $\\mathcal{L}{angle}$: 对于样本组成的三元组, 计算它们的teacher和student表示的空间相对位置, 计算二者的Huber loss作为$\\mathcal{L}{angle}$. $\\mathcal{L}_{HT}$: 计算teacher和student中间层表示的差的二范式, student的中间层需要经过一个单层FitNet使其规模等于teacher的中间层表示.   实验设置: 数据集有CIFAR-10, CIFAR-100和Tiny-ImageNet.  CIFAR-10, CIFAR-100: teacher使用ResNet110, VGG-19, DenseNet121, student为ResNet20; 比较不同baseline (OKD, FitNet, RKD, AvgMKD, DML) 在数据集上的表现; 比较不同baseline(OKD, AvgMKD, DML)在teacher数量为2,3,5时的表现. Tiny-ImageNet: teacher(ResNet110, ResNet56, ResNet32), student - ResNet20.   @article{LIU2020106, author={Yuang Liu and W. Zhang and Jijie Wang}, title = {Adaptive multi-teacher multi-level knowledge distillation}, journal = {Neurocomputing}, pages = {106-113}, year = {2020} }  ","permalink":"https://michelia-zhx.github.io/posts/2022-02-23-multi_teacher_knowledge_distillation-1/","summary":"Learning from Multiple Teacher Networks http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf loss: teachers的softmax输出取平均和student的交叉熵 中间层表示的相对相异度(仅适用于MTKD), 三元组$(q_i","title":"Paper Notes - Multi-Teacher Knowledge Distillation - 1"},{"content":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks https://arxiv.org/pdf/2004.05937.pdf\n Learning from Multiple Teacher Networks, KDD 2017 Efficient knowledge distillation from an ensemble of teachers. Interspeech 2017: 对teacher的logits取加权平均, 加权平均和student的logits计算交叉熵作为损失函数; 另外没有说权是怎么分配的 (提前设置好的). A Two-Teacher Framework for Knowledge Distillation. ISNN 2019 Feature-Level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks. ECAI 2020 Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation: 多教师知识蒸馏中如何整合多教师的知识的问题依然没有得到很好的解决. 不同的教师具有不同的重要性, 且有些教师会对学生的泛化性能产生负面影响. 本文提出了基于多任务学习的自适应方法 (没看懂). Ensemble Knowledge Distillation for Learning Improved and Efficient Networks. ECAI 2020 Knowledge Distillation based Ensemble Learning for Neural Machine Translation. ICLR 2021: 机器翻译方向的文章, 主要提出了新的损失函数. A Simple Ensemble Learning Knowledge Distillation. MLIS 2020: 只有一个损失函数 ($\\mathcal{L}{CL}+\\mathcal{L}{KD}$) (???) Stochasticity and Skip Connection Improve Knowledge Transfer. ICLR 2020 Amalgamating Knowledge towards Comprehensive Classification. AAAI 2019 Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification. ECCV 2020: 主要用于解决长尾问题, 每个teacher对应几类, 用样本数相近的几类数据去训练的效果会优于从长尾分布的数据中学习. Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning. IJCAI 2019  Semi-Supervised Knowledge Amalgamation for Sequence Classification. AAAI 2021   Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation. CVPR 2019  Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning. ECCV 2020   Highlight Every Step: Knowledge Distillation via Collaborative Teaching Hydra: Preserving Ensemble Diversity for Model Distillation  Diversity Matters When Learning From Ensembles. NIPS 2021   Knowledge flow: Improve upon your teachers. ICLR 2019  Learning from Multiple Teacher Networks. You, Shan et al. KDD 2017   http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf 这是第一篇提出Multi-teacher KD的文章\n  问题: 如何进行多教师知识蒸馏? 方法: 主要改进了loss function, 由三部分组成:  teachers的softmax输出取平均和student的交叉熵 中间层表示的相对相异度 (仅适用于MTKD), 三元组 $(q_i,q_i^+,q_i^-)$, 其中$q_i$是样本$i$在中间层的表示, 偏序关系$q_i^+ \u0026gt; q_i^-$由两者和$q_i$的距离$d$决定, 参数$w_s$决定选取哪层. 在不同teacher中, 输入的三个样本的中间层表示的偏序关系可能不同, 因此用投票法决定正确的偏序关系. 设计和student对应层输出的loss, 以此鼓励student的中间层的表示空间拥有和teacher近似的结构. student和groudtruth的交叉熵.   实验设置: 基于CIFAR-10, CIFAR-100, MNIST, SVHN的实验  CIFAR-10, 比较student不同层数和参数量 (11/250K, 11/862K, 13/1.6M, 19/2.5M) 时的表现 (compression rate, acceleration rate and classification accuracy) (和Fitnets比较) CIFAR-10, student均为11层, 比较当student的参数为250K和862K时, teacher数量为1, 3, 5时, Teacher, RDL, FitNets, KD和他们的准确率 CIFAR-10, CIFAR-100, 比较不同方法 (Teacher (5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法 (19层)) 在两个数据集上的准确率 MNIST, 比较不同方法 (Teacher (4层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法 (7层)) 的准确率 SVHN, 比较不同方法 (Teacher (5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法 (19层)) 的准确率   @inproceedings{you2017learning, author={You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng}, booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, title={Learning from multiple teacher networks}, pages={1285\u0026ndash;1294}, year={2017} }  A Two-Teacher Framework for Knowledge Distillation. ISNN 2019   https://link.springer.com/chapter/10.1007%2F978-3-030-22796-8_7 (找不到pdf, 只有网页版)\n  问题: single-teacher KD不行. 方法: 该框架由两个以不同策略训练的教师网络组成, 一个被严格地训练以指导学生网络学习复杂的特征 (loss为每一层表示的差), 另一个指导学生网络学习基于学到的特征进行的决策 (loss为教师和学生logits的交叉熵). 其中用到了adversarial learning的方法, 用一个discriminator分辨教师和学生的表示. 实验设置: 实验做得又少又拉, 也没跟single-teacher比, 就不写了. @InProceedings{10.1007/978-3-030-22796-8_7, title = {A Two-Teacher Framework for Knowledge Distillation}, author = {Chen, Xingjian, Su Jianbo, Zhang Jun\u0026quot;, booktitle = {Advances in Neural Networks \u0026ndash; ISNN 2019}, pages = {58\u0026ndash;66}, year = {2019} }  Feature-Level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks. ECAI 2020   https://ecai2020.eu/papers/405_paper.pdf (这篇和上次的FEED是同一篇)\n  问题: 多教师知识蒸馏在使用基于feature-map的蒸馏任务中不方便. 方法: 加入一些非线性转换层. loss function由两部分组成: student和groudtruth的交叉熵; student的feature经过n种非线性转换并归一化, 分别和teacher的feature (归一化后) 做差并求一范式. 实验设置: 数据集: CIFAR-100; 选取模型: student \u0026ndash; ResNet-56, ResNet-110, WRN28-10, ResNext29-16x64d; 没说teacher是谁. @article{Park2019FEEDFE, title={FEED: Feature-level Ensemble for Knowledge Distillation}, author={Seonguk Park and Nojun Kwak}, journal={ECAI}, year={2020} }  Ensemble Knowledge Distillation for Learning Improved and Efficient Networks. ECAI 2020   https://arxiv.org/pdf/1909.08097v1.pdf\n  问题: 由CNN组成的集成模型在模型泛化方面表现出显着改进，但代价是计算量大和内存需求大. (机翻的, 摘要提出来的问题貌似和他后面做的事没啥关系, 且这篇和上次看的倒数第二篇结构一样) 方法: 学生由多个branch组成, 每个branch和teacher一一对应, loss由三部分组成: teacher (多teacher输出相加) 和groundtruth的交叉熵, student (多branch的输出相加) 和groungtruth的交叉熵, teacher和student对应branch表示的KL散度和MSE. 结构:  实验设置: teacher \u0026ndash; ResNet14, ResNet20, ResNet26, ResNet32, ResNet44, ResNet56, and ResNet110; student \u0026ndash; a CNN with dense connections, a medium capacity CNN with 6 dense layers (DenseNet6), a large capacity CNN with 12 dense layers (DenseNet12); 数据集: EKD, CIFAR-10, CIFAR-100. @article{asif2019ensemble, title={Ensemble knowledge distillation for learning improved and efficient networks}, author={Asif, Umar and Tang, Jianbin and Harrer, Stefan}, journal={ECAI}, year={2020} }  Stochasticity and Skip Connection Improve Knowledge Transfer. Lee, Kwangjin et al. ICLR 2020   https://openreview.net/pdf?id=HklA93NYwS\n  问题: 部署多个教师网络有利于学生网络的学习, 但在一定程度上造成资源浪费. 方法: 利用单个教师网络生成多个教师网络 (加入add stochastic blocks和skip connections) 并训练学生网络, 在没有额外资源的情况下为学生网络提供足够的知识. 实验设置: 数据集 \u0026ndash; CIFAR-100 和 tiny imagenet, 并将这种方法应用到KD, AT(attention tranfer), ML. 实验中涉及到的teacher有ResNet 32, ResNet 110, WRN 28-10, MobileNet, WRN 40-4; 涉及到的student有VGG 13, ResNet 20, ResNet 32, WRN 40-4. @INPROCEEDINGS{9287227, author={Nguyen, Luong Trung and Lee, Kwangjin and Shim, Byonghyo}, title={Stochasticity and Skip Connection Improve Knowledge Transfer}, booktitle={2020 28th European Signal Processing Conference (EUSIPCO)}, pages={1537-1541}, year={2021} }  Amalgamating Knowledge towards Comprehensive Classification. AAAI 2019   https://arxiv.org/pdf/1811.02796v1.pdf\n  问题: 重用已经过训练的模型可以显著降低降低从头开始训练新模型的成本, 因为用于训练原始网络的注释通常不向公众公开. 方法: 使用multi-teacher KD的方法对多个模型进行合并, 得到轻量级的student模型. 方法分为两步: The feature amalgamation step \u0026ndash; 将teacher的每一个中间层表示都合并, 得到student对应的中间层表示. 一种简单的方式是直接concat, 但会导致student变成teacher的4倍大小. 因此在concat多个teacher的中间层表示后经过一个auto-encoder, 压缩student\u0026rsquo;s feature的同时保留重要信息; The parameter learning step \u0026ndash; 根据student相邻两层表示学习中间层参数. 损失函数由几部分组成: feature amalgamation \u0026ndash; teacher的中间层表示concat之后经过$1\\times 1$卷积得到压缩的表示, 再经过$1\\times 1$卷积试图复原, 和concat的表示之差 (的模) 作为loss; parameter learning: student前一层表示 $F_a^{l-1}$ 经过中间层后的表示 $\\hat{F}_a^l = conv(pool(activation(F_a^{l-1})))$ 和由teacher生成的下一层 $F_a^l$ 表示的差. 结构:  实验设置: 数据集 \u0026ndash; CUB-200- 2011, Stanford Dogs, FGVC-Aircraft, Cars; teacher \u0026ndash; AlexNet (在ImageNet上fine-tune) @inproceedings{shen2019amalgamating, title={Amalgamating knowledge towards comprehensive classification}, author={Shen, Chengchao and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, pages={3068\u0026ndash;3075}, year={2019} }  Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning. IJCAI 2019   https://arxiv.org/pdf/1906.10546.pdf\n  问题: 教师网络结构不一, 各自擅长解决不同的任务. 方法: student学习teacher经过转化的表示 (Common Feature Learning), 同时学习teacher输出的soft target. 结构:  实验设置: networks: alexnet, vgg-16, resnet-18, resnet-34, resnet-50. datasets: (classification) Stanford Dog, Stanford Car, CUB200-2011, FGVC-Aircraft, Catech 101; (face) CASIA, MS-Celeb-1M, CFP-FP, LFW, AgeDB-30. @inproceedings{10.5555/3367243.3367468, author = {Luo Sihui, Wang Xinchao, Fang Gongfan, Hu Yao, Tao Dapeng, Song Mingli}, title = {Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning}, booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence}, pages = {3087–3093}, year = {2019} }  Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation. CVPR 2019   https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.pdf (这篇是多教师, 但主要针对的是从多任务的教师集合中进行选择性学习, 其中只需了解第三条, 针对的是问题中的“学生选择某个老师特征？”)\n  问题: 如何利用多个针对不同任务, 在不同数据集上优化的教师模型, 来训练可以定制的可以处理选择性任务的学生? 方法: 假设没有可用的人工注释，并且每个老师可能是单任务或多任务. 首先从共享相同子任务的异构教师(Source Net)中提取特定的知识(Component Net), 合并提取的知识以构建学生网络(Target Net). 为了促进训练，作者采用了选择性学习方案，对于每个未标记的样本，学生仅从具有最小预测歧义的教师那里自适应地学习. Selective Learning: $I(p^t(x_i)) = -\\sum_ip^t(x_i)\\log(p^t(x_i))$, 针对每个样本学生只学置信度最大的教师. 实验设置: 数据集: CelebFaces Attributes Dataset (CelebA), Stanford Dogs, FGVC-Aircraft, CUB-200-2011, Cars. Source net: resnet-18; component net and target net adopt resnet-18-like network architectures. @inproceedings{shen2019customizing, title={Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation}, author={Shen, Chengchao and Xue, Mengqi and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={3504\u0026ndash;3513}, year={2019} }  Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning. ECCV 2020   https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510630.pdf (作者和\u0026quot;7\u0026quot;是同一个. 这篇文章主要针对多任务知识蒸馏, student的多个head针对不同任务, 但其中的Competi-Collaboration策略可以学习)\n  问题: 和\u0026quot;7\u0026quot;一样. 方法: 训练分为两步, 第一步整合teacher的多模态信息, 训练student的共享参数部分; 第二步用基于梯度的竞争-平衡策略, 训练student的multi-head部分, 每个部分针对不同的任务. 令$\\Omega, \\Theta_i, \\Theta_j$表示student的共享参数部分 (encoder) 和两个针对特定任务的head参数, Competi-Collaboration策略迭代如下:  Competition step: 固定student的共享参数部分, 最小化multi-head的输出和multi-teacher输出logits的差异. Collaborative step: 固定multi-head部分参数, 最小化$head_i, head_j$输出的logits和对应teacher的差, 以及student的encoder输出和teacher的encoder输出的差. 最后更新分配给$head_i, head_j$的权重 (计算loss时用到), 并归一化更新后的权重向量.   结构:  实验设置: 使用的数据集是Taskonomy dataset. teacher均使用taskonomy models, student的encoder使用ResNet-50加一个卷积层, decoder的结构根据具体任务各不相同. @InProceedings{10.1007/978-3-030-58539-6_38, author = {Luo Sihui, Pan Wenwen, Wang Xinchao, Wang Dazhou, Tang Haihong, Song Mingli}, title = {Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning}, booktitle = {Computer Vision \u0026ndash; ECCV 2020}, pages = {631\u0026ndash;646}, year = {2020} }  Highlight Every Step: Knowledge Distillation via Collaborative Teaching   https://arxiv.org/pdf/1907.09643v1.pdf (这篇里用到的attention loss还行)\n  问题: 现有的知识蒸馏方法时常忽略训练过程中与训练结果有关的有价值的信息. 方法: 本文一共使用两个teacher, 一个是从头开始训练的 (scratch teacher), 指导student走能到达最终logits的最佳路径 (loss包含和groundtruth的交叉熵, 和student soft target的L2 loss); 另一个是已经预训练好的 (expert teacher), 引导学生专注于对任务更有用的关键区域 (会计算和student中间层表示的attention loss). 结构:  实验设置: 数据集采用CIFAR-10, CIFAR-100, SVHN和Tiny-ImageNet. teachers和student均使用Wide-ResNet, 其中teachers为WRN-40-1, student采用WRN-16-1. @ARTICLE{9151346, author={Zhao Haoran, Sun Xin, Dong Junyu, Chen Changrui, Dong Zihe}, title={Highlight Every Step: Knowledge Distillation via Collaborative Teaching}, journal={IEEE Transactions on Cybernetics}, pages={1-12}, year={2020} }  Knowledge flow: Improve upon your teachers. ICLR 2019   https://openreview.net/pdf?id=BJeOioA9Y7\n  问题: 如今几乎对于所有给定的任务都可以使用现有的深度学习网络, 并且人们越来越不清楚在处理新任务时应从哪个网络开始, 或者选择哪个网络进行微调. 本文将\u0026quot;知识\u0026quot;从多个深度网络 (称为教师) 移动到一个新的深度网络模型 (称为学生). 教师和学生的结构可以任意不同, 他们也可以在具有不同输出空间的完全不同的任务上进行训练. 方法: 教师的中间层表示, 经过可训练的矩阵$Q$, 加权组成学生的中间层表示, 权重$w$也可学习. 实验设置: 主要是强化学习. @inproceedings{liu2018knowledge, title={Knowledge Flow: Improve Upon Your Teachers}, author={Iou-Jen Liu and Jian Peng and Alexander Schwing}, booktitle={International Conference on Learning Representations}, year={2019} }  Semi-Supervised Knowledge Amalgamation for Sequence Classification. AAAI 2021   https://www.aaai.org/AAAI21Papers/AAAI-1292.ThadajarassiriJ.pdf\n  问题: 每个教师在不同的训练集上训练, 导致他们对于未知的种类样本的输出是不可预测的, 并且和其他教师的输出无关. 因此在融合多教师的知识时, 一些教师有可能会给出很高置信度的错误分类. 方法: 包含两个部分, 一个是 Teacher Trust Learner (TTL), 在有标注训练集上训练对于给定输入样本, 每个教师的可信度有多高 $P(y_j\\in\\mathcal{Y}_k|X)$; 另一个是 Knowledge Amalgamator, 用于将多个教师给出的概率分布整合成在最终类别集合上的概率分布. 实验设置: Datasets: SyntheticControl (SYN), MelbournePedestrian (PED), Human Activity Recognition Using Smartphones (HAR), ElectricDevices (ELEC). Baselines: Original Teachers, SupLSTM, SelfTrain. @article{Sun2021CollaborativeTL, title={Collaborative Teacher-Student Learning via Multiple Knowledge Transfer}, author={Liyuan Sun, Jianping Gou, Lan Du, Dacheng Tao}, journal={ArXiv}, year={2021} }  Hydra: Preserving Ensemble Diversity for Model Distillation   https://arxiv.org/pdf/2001.04694.pdf\n  普通multi-teacher KD对teacher的预测值取平均, 这样会丧失多teacher结果包含的不确定性信息, 本文将student拆分成body和多个head, 每个head对应一个teacher, 以保留多teacher输出的多样性 假设有M个teacher, 首先训练一个head直至其收敛至teacher的平均, 再添加其他M-1个head, M个head一起训练, 实验证明如果没有第一个head会很难收敛. 作者定义了一个模型不确定性, 由数据不确定性和总不确定性组成(我不理解为什么是这个顺序). 结构:  实验设置:  数据集: a spiral toy dataset(用于可视化并解释模型不确定性), MNIST(测试时用了它的测试集和Fashion-MNIST), CIFAR-10(测试时用了它的测试集, cyclic translated test set, 80 different corrupted test sets 和 SVHN). 模型: toy dataset - 两层MLP, 每层100个结点; MNIST - MLP; CIFAR-10 - ResNet-20 V1. 在回归问题中, 所有数据集均使用MLP.   @article{DBLP:journals/corr/abs-2001-04694, author = {Linh Tran, Bastiaan S. Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V. Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Sebastian Nowozin, Rodolphe Jenatton}, title = {Hydra: Preserving Ensemble Diversity for Model Distillation}, journal = {CoRR}, year = {2020} }  Diversity Matters When Learning From Ensembles. NIPS 2021   https://papers.nips.cc/paper/2021/file/466473650870501e3600d9a1b4ee5d44-Paper.pdf\n  问题: 作者的假设是，一个蒸馏模型应该尽可能多地吸收集成模型内部的功能多样性, 而典型的蒸馏方法不能有效地传递这种多样性, 尤其是实现接近零训练误差的复杂模型. 方法: 首先证明了上述猜想, 随后作者提出了一种蒸馏扰动策略, 通过寻找使得集成成员输出不一致的输入来揭示多样性. 作者发现用这种扰动样本蒸馏出的模型确实表现出更好的多样性.  对于多个教师模型, 作者用ODS (输出多样性采样) 对输入样本添加扰动, 最大程度地提高集成的输出在所生成样本之间的多样性. 教师分别对添加扰动后的样本生成预测, 学生模型可以最大程度地学习教师模型的多样性. 设计的损失函数也很简单, CE + KD   结构:  实验设置: 数据集: CIFAR-10, CIFAR-100, TinyImageNet. 在CIFAR-10上用的是ResNet-32, 在CIFAR-100, TinyImageNet上用的是WideResNet-28x10. @inproceedings{nam2021diversity, title={Diversity Matters When Learning From Ensembles}, author={Nam, Giung and Yoon, Jongmin and Lee, Yoonho and Lee, Juho}, booktitle={Thirty-Fifth Conference on Neural Information Processing Systems}, year={2021} }  Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective  https://arxiv.org/abs/2102.00650v1\n  $L_{ce}=-y_k\\log \\hat{y}{k,1}^{s}, L{kd}=-\\tau^2\\sum_k\\hat{y}{k,\\tau}^t\\log \\hat{y}{k,\\tau}^s$对两种期望错误率$\\text{error}{ce},\\text{error}{kd}$进行偏置-方差分解. $L_{kd}=L_{kd}-L_{ce}+L_{ce}$, 根据上一条, $L_{kd}-L_{ce}$导致方差减小, $L_{ce}$导致偏置减小. If a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. 定义了 regularization sample: 定义两个量$a=\\dfrac{\\partial L_{ce}}{\\partial z_i}, b=\\dfrac{\\partial(L_{kd}-L_{ce})}{\\partial z_i}$, 对于一个样本, 当$b \u0026gt; a$时, 方差主导了优化的方向, 因此将这个样本定义为regularization sample. 实验表明, regularization sample的数量和训练的效果有一定的关系 (区别在于有没有 label smoothing的效果相差很大, 同时 regularization sample 的数量也相差很大; 但是$\\tau$取不同值时训练效果和 regularization sample 的数量没有明显的关系)  ","permalink":"https://michelia-zhx.github.io/posts/2022-02-24-multi_teacher_knowledge_distillation-2/","summary":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks https://arxiv.org/pdf/2004.05937.pdf Learning from Multiple Teacher Networks, KDD 2017 Efficient knowledge distillation from an ensemble of teachers. Interspeech 2017: 对teacher的logits取加权平均, 加权平均和student的logit","title":"Paper Notes - Multi-Teacher Knowledge Distillation - 2"},{"content":"Definition  Image Classification: 输入图片, 输出图中目标物体的类别. Object Localization: 输入图片, 输出图中物体的 bounding box. Object Detection: 输入图片, 输出图中物体的 bounding box 和类别.  R-CNN Model Family 采用region proposal methods, 首先生成潜在的bounding boxes, 然后采用分类器 识别这些bounding boxes区域. 最后通过post-processing来去除重复bounding boxes来进行优化.\n这类方法流程复杂, 存在速度慢和训练困难的问题.\nR-CNN R-CNN 由三个部分组成:\n Region Proposal: Generate and extract category independent region proposals. Feature Extractor: Extract feature from each candidate region. Classifier: Classify features as one of the known class.  The feature extractor used by the model was the AlexNet deep CNN that won the ILSVRC-2012 image classification competition. The output of the CNN was a 4,096 element vector that describes the contents of the image that is fed to a linear SVM for classification, specifically one SVM is trained for each known class. 问题是运行很慢, test阶段CNN要从约2000个proposed region上提取特征.\nFast R-CNN 提出了R-CNN的三个限制:\n Training is a multi-stage pipeline. Training is expensive in space and time. Object detection is slow.  Fast R-CNN只训练一个模型去学习物体位置和分类, 而不是一个pipeline, 输入是一个region proposal的集合, 经过deep CNN, 进行特征提取. CNN的结尾是 Region of Interest Pooling Layer (ROI Pooling), 针对输入的candidate 进行特征提取. CNN的输出送入一个FC, 得到两个输出, 一个用于softmax layer预测类别, 另一个用于regression用于生成bounding box.\n这个过程针对每一个region of interest进行循环.\nFaster R-CNN 由两部分组成:\n Region Proposal Network: Convolutional neural network for proposing regions and the type of object to consider in the region. Fast R-CNN: Convolutional neural network for extracting features from the proposed regions and outputting the bounding box and class labels.  RPN网络接受CNN的输出, feature map送入一个小型网络得到许多region proposals, 每个对应一个分类. Region proposals是bounding boxes, 或者说anchor boxes, 后续优化. Class prediction is binary, indicating the presence of an object, or not, so-called \u0026ldquo;objectness\u0026rdquo; of the proposed region.\nYOLO Model Family YOLO 训练单独一个神经网络, 端到端, 接受一个图片直接预测bounding box. 准确率不高但是速度快.\nYOLO首先将图像分为 $$S\\times S$$ 的格子 (grid cell). 如果一个目标的中心落入格子, 该格子就负责检测该目标. (即使一个对象跨越多个网格, 它也只会被分配到其中点所在的单个网格. 可以通过增加更多网格来减少多个对象出现在同一网格单元中的几率.)\n每一个格子 (grid cell) 预测bounding boxes(B)和该boxes的置信值(confidence score). 置信值代表box包含一个目标的置信度. 然后, 我们定义置信值为 $$Pr(Object)*IOU^{truth}_{pred}$$ . 如果没有目标, 置信值为零. 另外, 我们希望预测的置信值和ground truth的intersection over union (IOU)相同.\n每一个bounding box包含5个值: $$x, y, w, h$$ 和confidence. $$(x, y)$$ 代表与格子相关的box的中心. $$(w, h)$$ 为与全图信息相关的box的宽和高. confidence代表预测boxes的IOU和gound truth. (IOU = 交叉面积/联合的面积)\n每个格子(grid cell)预测条件概率值C $$Pr(Class_i|Object)$$ , 概率值C代表了格子包含一个目标的概率, 每一格子只预测一类概率. 在测试时, 每个box通过类别概率和box置信度相乘来得到特定类别置信分数: $$Pr(Class_i|Object)*Pr(Object)*IOU^{truth}{pred} = Pr(Class_i)*IOU^{truth}{pred}$$\n这个分数代表该类别出现在box中的概率和box和目标的合适度. 例子讲解\n当一个目标不止一次被识别, 非极大值抑制可以显着提高YOLO的效果.\nYOLO相对于传统方法有如下有优点：\n 非常快. YOLO预测流程简单, 速度很快. 我们的基础版在Titan X GPU上可以达到45帧/s; 快速版可以达到150帧/s. 因此, YOLO可以实现实时检测. YOLO采用全图信息来进行预测. 与滑动窗口方法和region proposal-based方法不同, YOLO在训练和预测过程中可以利用全图信息. Fast R-CNN检测方法会错误的将背景中的斑块检测为目标, 原因在于Fast R-CNN在检测中无法看到全局图像. 相对于Fast R-CNN, YOLO背景预测错误率低一半. YOLO可以学习到目标的概括信息 (generalizable representation), 具有一定普适性. 我们采用自然图片训练YOLO, 然后采用艺术图像来预测. YOLO比其它目标检测方法 (DPM和R-CNN) 准确率高很多.  YOLOv2 (YOLO9000) and YOLOv3 YOLOv2 model makes use of anchor boxes, pre-defined bounding boxes with useful shapes and sizes that are tailored during training.\nThe choice of bounding boxes for the image is pre-processed using a k-means analysis on the training dataset.\n重要的是, 更改了边界框的预测表示形式, 以允许较小的更改对预测产生较小的影响, 从而产生更稳定的模型. 不是直接预测位置和大小, 而是预测偏移量, 以相对于网格单元移动和重塑预定义的锚框, 并通过逻辑函数对其进行阻尼.\n[YOLOF: You Only Look One-level Feature](CVPR2021: https://arxiv.org/pdf/2103.09460.pdf) Problem: Address optimization problem by utilizing only one-level feature for detection. Two key components, Dilated Encoder and Uniform Matching are proposed and bring considerable improvements.\nPerformance: YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being $$2. 5\\times$$ faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with $$7\\times$$ less training epochs.\nSSD: Single Shot MultiBox Detector  问题: RCNN系列为two-stage方法, 先预先回归一次边框, 再进行骨干网络训练, 所以精度更高, 但速度方面有待提升. YOLO为one-stage方法, 只做一次边框回归和打分, 速度快但对小目标效果差, 对尺寸敏感. 使用one-stage思想, 融入Faster R-CNN中的anchor思想, 做了特征分层提取并以此计算边框回归和分类操作, 因此可以适应多尺度目标的训练和检测任务. 在每个stage中根据feature map的大小按照固定的ratio和scale生成default boxes. 例如conv9的输出feature map为5*5, 每个点默认生成6个box, 因此一张feature map上有5*5*6=150个default boxes, 而后每个default box将生成(c+1+4)维的特征向量, 其中c是类别数, 1代表背景, 4是box的偏移和缩放尺度. SSD的backbone是VGG16, 将最后的fc6和fc7转化成conv6和conv7, 再在之后加上不同尺度的conv8, 9, 10, 11四个卷积网络层. 联合损失函数 $L(x,c,l,g) = \\dfrac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))$, 其中$L_{conf}$代表分类误差, 使用softmax; $L_{loc}$代表回归误差, 采用smooth L1 loss.  $L_{loc}(x,j,g) = \\sum_{i\\in Pos}^N\\sum_{m\\in{cx,cy,w,h}}x_{ij}^ksmooth_{L1}(l_i^m - \\hat{g}_j^m)$ $L_{conf}(x,c) = -\\sum_{x\\in Pos}x_{ij}^p\\log(\\hat{c}{i}^p) - \\sum{i\\in Neg}\\log(\\hat{c}i^0), where\\ \\hat{c}{i}^p = \\dfrac{\\exp(c_i^p)}{\\sum_p\\exp(c_i^p)}$   训练策略:  匹配策略: 第一步是根据最大的overlap将ground truth和default box进行匹配, 第二步是将default boxes与overlap大于某个阈值的fround truth进行匹配. Default Boxes生成器: $S_k = S_{min}+\\dfrac{S_{max}-S_{min}}{m-1}(k-1), k\\in[1,m], ratio: a_r\\in{1,2,\\dfrac{1}{2}, 3, \\dfrac{1}{3}}$ Hard Negative Mining: 根据confidence loss对所有box进行排序, 选取置信度误差较大的top-k作为负样本, 使得正负样本比例控制在1:3之内.    PS: 正负样本怎么用啊啊啊啊\n","permalink":"https://michelia-zhx.github.io/posts/2021-09-12-object_detection/","summary":"Definition Image Classification: 输入图片, 输出图中目标物体的类别. Object Localization: 输入图片, 输出图中物体的 bounding box. Object Detection: 输入图片, 输出图中物体的 bounding box 和类别. R-CNN Model Family 采用region proposal methods, 首","title":"Paper Notes - Object Detection"},{"content":"","permalink":"https://michelia-zhx.github.io/posts/2022-04-03-self_attention/","summary":"","title":"Paper Notes - Self-Attention"},{"content":" Why self-supervised learning: 主要的问题在于获取数据及其标注部分. Definition:  Self-supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one: \u0026ldquo;Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?\u0026rdquo; Replace the human annotation block by creatively exploiting some property of data to set up a pseudo-supervised task. 学到这些特征后, 利用迁移学习将特征在下游的监督任务上进行微调, 从而利用更少的数据进行训练.    Self-Supervised Learning from Image Pattern 1: Reconstruction  Image Colorization: 生成彩色和黑白的图片对, 黑白图片经过encoder-decoder生成预测的图片, 和真是图片计算L2 loss. 如此可以学到图片中的物体及其关联部分以正确上色. Image Superresolution: 基于GAN的SRGAN, 低分辨率的图片经过卷积网络生成高分辨率图片, 计算和原图的MSE. 同时将生成的图片和真实图片送入二分类器, 判断哪个是真实图片, 以此使生成器学到如何生成具有细节的高分辨率图片. Image Impainting: 将图片扣掉一块, 由生成器生成完整的图片, 后续步骤同上. Cross-Channel Prediction: 一张图分成灰度图和颜色信道, 用灰度图预测颜色信道, 用颜色信道预测灰度图, 将预测的两个channel复合成预测的图片, 和原图对比计算loss以改进模型.  Pattern 2: Common Sense Tasks  Image Jigsaw Puzzle: 图像分成9个patch, 按照汉明距离最大的64种打乱方式打乱, 输入是9个patch, 需要预测是按照哪一种方式 (1-64) 打乱的. 这一任务要求模型学习物体里的各部分是如何组合的, 以及物体的形状. Context Prediction: 图中取9个相邻的patch选取中心的patch和周围随意一个patch, 输入和前一条类似的网络, 预测两者位置关系. Geometric Transformation Recognition: 将旋转后的图片输入, 预测旋转了多少度 (0, 90, 180, 270). 这个任务要求模型学习物体的地点, 类型和造型.  Pattern 3: Automatic Label Generation  Image Clustering: 在无标注数据集上对图片进行聚类. Deep Clustering, 图片首先被聚类, 然后这些类才被作为classes. Synthetic Imagery: 用game engine生成人造图片, 和真实的图片送入convnet, 并判断生成的特征是属于人造的or真实存在的场景图片.  Self-Supervised Learning from Video  Frame Order Verification: (不)打乱帧的顺序, 送入convnet, 预测输入顺序是否是正确的.  SimCLR  Contrastive Learning: It attempts to teach machines to distinguish between similar and dissimilar images. 需要构造相似的图片对和不相似的图片对. 一张图片被经过随机变换后和原图构成相似图片对 $(x_i,x_j)$, 两张图片分别送入encoder以获取表示 $(h_i,h_j)$, 在经过非线性全连接层以获得新的表示 $(z_i,z_j)$, 最后判断 $(z_i,z_j)$ 的相似性.  步骤  Self-supervised Formulation  Batch Size = N Transformation function T = random(crop + flip + color jitter + grayscale).   Getting Representation  针对一个batch里的所有图片做随机变换, 共获得2N张图片. 增强后的图片对经过共享参数的encoder生成表示$(h_i, h_j)$. 作者使用ResNet-50, 输出的特征为2048维向量.   Projection Head  Projection Head g(·) = Dense + Relu + Dense $(h_i, h_j)$经过 g 生成新的表示$(z_i, z_j)$, 在他们之间计算相似度.   Tuning Model, 针对N张图片生成的增强后的2N个表示, 进行如下操作  计算cosine相似度, 同一张图增强后相似性高, 其余很低 将2N张图片组成很多对, 用softmax计算两张图片相似的概率, 除了同一张图生成的两个表示很相似, 其余的图片对全部是负例. 且这个方法不需要memory bank, queue等特别的结构. 计算损失: $l(i,j) = -\\log\\dfrac{\\exp(s_{i,j})}{\\sum_{k=1}^{2N}l_{[k!=i]}\\exp(s_{i,k})}$ 一个batch的损失: $L = \\dfrac{1}{2N}\\sum_{k=1}^M[l(2k-1, 2k) + l(2k, 2k-1)]$ 优化这个损失函数可以提升图片表示能力, 是模型学会区分图片是否相似 (如果两张图片属于同一类是否需要更高的相似度? 但是没有标注, 无法判断两张照片是否属于同一类.)    Downstream Tasks PIRL: Pretext-Invariant Representation Learning  存在的问题: 随机变换有可能导致不同的图生成相似的新图. 解决方法: 使同一张图变换后的新图尽可能相似, 不同图变换后的新图尽可能不相似.  PIRL Framework  原图经过一个转换后生成新图, 两张图经过共享参数的convnet $\\theta$ 后得到两个表示$V_I, V_{I^T}$, $V_I$经过projection head $f$后得到新的表示 $f(V_I)$, $V_{I^T}$经过projection head $g$后得到新的表示 $g(V_{I^T})$. 优化损失函数使得两个表示尽量相似, 而$f(V_I)$和memory bank里其他图生成的表示尽量不相似. 用memory bank存储别的图像生成的表示, 以避免更大的batch size 损失函数: $h(f(V_I), g(V_{I^T})) = \\dfrac{\\exp(\\frac{s(f(V_I), g(V_{I^T}))}{\\tau})}{\\exp(\\frac{s(f(V_I), g(V_{I^T})}{\\tau}) + \\sum_{I'\\in D_N}\\exp(\\frac{g(V_{I^T}), s(f(V_{I'})}{\\tau})}$ $L_{NCE}(I, I^t) = -\\log[h(m_I, g(V_{I^t}))] - \\sum_{I'\\in D_N}\\log[1-h(g(V_{I^t}), m_{I'})]$  Self-Labelling Combining clustering and representation learning together to learn both features and labels simultaneously.\n 生成标签然后用标签训练一个模型 由训练好的模型生成新的标签 重复以上操作 以上形成鸡蛋问题, 最初用随机初始化的Alexnet, 在Imageet上评估.  Self-Labelling Pipeline  利用一个随机初始化的模型为增强后的无标签数据生成标签 使用Sinkhorn-Knopp算法将无标签图片聚类, 得到新的标签 利用新生成的标签训练模型上, 利用交叉熵损失优化 重复以上步骤  Sinkhron-Knopp Algorithm: 源于最优运输问题 (仓库, 商店, 运输距离) - 原问题: 将N个样本分到K个类中 - 限制条件: N个样本被均分到K个类中 - 代价矩阵: 将 使用这种分类方式训练出的模型的模型表现 作为这种划分方式的代价. 如果代价很高, 说明需要调整划分方式以接近理想效果.\nFixMatch for Semi-Supervised Learning  Intuition: 数据集中只有少部分标注数据和大部分未标注数据, 先用标注数据有监督训练一个模型. 对于未标注数据, 将所有图片做两种变换, 分别送入之前训练的模型, 由于是同一张图片生成的输入, 因此模型的输出应该相同. 模型结构:  将已标记数据用于训练, 交叉熵损失作为监督学习的损失函数: $l_s = \\dfrac{1}{B}\\sum_{b=1}^BH(p_b, p_m(y|\\alpha(x_b)))$ 伪标记: 对于一张未标记图片, 先对它进行weak augmentation, 输入模型得到伪标签, 再和strongly augmented image的输出对比. 针对未标记数据, 半监督部分损失函数: $l_u = \\dfrac{1}{\\mu B}\\sum_{b=1}^{\\mu B}1(max(q_b\\geq \\tau)H(\\hat{q}_b, p_m(y|A(u_b))))$, $loss = l_s + \\mu l_u$, 其中$\\tau$代表伪标记生成的阈值, 当weakly augmented image的softmax超过阈值则生成伪标记, 并和strongly augmented image的结果计算较差熵. 在最终loss的计算中, 随着训练的进度$\\mu$逐渐增大, 具体可以解释为前期对用少量样本训练出的模型不信任, 后期会逐渐加入对未标记数据的考量.    DeepCluster  主要思想: 图片经过augmentation之后送入convnetm 取分类之前的特征, 经过pca降维, 用kmeans聚类得到各个类别. 将kmeans的结果作为伪标记, 和convnet输出的表示经过分类层得到的结果计算交叉熵.  ","permalink":"https://michelia-zhx.github.io/posts/2021-09-05-self_supervised_learning/","summary":"Why self-supervised learning: 主要的问题在于获取数据及其标注部分. Definition: Self-supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one: \u0026ldquo;Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?\u0026rdquo; Replace","title":"Paper Notes - Self-Supervised Learning"},{"content":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale 在整体的实现上, 原文完全使用原始bert的transformer结构, 主要是对图片转换成类似token的处理, 原文引入了一个patch的概念, 即将输入图片划分为一个个的patch, 然后对于每一个patch转换 (主要是flatten操作), 转换成类似bert的输入结构. 整体的框架如下图所示:   图像块嵌入 (Patch Embeddings)\n标准 Transformer 接受 一维标记嵌入序列 (Sequence of token embeddings) 作为输入. 为处理 2D 图像, 将图像 reshape 为一个展平 (flatten) 的 2D 块序列 $x_p\\in \\mathbb{R}^{N\\times(P^2 \\cdot C)}$, 其中 $H\\times W$ 是原始图像的分辨率, $C$ 是通道数 (RGB 图像 $C=3$), $P^2$ 是每个图像块的分辨率, $N=HW/P^2$ 是产生的图像块数, 即 Transformer 的有效输入序列长度.\nTransformer 在其所有层中使用恒定的隐向量 (latent vector) 大小 $D$, 因此我们将图像块展平, 并使用 FC 层将维度 $P^2 \\cdot C$ 映射为 $D$ 维, 同时保持图像块数 $N$ 不变. 此投影输出称为 图像块嵌入 (Patch Embeddings) (本质就是对每一个展平后的 patch vector $x_p\\in \\mathbb{R}^{N\\times(P^2 \\cdot C)}$ 做一个线性变换 / 全连接层 $E\\in \\mathbb{R}^{(P^2 \\cdot C)\\times D}$, 由 $P^2 \\cdot C$ 维降维至 $D$ 维, 得到 $x_pE\\in \\mathbb{R}^{N\\times D}$), 这好比于 NLP 中的词嵌入 (Word Embeddings).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  class PatchEmbed(nn.Module): \u0026#34;\u0026#34;\u0026#34; Image to Patch Embedding \u0026#34;\u0026#34;\u0026#34; def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768): super().__init__() # (H, W) img_size = to_2tuple(img_size) # (P, P) patch_size = to_2tuple(patch_size) # N = (H // P) * (W // P) num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) self.img_size = img_size self.patch_size = patch_size self.num_patches = num_patches # 可训练的线性投影 - 获取输入嵌入 self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): B, C, H, W = x.shape # FIXME look at relaxing size constraints assert H == self.img_size[0] and W == self.img_size[1], \\ f\u0026#34;Input image size ({H}*{W}) doesn\u0026#39;t match model ({self.img_size[0]}*{self.img_size[1]}).\u0026#34; # (B, C, H, W) -\u0026gt; (B, D, (H//P), (W//P)) -\u0026gt; (B, D, N) -\u0026gt; (B, N, D) # D=embed_dim=768, N=num_patches=(H//P)*(W//P) # torch.flatten(input, start_dim=0, end_dim=-1) # 形参: 展平的起始维度和结束维度  # 可见 Patch Embedding 操作 3 步到位 x = self.proj(x).flatten(2).transpose(1, 2) return x     可学习的嵌入 (Learnable Embedding)\n类似于 BERT 的 [class] token, 此处为图像块嵌入序列预设一个可学习的嵌入$z^0_0=x_{class}$, 该嵌入在 Transformer 编码器输出的状态/特征$z_L^0$用作图像表示$y$. $$ \\begin{aligned} \u0026amp;z_0 = [x_{class}, x_p^1E, x_p^2E, \u0026hellip;, x_p^NE] + E_{pos}\\ \u0026amp;z_l' = MSA(LN(z_{l-1})) + z_{l-1}\\ \u0026amp;z_l = MLP(LN(z_l'))+z_l'\\ \u0026amp;y = LN(z_L^0) \\end{aligned} $$ 无论是预训练还是微调, 都有一个分类头 (Classification Head) 附加在$z_L^0$之后, 从而用于图像分类. 分类头在预训练时由一个单层 MLP 实现, 在微调时由单个线性层实现 (多层感知机与线性模型类似, 区别在于 MLP 相对于 FC 层数增加且引入了非线性激活函数, 例如 FC + GELU + FC 形式的 MLP).\n更明确地, 上式中给长度为$N$的嵌入向量后追加了一个分类向量, 用于训练 Transformer 时学习类别信息. 假设将图像分为 $N$ 个图像块, 输入到 Transformer 编码器中就有 $N$ 个向量, 但该取哪一个向量用于分类预测呢? 一个合理的做法是手动添加一个可学习的嵌入向量作为用于分类的类别向量$x_{class}$, 同时与其他图像块嵌入向量一起输入到 Transformer 编码器中, 最后取追加的首个可学习的嵌入向量作为类别预测结果. 所以, 追加的首个类别向量可理解为其他个图像块寻找的类别信息. 从而最终输入 Transformer 的嵌入向量总长度为$N+1$. 可学习嵌入在训练时随机初始化, 然后通过训练得到, 其具体实现为:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ### 随机初始化 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # shape = (1, 1, D) ### 分类头 (Classifier head) self.head = nn.Linear(self.num_features, num_classes) if num_classes \u0026gt; 0 else nn.Identity() ### 前馈过程 (Forward) B = x.shape[0] # Batch Size # 通过 可学习的线性投影 获取 Input Imgaes 的 Patch Embeddings (实现在 3.1 节) x = self.patch_embed(x) # x.shape = (B, N, D) # 可学习嵌入 - 用于分类 cls_tokens = self.cls_token.expand(B, -1, -1) # shape = (B, 1, D) # 按元素相加 附带 Position Embeddings x = x + self.pos_embed # shape = (B, N, D) - Python 广播机制 # 按通道拼接 获取 N+1 维 Embeddings x = torch.cat((cls_tokens, x), dim=1) # shape = (B, N+1, D)     位置嵌入 (Position Embeddings)\n位置嵌入 $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$ 也被加入图像块嵌入, 以保留输入图像块之间的空间位置信息. 不同于 CNN, Transformer 需要位置嵌入来编码 patch tokens 的位置信息, 这主要是由于自注意力的扰动不变性 (Permutation-invariant), 即打乱 Sequence 中 tokens 的顺序并不会改变结果.\n相反, 若不给模型提供图像块的位置信息, 那么模型就需要通过图像块的语义来学习拼图, 这就额外增加了学习成本. ViT 论文中对比了几种不同的位置编码方案:\n 无位置嵌入 1-D 位置嵌入: 考虑把 2-D 图像块视为 1-D 序列 2-D 位置嵌入: 考虑图像块的 2-D 位置 (x, y) 相对位置嵌入: 考虑图像块的相对位置  最后发现如果 不提供位置编码效果会差, 但其它各种类型的编码效果效果都接近, 这主要是因为 ViT 的输入是相对较大的图像块而非像素, 所以学习位置信息相对容易很多.\nTransformer 原文中默认采用 固定位置编码, ViT 则采用 标准可学习/训练的 1-D 位置编码嵌入, 因为尚未观察到使用更高级的 2-D-aware 位置嵌入能够带来显著的性能提升. 在输入 Transformer 编码器之前直接 将图像块嵌入和位置嵌入按元素相加:\n1 2 3 4 5 6  # 多 +1 是为了加入上述的 class token # embed_dim 即 patch embed_dim self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) # patch emded + pos_embed : 图像块嵌入 + 位置嵌入 x = x + self.pos_embed     Transformer 编码器\nTransformer 编码器 由交替的 多头自注意力层 (MSA, 附录 A) 和 多层感知机块 (MLP, 等式 2, 3) 构成. 在每个块前应用 层归一化 (Layer Norm), 在每个块后应用 残差连接 (Residual Connection).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # MHA class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) # 附带 dropout self.proj_drop = nn.Dropout(proj_drop) def forward(self, x): B, N, C = x.shape qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B, N, C) x = self.proj(x) x = self.proj_drop(x) return x   在 Transformer中, MSA 后跟一个 FFN (Feed-forward network), 其包含 两个 FC 层, 第一个 FC 将特征从维度 变换成 , 第二个 FC 将特征从维度 恢复成 , 中间的非线性激活函数均采用 GeLU (Gaussian Error Linear Unit, 高斯误差线性单元) —— 这实质是一个 MLP (多层感知机与线性模型类似, 区别在于 MLP 相对于 FC 层数增加且引入了非线性激活函数, 例如 FC + GeLU + FC), 实现如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  class Mlp(nn.Module): def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() out_features = out_features or in_features hidden_features = hidden_features or in_features self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() self.fc2 = nn.Linear(hidden_features, out_features) self.drop = nn.Dropout(drop) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x   一个 Transformer Encoder Block 就包含一个 MSA 和一个 FFN, 二者都有 跳跃连接 和 层归一化 操作构成 MSA Block 和 MLP Block, 实现如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # Transformer Encoder Block class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super().__init__() # 后接于 MHA 的 Layer Norm self.norm1 = norm_layer(dim) # MHA self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop) # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here self.drop_path = DropPath(drop_path) if drop_path \u0026gt; 0. else nn.Identity() # 后接于 MLP 的 Layer Norm self.norm2 = norm_layer(dim) # 隐藏层维度 mlp_hidden_dim = int(dim * mlp_ratio) # MLP self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop) def forward(self, x): # MHA + Add \u0026amp; Layer Norm x = x + self.drop_path(self.attn(self.norm1(x))) # MLP + Add \u0026amp; Layer Norm x = x + self.drop_path(self.mlp(self.norm2(x))) return x   集合了 类别向量、图像块嵌入 和 位置编码 三者到一体的 输入嵌入向量 后, 即可馈入Transformer Encoder. ViT 类似于 CNN, 不断前向通过 由 Transformer Encoder Blocks 串行堆叠构成的 Transformer Encoder, 最后 提取可学习的类别嵌入向量 —— class token 对应的特征用于 图像分类.\n  归纳偏置与混合架构\n归纳偏置 (Inductive bias): 注意到, Vision Transformer 的图像特定归纳偏置比 CNN 少得多. 在 CNN 中, 局部性、二维邻域结构 和 平移等效性 存在于整个模型的每一层中. 而在 ViT 中, 只有 MLP 层是局部和平移等变的, 因为自注意力层都是全局的. 二维邻域结构 的使用非常谨慎: 在模型开始时通过将图像切分成块, 并在微调时调整不同分辨率图像的位置嵌入. 此外, 初始化时的位置嵌入不携带有关图像块的 2D 位置的信息, 图像块之间的所有空间关系都必须从头开始学习.\n混合架构 (Hybrid Architecture): 作为原始图像块的替代方案, 输入序列可由 CNN 的特征图构成. 在这种混合模型中, 图像块嵌入投影 $E$ 被用在经 CNN 特征提取的块而非原始输入图像块. 作为一种特殊情况, 块的空间尺寸可以为 $1\\times 1$, 这意味着输入序列是通过简单地将特征图的空间维度展平并投影到 Transformer 维度获得的. 然后, 如上所述添加了分类输入嵌入和位置嵌入, 再将三者组成的整体馈入 Transformer 编码器. 简单来说, 就是先用 CNN 提取图像特征, 然后由 CNN 提取的特征图构成图像块嵌入. 由于 CNN 已经将图像降采样了, 所以块尺寸可为 $1\\times 1$.\n  ","permalink":"https://michelia-zhx.github.io/posts/2022-04-04-vit/","summary":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale 在整体的实现上, 原文完全使用原始bert的transformer结构, 主要是对图片转换成类似token的处理, 原文引","title":"Paper Notes - Vision Transformer"},{"content":"Mining Typhoon Knowledge with Neural Networks \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999  需解决的问题: 神经网络的两个缺点 \u0026ndash; 数据量大, 训练时间长; 神经网络对知识的学习果不能直接用于决策. Fast neural model - FTART (Firld Theory based Adaptive Resonance Theory): 隐层的激活函数是Sigmoid函数, 输入层和第二层之间使用Gaussian权重, 并更新. 第二层用于分类输入,第三层用于分类输出, 在这两层之间建立关系来进行有监督学习. Rule extraction algorithm - SPT (Statistic based Producing and Testing):  用大量实例来训练一个神经网络 结合输入和神经网络的输出来构造一个虚拟示例集, 如果存在多个输入分量的组合, 且投影到它时等价的示例属于某个类的概率为$\\lambda$, 则通过将组合作为前因, 类别作为后继来构造规则. 如果没有这样的组合, 则选择具有最佳聚类效果的连续输入组件并离散化. (?) 如何去噪: (?).    FANRE: A Fast Adaptive Neural Regression Estimator \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999  Adaptive Resonance Theory: 像是一个KNN网络, 可以实现神经元的动态扩充. 对于新来的样本, 和之前的聚类中心进行比较, 如果符合阈值设定(形成共振), 则单独训练与之匹配的聚类中心对应的神经元的相关链接权重, 其他神经元保持不变; 如果所有神经元都不匹配, 那就创造一个新的神经元分配给这个数据形成新的一类. FANRE的结构: 输入层, 输出层, 中间两层隐层. 最初隐层为空, 随输入动态添加隐层结点. 增量学习, 每个样例只过一遍.  两个参数$\\theta_{ij}, \\alpha_{ij}$, 代表从第一层的$unit_i$到第二层的$unit_j$的高斯权重的响应中心和响应特征宽度. $Err_{max}$: 最大容错值, $Vig_1$: first-degree vigilance, $Vig_2$: second-degree vigilance. 且有$Err_{max} \u0026lt; Vig_1 \u0026lt; Vig_2$   FANRE的学习流程: 输入新样例 $\\Rightarrow$ 第二层竞争 $\\Rightarrow$ 第三层竞争 $\\Rightarrow$ 计算网络输出:(*)  $Err \u0026lt; Err_{max}$ [已有的attracting basin能覆盖当前样例, 不需要调整] $\\Rightarrow$ 下一个样例(**) $Err \\geq Err_{max}$:  $Err_{max} \\leq Err \u0026lt; Vig_1$ [虽然总体的近似表现不尽如人意, 但内部对输入输出模式的近似还是可用的] $\\Rightarrow$ 调整$\\theta_{ij}'$和$\\alpha_{ij}$, 回到(*) $Vig_1 \\leq Err \u0026lt; Vig_2$ [由结点$u$表示的对输出的近似可用, 但由第二层结点表示的对输入的近似不合适] $\\Rightarrow$ 第二层添加一个结点, 回到(**) $Err \\geq Vig_2$ [由结点$u$表示的对输出的近似, 和由第二层结点表示的对输入的近似均不合适] $\\Rightarrow$ 第二层和第三层各添加一个结点, 回到(**)      Ensemble of GA based Selective Neural Network Ensembles \u0026ndash; Jian-Xin Wu, Zhi-Hua Zhou, Zhao-Qian Chen - 2002 $\\textbf{GASEN}$\n $N$个基学习器$f_i:\\mathbf{R}^m\\rightarrow\\mathbf{R}^n$加权: $\\overline{f}(x) = \\sum_{i=1}^Nw_if_i(x)$ 设对于输入$x$, 期望输出为$d(x)$, 则基学习器和ensemble的误差为 $$E_i(x) = (f_i(x)-d(x))^2, E(x) = (\\overline{f}(x)-d(x))^2$$ 设$x$服从分布$p(x)$, 则基学习器和ensemble在分布上的泛化误差为 $$E_i = \\int p(x)E_i(x)\\rm{d}x, E = \\int p(x)E(x)\\rm{d}x$$ 平均误差为$\\overline{E}(x) = \\sum_{i=1}^Nw_iE_i(x)$, 平均泛化误差为$\\overline{E} = \\int p(x)\\overline{E}(x)\\rm{d}x$ Ambiguity of the i-th learner on input $x$: $$A_i(x) = (f_i(x)-\\overline{f}(x))^2, A_i = \\int p(x)A_i(x)\\rm{d}x$$ $$\\overline{A}(x)=\\sum_{i=1}^Nw_iA_i(x), \\overline{A} = \\int p(x)\\overline{A}(x)\\rm{d}x$$ 定义ensemble的泛化性能: $E = \\overline{E} - \\overline{A}$ 两个基学习器的相关性: $$C_{ij} = \\int p(x)(f_i(x)-d(x))(f_j(x)-d(x))\\rm{d}x$$ $$E = \\sum_{i=1}^N\\sum_{j=1}^Nw_iw_jC_{ij}$$ 当对于第$k$个基学习器, 满足 $$(2N-1)\\sum_{i=1,i\\neq k}^N\\sum_{j=1,j\\neq k}^NC_{ij} \u0026lt; 2(N-1)^2\\sum_{i=1,i\\neq k}^NC_{ik} + (N-1)^2E_k$$  $\\textbf{e-GASEN}$ 先用GASEN算法训几个ensembles, 再用简单的集成算法把这几个ensembles集成起来.\nHybrid Decision Tree \u0026ndash; Zhi-Hua Zhou, Zhao-Qian Chen, 2002  处理有序属性 \u0026ndash; 定量分析; 无序属性 \u0026ndash; 定性分析. 结合symbolic leanring (无序属性) 和 neural learning (有序属性). 树的扩展: 将属性集分为无序属性$\\mathcal{L}_0$和有序属性$\\mathcal{L}_1$, 先将HDT按照无序属性扩展, 当树的分支因为结点里的样本属于同一类别而无法扩展时, 终止扩展; 当因为结点中无序属性均被使用过而无法扩展时, 将这个叶结点标记为neural node. Neural Processing: 落入neural node的样本被连续属性集重新表示, 并被归一化. 然后使用FANNC做分类 (增量学习) 几种增量学习:  E-IL (Example-Incremental Learning) —— 新的样本到来时, 保证学到新知识的同时不要牺牲过多的旧知识. 非增量学习方法会有灾难性遗忘的缺陷. C-IL (Class-Incremental Learning) —— 当新样例属于新类时, 学到新的知识且不用牺牲太多旧的知识 (例如重新学习整个系统) A-IL (Attribute-Incremental Learning) —— 当新样例带有新属性时, 学到新的知识且不用牺牲太多旧的知识 (例如重新学习整个系统)    Face recognition with one training image per person \u0026ndash; Jian-Xin Wu, Zhi-Hua Zhou, 2002  人脸识别算法主要有两种类别: geometric feature-based and template-based techniques. PCA属于后者, 但没有考虑标签信息; 而考虑标签信息的每个类别至少需要两张图片 (LDA). $(PC)^2A$ —— projection-combined principal component analysis: 针对每张图片对其进行变换  $x\\in [1,N_1], y\\in [1,N_2], P(x,y)\\in [0,1]$, $P(x,y)$是灰度图 $V_P(x) = \\sum_{y=1}^{N_2}P(x,y), H_P(y) = \\sum_{x=1}^{N_1}P(x,y)$ $\\overline{P} = \\dfrac{\\sum_{x=1}^{N_1}\\sum_{y=1}^{N_2}P(x,y)}{N_1N_2}, M_P(x,y) = \\dfrac{V_P(x)H_P(y)}{N_1N_2\\overline{P}}$ $P_{\\alpha}(x,y) = \\dfrac{P(x,y)+\\alpha M_P(x,y)}{1+\\alpha}$ $P_{\\alpha}'(x,y) = \\dfrac{P_{\\alpha}(x,y) - \\min(P_{\\alpha}(x,y))}{\\max(P_{\\alpha}(x,y)) - \\min(P_{\\alpha}(x,y))}$   最后在projection-combined version of image $P_{\\alpha}(x,y)$上使用$PCA$.  Learning a Rare Event Detection Cascade by Direct Feature Selection \u0026ndash; Jianxin Wu, James M.Rehg, Matthew D.Mullin, 2003  人脸检测是稀有事件检测的典型例子, 给一些人脸大小的图片, 其中很少的部分会包含人脸, target patterns occur with much lower frequency than non-targets. 搜索-分类: 搜索图片中可能的区域, 再判别是否包含脸. Viola-Jones framework 包含三个元素: 层叠式结构, 一些长方形特征, 基于AdaBoost的算法 - 在每个分类器中构造长方形特征的ensemble. 每个分类器拒绝一部分不包含人脸的区域, 并使包含人脸的通过. 在每个结点, 给定一个训练集${x_i,y_i}$, 训练目标是从总共$F$个特征中选出一些弱分类器${h_t}$, 集成的分类器$H_i$需要有很高的检测率$d_i$和中等的假正率$f_i$, 则整个层叠式模型的检测率$d = \\prod_{i=1}^nd_i$和假正率$f = \\prod_{i=1}^nf_i$, 可以保证有较高的检测率和很低的假正率. 第$t$轮boosting后, ensemble表示为 $$H(x) = \\left{\\begin{aligned} 1\\quad \u0026amp; \\sum_{t=1}^T\\alpha_th_t(x) \\geq \\theta\\ 0\\quad \u0026amp; otherwise \\end{aligned}\\right.$$ 训练一个结点的过程:  训练所有的弱分类器, (*)判断是否$d \u0026gt; D?$  yes $\\Rightarrow$ 添加这个特征以最小化ensemble的假正率 no $\\Rightarrow$ 添加这个特征以最大化ensemble的检测率 (以上的最大化和最小化均通过穷举法完成, 选择加入ensemble后能给ensemble带来最大提升的classifier)   如果 $f \\geq F or d \\leq D$ $\\Rightarrow$ 返回(*)   和Viola-Jones相比, 本算法在每个结点每个弱学习器之训练一次, 而Viola-Jones算法中每个弱学习器每针对一个特征就要训练一次.(?)  A Scalable Approach to Activity Recognition based on Object Use \u0026ndash; Jianxin-Wu, Adebola Osuntogun, Tanzeem Choudhury, ICCV 2006  使用动态贝叶斯网络 (Dynamic Bayesian Network), 从视频中稀疏且有噪声的RFID传感器数据和一些活动的常识学习训练模型. Object-use Based Activity Recognition:  $A^t, O^t, R^t, V^t$分别代表活动, 使用的物体, RFID和视频帧. DBN具有的参数: 先验$P(A^1)$, 观测模型$P(O^1|A^1), P(O^{t+1}|O^{t}, A^{t+1})$, 状态转移模型$P(A^{t+1}|A^{t})$, 输出模型$P(V^t|O^t), P(R^t|O^t)$ 如何确定正在使用的物体 —— 除了RFID传感器数据, 借助视频, 将像素组成$8\\times 8$的superpixels, 对比当前帧$t$和$t-3$$t+3$两帧的superpixels, 计算差距, 若差距均超过阈值则将其中的物体认定为正在使用的物体. 将segmented area中提取SIFT特征, 将视频帧看成的集合$V^t = (v^t_1,v^t_2,\u0026hellip;,v^t_{n^t})$, 其中任两个SIFT特征相互独立, 且$P(V^t|O^t) = \\prod_{i=1}^{n^t}P(v^t_i|O^t) = \\prod_{i=1}^{n^t}\\mathbf{h}_{O^t}(v_i^t)$   Learning object models w/o human labeling  在EM算法中使用RFID readings和common knowledge去学习object models. E步: 估计给定$R^t, V^t, \\mathbf{h}_{O^t}(v_i^t)$时$O^t$的边际概率 用standard junction tree算法估计每一个时刻$O^t$的边际概率 给定$O^t$的边际概率, $V^t$和$A^t$独立 M步: 计数, 更新$\\mathbf{h}_{O^t}(v_i^t)$   Specify parameters from domain knowledge  不想细看了    ","permalink":"https://michelia-zhx.github.io/posts/2021-07-15-pros_paper_notes/","summary":"Mining Typhoon Knowledge with Neural Networks \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999 需解决的问题: 神经网络的两个缺点 \u0026ndash; 数据量大, 训练时间长; 神经网络对知识的学习果不能直接用于决策. Fast neural model - FTART (Firld Theory","title":"Paper Notes - Week 1"},{"content":"Multi-Instance Multi-Label Learning with Application to Scene Classification \u0026ndash; Zhi-Hua Zhou, Min-Ling Zhang, NIPS 2006  Multi-instance: 一个example包含多个instance, example只对应1个label; Multi-label: 一个example对应多个label. 以Multi-instance或Multi-label为桥梁, 将multi-instance multi-label learning ($f_{MIML}: 2^{\\mathcal{X}}\\rightarrow 2^{\\mathcal{Y}}$) 转化为传统机器学习任务.  方法一 (通过Multi-instance): 先转化为$f_{MIL}:2^{\\mathcal{X}}\\times \\mathcal{Y}\\rightarrow {-1. +1}$, 再转化为$f_{SISL}: \\mathcal{X}\\times\\mathcal{Y}\\rightarrow{-1, +1}$, $f_{MIL}(X_i, y) = sign[\\sum_{i=1}^{n_i}f_{SISL}(x_{j}^{(i)}, y)]$ 方法二 (通过Multi-label): 先转化为$f_{MLL}:\\mathcal{Z}\\rightarrow 2^{\\mathcal{Y}}$, 对于任意$z_i\\in \\mathcal{Z}, f_{MLL}(z_i) = f_{MIML}(X_i)$ if $z_i\\in\\phi(X_i), \\phi: 2^{\\mathcal{X}}\\rightarrow\\mathcal{Z}$, 再转化为$f_{SISL}:\\mathcal{Z}\\times\\mathcal{Y}\\rightarrow{-1, +1}$, $f_{MLL}(z_i) = {y|\\arg_{y\\in\\mathcal{Y}}[f_{SISL}(z_i,y)=+1}$   MINIBOOST (在每轮boosting中试图将$\\mathcal{F}(B)$扩展为$\\mathcal{F}(B)+cf(B)$)  将每个MIML样本 $(X_u, Y_u)$ 转化为 $|\\mathcal{Y}|$ 个multi-instance bags ${[(X_u,y_1), \\Psi(X_u,y_1)],\u0026hellip;,[(X_u,y_{|\\mathcal{Y}|}), \\Psi(X_u,y_{|\\mathcal{Y}|})]}$ 初始化每个bag的权重$W^{(i)} = \\dfrac{1}{m\\times |\\mathcal{Y}|}$ 对于迭代次数$t=1,2,\u0026hellip;,m\\times |\\mathcal{Y}|$:  令$W_j^{(i)} = W^{(i)}/n_i$, 将bag 的label $\\Psi(X^{(i)},y^{(i)})$赋给其中的instance $(x_j^{(i)}, y^{(i)})$， 训练一个 instance-level的学习器 $h_t[(x_j^{(i)}, y^{(i)})]$. 对第 $i$个bag, 计算其错误率, $e^{(i)} = \\dfrac{\\sum_{i=1}^{n_i}\\llbracket{h_t[(x_j^{(i)}, y^{(i)})]\\neq\\Psi(X^{(i)},y^{(i)})}\\rrbracket}{n_i}$ 若 $e^{(i)} \u0026lt; 0.5$ 对所有 $i\\in [1,2,\u0026hellip;,m\\times|\\mathcal{Y}| ]$, 则跳出循环 计算$c_t=\\argmin_{c_t}\\sum_{i=1}^{m\\times|\\mathcal{Y}}W^{(i)}\\exp[(2e^{(i)}-1)c_t]$ 如果$c_t \u0026lt; 0$, 则跳出循环 令 $W^{(i)} = W^{(i)}\\exp[(2e^{(i)}-1)c_t]$, 重新正则化使得 $0\\leq W^{(i)}\\leq 1$且$\\sum_{i=1}^{m\\times|\\mathcal{Y}| }W^{(i)} = 1$   返回 $Y^* = {y|\\arg_{y\\in\\mathcal{Y}}sign(\\sum_j\\sum_tc_th_t[(x_j^,Y)])=+1}$ ($x_j^$是$X^*$的instance) P.S. 损失函数为: $$\\begin{aligned} E_{\\mathcal{B}}E_{\\mathcal{G}|\\mathcal{B}}[\\exp(-g\\mathcal{F}(B)+c(-g\\mathcal{F}(B)))] \u0026amp;= \\sum_i W^{(i)}\\exp[c\\left(-\\dfrac{g^{(i)}\\sum_jh(\\mathbf{b}_j^{(i)})}{n_i}\\right)]\\ \u0026amp;= \\sum_iW^{(i)}\\exp[(2e^{(i)}-1)c] \\end{aligned}$$   MINISVM  对MIML样本 $(X_u, Y_u), \\Gamma = {X_u|u=1,2,\u0026hellip;,m}$ 从$\\Gamma$中随即选择$k$个元素初始化$M_t$, 重复以下直到$M_t$不再变化:  $\\Gamma_t = {M_t}$ (t=1,2,\u0026hellip;,k) 对于每一个 $X_u\\in (\\Gamma-{M_t|t=1,2,\u0026hellip;,k})$:  $index = \\argmin_{t\\in{1,\u0026hellip;,k}}d_H(X_u, M_t), \\Gamma_{index} = \\Gamma_{index} \\cup {X_u}$   $M_t = \\argmin_{A\\in\\Gamma_t}\\sum_{B\\in\\Gamma_t}d_H(A, B)$ (t=1,2,\u0026hellip;,k) 将 $(X_u, Y_u)$ 转化为 multi-instance 样本 $(z_u, Y_u), z_u = (z_{u1}, z_{u2}, \u0026hellip;, z_{uk}) = (d_H(X_u, M1), d_H(X_u, M_2),\u0026hellip;,d_H(X_u,M_k))$ 对于每个$y\\in\\mathcal{Y}$, 生成一个数据集 $\\mathcal{D} = {(z_u,\\Phi(z_u,y))|u=1,2,\u0026hellip;,m}$, 然后训练一个SVM $h_y = SVMTrain(\\mathcal{D}_y)$ 返回 $Y^* = {\\argmax_{y\\in\\mathcal{Y}}h_y(z^)}\\cup{y|h_y(z^) \\geq 0, y\\in\\mathcal{Y}}$ P.S. Hausdorff distance: $d_H(A,B) = \\max{\\max_{a\\in A}\\min_{b\\in B}|\\mathbf{a} - \\mathbf{b}|, \\max_{b\\in B}\\min_{a\\in A}|\\mathbf{b} - \\mathbf{a}|}$      On Multi-Class Cost-Sensitive Learning \u0026ndash; Zhihua Zhou, Xuying Liu, AAAI 2006  rescale classes 针对二分类问题有很好的效果, 但是针对多分类代价敏感问题效果不好. 原理: rescale让代价不敏感的算法变得代价敏感, 对于二分类问题, $p = P(class=1|\\mathbf{x})$, 作出最优选择的阈值$p^$满足 $$p^\\times\\epsilon_{11}+(1-p^)\\times\\epsilon_{21} = p^\\times\\epsilon_{12}+(1-p^*)\\times\\epsilon_{22}$$ Elkan Theorem: 对应原先的概率阈值$p_0$设置新的阈值$p^$, 则第二类的个数应乘以$\\dfrac{p^}{1-p^*}\\dfrac{1-p_0}{p_0}$. 推广到多分类, rescale时$i$类相比于$j$的比例是$\\tau_{opt}(i,j)=\\dfrac{\\epsilon_{ij}}{\\epsilon_{ji}}, \\epsilon_i = \\sum_{j=1}^c\\epsilon_{ij}, w_i = \\dfrac{(n\\times\\epsilon_i)}{\\sum_{k=1}^cn_k\\times\\epsilon_k}, w_i$是赋给每个类样本个数的权重. 传统rescale方法赋的比例$\\tau(i,j) = \\dfrac{w_i}{w_j} = \\dfrac{\\epsilon_i}{\\epsilon_j}$, 和上一条相比只在$c=2$时一样, 其余情况不等, 因此传统rescale方法针对多分类情况会不适用. The $RESCALE_{new}$ Approach $$\\begin{aligned} \\dfrac{w_1}{w_2} = \\dfrac{\\epsilon_{12}}{\\epsilon_{21}}, \\dfrac{w_1}{w_3} = \\dfrac{\\epsilon_{13}}{\\epsilon_{31}}, \u0026hellip;, \\dfrac{w_1}{w_c} \u0026amp;= \\dfrac{\\epsilon_{1c}}{\\epsilon_{c1}}\\ \\dfrac{w_2}{w_3} = \\dfrac{\\epsilon_{23}}{\\epsilon_{32}}, \u0026hellip;, \\dfrac{w_2}{w_c} \u0026amp;= \\dfrac{\\epsilon_{2c}}{\\epsilon_{c2}}\\ \u0026hellip;\\ \\ \\ \\ \\ \\ \\ \\ \u0026hellip;\\ \\ \\ \\ \\ \\ \\ \u0026amp;\\ \u0026hellip;\\ \\dfrac{w_{c-1}}{w_c} \u0026amp;= \\dfrac{\\epsilon_{c-1,c}}{\\epsilon_{c,c-1}} \\end{aligned}$$ 可以构造出一个$c$元一次方程组.  Where am I: Place instance and category recognition using spatial $\\tt{PACT}$ \u0026ndash; Jianxin Wu, James M.Rehg, 2008, CVPR  \u0026ldquo;Where am I\u0026rdquo; 问题: 经典的机器人问题, recognize instances and categories of places or scenes. 在视觉中常被处理为场景识别问题 (场景的类别而不是具体位置) PACT: Principal component Analysis of Census Transform histograms.  CT: 一个没有参数的局部转换方法, 可以建立局部的相关性. 将图片转换成灰度图, 每九个像素($3\\times 3$), 将中间的像素灰度和周围8个比较(比较结果用0/1)表示, 构成一个8位的二进制数, 转换成十进制数后作为CT代替原来的像素. CT的传递性保证了相距很远的像素对应的CT也有相关性. 再构造直方图统计不同CT值的数量 最后使用PCA计算对应特征值最大的特征向量, 对应主成分, 即图片中最主要的形状.   Spatial PACT: 将图片分成小份, 在区域中其中计算相关性. 构造 \u0026ldquo;spatial pyramid\u0026rdquo;, 0,1,2-level对应不同的划分方式.  Semi-Supervised Learning with Very Few Labeled Training Examples  传统半监督学习算法需要先通过少量标注数据训练一个初始弱学习器$h$, 再利用$h$和无标注数据. 但是在现实中很多任务能获得的标注数据很少, 比如基于内容的图像检索(搜索相似图片)或在线网页推荐(只拥有一个用户感兴趣的页面) OLTV (learning with One Labeled example and Two Views):  CCV (Canonical correlation analysis): 定义两个视角间的相关投影. $X=(x_0,x_1,\u0026hellip;,x_{l-1}), Y=(y_0,y_1,\u0026hellip;,y_{l-1})$, 寻找两个基向量集合, 以最大化两个视角向量分别在基向量$w_x,w_y$上投影的相关性: $$\\mathop{\\arg\\min}\\limits_{w_x,w_y} \\left(\\dfrac{w^T_xC_{xy}w^T_y}{\\sqrt{w^T_xC_{xx}w^T_x\\cdot w^T_yC_{yy}w^T_y}}\\right)\\ w^T_xC_{xx}w^T_x=1, w^T_yC_{yy}w^T_y=1$$ 最终可以得到一个线性关系$C_{xy}C_{yy}^{-1}C){yx}w_x=\\lambda^2C_{xx}w_x$. 也可用核函数映射到高维, 目标函数变为 $$\\mathop{\\arg\\min}\\limits_{\\alpha, \\beta}\\dfrac{\\alpha^TS_xS_x^TS_yS_y^T\\beta}{\\sqrt{\\alpha^TS_xS_x^TS_xS_x^T\\alpha\\cdot\\beta^TS_yS_y^TS_yS_y^T\\beta}}$$ 两个核矩阵为$K_x=S_xS_x^T, K_y=S_yS_y^T$, $\\alpha, \\beta$可以由$(K_x+\\kappa I)^{-1}K_y(K_y+\\kappa I)^{-1}K_x\\alpha = \\lambda^2\\alpha, \\beta = \\dfrac{1}{\\lambda}(K_y+\\kappa I)^{-1}K_x\\alpha$算出. 因此对于所有的$(x^,y^)$, 投影可以利用$\\alpha, \\beta$算出. 继而算出新样本和旧样本的投影的相似度, 将最像的作为新的正例, 最不像的作为负例.    ","permalink":"https://michelia-zhx.github.io/posts/2021-07-22-pros_paper_notes/","summary":"Multi-Instance Multi-Label Learning with Application to Scene Classification \u0026ndash; Zhi-Hua Zhou, Min-Ling Zhang, NIPS 2006 Multi-instance: 一个example包含多个instance, example只对应1个label; Multi-label: 一个example对应多个","title":"Paper Notes - Week 2"},{"content":"1 快排 1.1 快排 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1000010; int a[N]; void quicksort(int a[], int l, int r){ if (l \u0026gt;= r) return; int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1]; while (i \u0026lt; j){ do i ++ ; while (a[i] \u0026lt; x); do j -- ; while (a[j] \u0026gt; x); if (i \u0026lt; j) swap(a[i], a[j]); } quicksort(a, l, j); quicksort(a, j+1, r); } int main(){ int n; cin \u0026gt;\u0026gt; n; for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); } quicksort(a, 0, n-1); for(int i=0; i\u0026lt;n; i++) printf(\u0026#34;%d \u0026#34;, a[i]); return 0; }   1.2 第k大的数  执行完partition操作后，设左边的长度为L , 枢轴点的位置为p (将枢轴元素视为属于左部即p==L)  若L == k，则枢轴点v 即为第k 大数 若k \u0026lt; L, 则在左部求其第k大数 若k \u0026gt; L, 则在右部求其第k − L大数    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1000010; int a[N]; int quicksort(int a[], int l, int r, int k){ if (l \u0026gt;= r){ return a[l]; } int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1]; while (i \u0026lt; j) { do i ++ ; while (a[i] \u0026lt; x); do j -- ; while (a[j] \u0026gt; x); if (i \u0026lt; j) swap(a[i], a[j]); } if (j - l + 1 \u0026gt;= k){ return quicksort(a, l, j, k); } else return quicksort(a, j + 1, r, k - (j - l + 1)); } int main(){ int n, k; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; for(int i=0; i\u0026lt;n; i++) cin \u0026gt;\u0026gt; a[i]; cout \u0026lt;\u0026lt; quicksort(a, 0, n-1, k) \u0026lt;\u0026lt; endl; return 0; }   2 归并排序 2.1 归并排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1e6 + 10; int a[N], tmp[N]; void merge_sort(int q[], int l, int r){ if (l \u0026gt;= r) return; int mid = l + r \u0026gt;\u0026gt; 1; merge_sort(q, l, mid), merge_sort(q, mid + 1, r); int k = 0, i = l, j = mid + 1; while (i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= r) if (q[i] \u0026lt;= q[j]) tmp[k ++ ] = q[i ++ ]; else tmp[k ++ ] = q[j ++ ]; while (i \u0026lt;= mid) tmp[k ++ ] = q[i ++ ]; while (j \u0026lt;= r) tmp[k ++ ] = q[j ++ ]; for (i = l, j = 0; i \u0026lt;= r; i ++, j ++ ) q[i] = tmp[j]; } int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 0; i \u0026lt; n; i ++ ) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); merge_sort(a, 0, n - 1); for (int i = 0; i \u0026lt; n; i ++ ) printf(\u0026#34;%d \u0026#34;, a[i]); return 0; }   2.2 逆序对的数量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;using namespace std; const int N = 100010; int a[N]; int nums; unsigned long result = 0; void merge_sort(int a[], int l, int r){ if (l \u0026gt;= r) return; int mid = (l+r) \u0026gt;\u0026gt; 1; merge_sort(a, l, mid); merge_sort(a, mid + 1, r); int temp[r - l + 1]; int lptr = l; int rptr = mid + 1; int tempptr = 0; while(lptr \u0026lt;= mid \u0026amp;\u0026amp; rptr \u0026lt;= r){ if(a[lptr] \u0026lt;= a[rptr]) temp[tempptr++] = a[lptr++]; else { temp[tempptr++] = a[rptr++]; result += (mid - lptr + 1); } } while (lptr \u0026lt;= mid) temp[tempptr++] = a[lptr++]; while (rptr \u0026lt;= r) temp[tempptr++] = a[rptr++]; for (int i = l, j = 0; i \u0026lt;= r; i ++, j ++){ a[i] = temp[j]; } } int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;nums); for(int i = 0; i \u0026lt; nums; i++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); merge_sort(a, 0, nums-1); cout \u0026lt;\u0026lt; result; return 0; }   3 二分 3.1 数的范围  对于每个查询，返回一个元素 k 的起始位置和终止位置  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #include \u0026lt;iostream\u0026gt;using namespace std; const int maxn = 100005; int n, q, x, a[maxn]; int main() { scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n, \u0026amp;q); for (int i = 0; i \u0026lt; n; i++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); while (q--) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); int l = 0, r = n - 1; while (l \u0026lt; r) { int mid = l + r \u0026gt;\u0026gt; 1; if (a[mid] \u0026lt; x) l = mid + 1; else r = mid; } if (a[l] != x) { printf(\u0026#34;-1 -1\\n\u0026#34;); continue; } int l1 = l, r1 = n; while (l1 + 1 \u0026lt; r1) { int mid = l1 + r1 \u0026gt;\u0026gt; 1; if (a[mid] \u0026lt;= x) l1 = mid; else r1 = mid; } printf(\u0026#34;%d %d\\n\u0026#34;, l, l1); } return 0; }   3.2 数的三次方根 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #include\u0026lt;iostream\u0026gt;using namespace std; double n,l,r,mid; double q(double a){return a*a*a;} int main(){ cin \u0026gt;\u0026gt; n; l=-100, r=100; while(r - l \u0026gt;= 1e-7){ mid=(l+r)/2; if (q(mid)\u0026gt;=n) r=mid; else l=mid; } printf(\u0026#34;%.06f\u0026#34;, l); return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-01-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%951/","summary":"1 快排 1.1 快排 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1000010; int a[N]; void quicksort(int a[], int l, int r){ if (l \u0026gt;= r) return; int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1];","title":"基础算法(一)"},{"content":"1 双指针  两个指针指向两个序列: 归并排序 两个指针指向一个序列: 快排  1.1 最长不重复子序列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include\u0026lt;iostream\u0026gt;#include\u0026lt;vector\u0026gt;#include\u0026lt;algorithm\u0026gt;using namespace std; const int maxn = 1e6+10; int a[maxn], q[maxn]; int n, ans=0; int main(){ cin \u0026gt;\u0026gt; n; for(int i=0;i\u0026lt;n;i++) cin \u0026gt;\u0026gt; a[i]; for(int i=0,j=0;i\u0026lt;n;i++){ q[a[i]]++; while(q[a[i]]\u0026gt;1){ q[a[j]]--; j++; } ans=max(ans,i-j+1); } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; endl; }   2 位运算  n的二进制表示中第k位是几  把第k位移到最后一位 n \u0026raquo; k   lowbit(x): 返回x二进制的最后一位1 (返回值是一个二进制数)  x \u0026amp; (-x) = x \u0026amp; (~x+1) x = 1010\u0026hellip;10000, ~x=0101\u0026hellip;01111, ~x+1=0101\u0026hellip;10000   n的二进制表示中有多少个1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include\u0026lt;iostream\u0026gt;using namespace std; int lowbit(int x){ return x \u0026amp; -x; // 返回x二进制的最后一位1 (返回值是一个二进制数) } int main(){ int n; cin \u0026gt;\u0026gt; n; // 多次测试  while(n--){ int x; cin \u0026gt;\u0026gt; x; int res = 0; while (x){ x -= lowbit(x); // 把最后一位1去掉  res ++; } printf(\u0026#34;%d \u0026#34;, res); } return 0; }   3 离散化 3.1 离散化  整数序列a, 值域大但个数少, 将他们映射到从0开始的数  a中可能有重复元素(去重) 如何算出a[i]映射(离散化后)的数(二分)    1 2 3 4 5 6 7 8 9 10 11 12 13  vector\u0026lt;int\u0026gt; alls; sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); int find(int x){ int i=0, r=alls.size()-1; while (l \u0026lt; r){ int mid = (l+r)/2; if (alls[mid] \u0026gt;= x) r = mid; else l = mid+1; } return r+1; // }   3.2 区间和  首先进行 n 次操作, 每次操作将某一位置 x 上的数加 c. 接下来, 进行 m 次询问, 每个询问包含两个整数 l 和 r, 你需要求出在区间 [l,r] 之间的所有数的和.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  #include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;algorithm\u0026gt;using namespace std; typedef pair\u0026lt;int, int\u0026gt; PII; const int N=300100; int a[N], s[N]; vector\u0026lt;int\u0026gt; alls; vector\u0026lt;PII\u0026gt; add, query; int find(int x){ int l=0, r=alls.size()-1; while (l \u0026lt; r){ int mid = (l+r)/2; if (alls[mid] \u0026gt;= x) r = mid; else l = mid+1; } return r+1; // } int main(){ int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; while(n--){ int x, c; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; c; add.push_back({x, c}); alls.push_back(x); } while(m--){ int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; query.push_back({l, r}); alls.push_back(l); alls.push_back(r); } //去重  sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); for (auto item: add){ int x = find(item.first); a[x] += item.second; } for(int i = 1; i \u0026lt;= alls.size(); i ++ ) s[i] = s[i - 1] + a[i]; for (auto item: query){ int l = find(item.first), r = find(item.second); cout \u0026lt;\u0026lt; s[r] - s[l-1] \u0026lt;\u0026lt; endl; } return 0; }   4 区间合并  返回n个区间合并之后的个数  按左端点排序 分新区间与前一个区间关系的三种情况更新区间列表    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  #include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;algorithm\u0026gt;using namespace std; typedef pair\u0026lt;int, int\u0026gt; PII; vector\u0026lt;PII\u0026gt; p1, p2; int main(){ int n; cin \u0026gt;\u0026gt; n; while(n--){ int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; p1.push_back({l,r}); } sort(p1.begin(), p1.end()); int res = 1; for(auto item: p1){ if (p2.size() == 0) p2.push_back(p1[0]); int l = item.first, r = item.second; if (l \u0026lt;= p2[p2.size()-1].second){ if (r \u0026gt;= p2[p2.size()-1].second) p2[p2.size()-1].second = r; } else{ res += 1; p2.push_back({l,r}); } } cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; endl; return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-03-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%952/","summary":"1 双指针 两个指针指向两个序列: 归并排序 两个指针指向一个序列: 快排 1.1 最长不重复子序列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include\u0026lt;iostream\u0026gt;#include\u0026lt;vector\u0026gt;#include\u0026lt;algorithm\u0026gt;using namespace std; const int maxn =","title":"基础算法(二)"},{"content":"1 链表与邻接表 1 2 3 4 5  struct Node{ int val; Node *next; } new Node();   1.1 用数组模拟链表  用数组模拟单链表(静态链表): 邻接表(存储图和树)  O(1)时间找下一个点, O(n)时间找上一个点    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 100010; int head, e[N], ne[N], idx; void init(){ head = -1; } void add_to_head(int x){ ne[idx] = head; head = idx; e[idx] = x; idx ++; } void add(int k, int x){ e[idx] = x; ne[idx] = ne[k]; ne[k] = idx; idx ++; } void del(int k){ ne[k] = ne[ne[k]]; }   1.2 用数组模拟双链表  每个节点有两个指针, 指向前后 l[N], r[N], 0对应head, 1对应tail, 两个边界不包含实质内容  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 100010; int head, e[N], l[N], r[N], idx; void init(){ r[0] = 1; l[1] = 0; idx = 2; } void add(int k, int x){ e[idx] = x; r[idx] = r[k]; l[idx] = k; l[r[k]] = idx; r[k] = idx; idx ++; } void del(int k){ r[l[k]] = r[k]; l[r[k]] = l[k]; }   2 栈与队列 2.1 模拟栈 1 2 3 4 5 6 7  int stk[N], tt; // 插入 stk[ ++t] = x; // 弹出 t -- ; //判断是否为空 return t \u0026gt; 0; // 不空   2.2 单调栈  求每一个数左边离它最近且比它小的数, 没有返回-1 事实上对于这样的要求, 只需要保留单调上升序列即可, 逆序对的第一个元素删掉  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 100010; int n; int stk[N], tt; int main(){ ios::sync_with_stdio(false); scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i\u0026lt;n; i++){ int x; scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); while (tt \u0026amp;\u0026amp; stk[tt] \u0026gt;= x) t --; if (tt) printf(\u0026#34;%d \u0026#34;, stk[tt]); else printf(\u0026#34;-1 \u0026#34;); stk[ ++t] = x; } return 0; }   2.3 滑动窗口里的最大值和最小值 (单调队列)  在找最小值时, 只要前面的元素比后面的元素大, 那么大的元素一定没有用 找最大值是完全对称的写法  3 kmp  暴力怎么做 如何优化  next[i] = j表示p[1,\u0026hellip;,j] = p[i-j+1,\u0026hellip;,i]    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 10010, M=100010; int n, m; int p[N], s[M]; int ne[N]; int main(){ cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; p + 1 \u0026gt;\u0026gt; m \u0026gt;\u0026gt; s + 1; // 求next过程  for (int i=2, j=0; i\u0026lt;=m; i++){ while (j \u0026amp;\u0026amp; p[i] != p[j+1]) j = ne[j]; if (p[i] == p[j+1]) j ++; ne[i] = j; } // 匹配过程  for (int i=1, j=0; i\u0026lt;=m; i++){ while (j \u0026amp;\u0026amp; s[i] != p[j+1]) j = ne[j]; if (s[i] != p[j+1]) j ++; if (j == n){ // 匹配成功  } } }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-07-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841/","summary":"1 链表与邻接表 1 2 3 4 5 struct Node{ int val; Node *next; } new Node(); 1.1 用数组模拟链表 用数组模拟单链表(静态链表): 邻接表(存储图和树) O(1)时间找下一个点, O(n)","title":"数据结构(一)"},{"content":"系统为某一程序分配空间所需时间，与空间大小无关，与申请次数有关！\n哈希  存储结构（一般只有添加和查找操作，如果要删，在点上打一个标记即可）  拉链法（槽+链表） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  #include \u0026lt;iostream\u0026gt;using namespace std; const int N=100003; int h[N], e[N], ne[N], idx; void insert(int x){ int k = (x % N + N) % N; // 让余数为正  e[idx] = x; //新点的真正存储位置  ne[idx] = h[k]; //新点的next指针指向h[k](指向的点)  h[k] = idx ++; //h[k]指向新点 } bool find(int x){ int k = (x % N + N) % N; for (int i = h[k]; i != -1; i = ne[i]){ if (e[i] == x) return true; } return false; } int main(){ bool flag = true; for (int i = 100000;; i++){ for (int j=2; j*j\u0026lt;=i; j++){ if (i % j == 0){ flag = false; break; } } if (flag == false) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; break; } } int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); memset(h, -1, sizeof h ); while(n--){ char op[2]; int x; int scanf(\u0026#34;%s%d\u0026#34;, op, \u0026amp;x); if (*op == \u0026#39;I\u0026#39;) insert(x); else{ if (find(x)) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } } return 0; }    开放寻址法（一般初始化两倍大小的数组）：插入——从第k个坑位开始，如果有人，则往后，遇到没人的插入；查找——从第k个坑位开始，如果有人，就判断；如果没人，说明查找失败 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \u0026lt;iostream\u0026gt;using namespace std; const int N=200003, null = 0x3f3f3f3f; int h[N]; int find(int x){ int k = (x % N + N) % N; while(h[k] != null \u0026amp;\u0026amp; h[k] != x){ k ++; if (k == N) k = 0; } return k; } int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); memset(h, 0x3f, sizeof h); while(n--){ char op[2]; int x; scanf(\u0026#34;%s%d\u0026#34;, op, \u0026amp;x); int k = find(x); if (*op == \u0026#39;I\u0026#39;) h[k] = x; else{ if (h[k] != null) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } } }      字符串前缀哈希法  预处理出所有前缀的哈希，h[i]=str(0:i-1)的哈希值 讲字符串表示成一个p进制的数 \u0026ldquo;ABDC\u0026quot;→(1,2,3,4)_p = (p^3+2p^2+3p+4) mod Q 一般不把字母映射成0 $$ \\text{L到R这段的哈希值: }h[R]-h[L-1]*p^{R-L+1} $$ O(1)时间求某一段的哈希值    C++ STL  vector，变长数组，倍增的思想  size(), empty(), clear() front(), back() push_back(), a.pop_back() begin(), end()   pair\u0026lt;int, int\u0026gt;  first(), second(), 支持比较运算，以first为第一关键字，second为第二关键字   string，字符串，substr(), c_str()  size(), length(), empty(), clear()   queue，队列  push(), front(), back(), pop() size(), empty()   qriority_queue，优先队列（默认是大根堆）  push(), top(), pop(), clear() 定义小根堆 priority_queue\u0026lt;int, vector, greater\u0026gt; heap;   stack，栈  push(), top(), pop() size(), empty()   deque，双端队列（效率很低，比数组慢）  size(), empty(), clear(); front(), back() push_back(), pop_back() push_front(), pop_front() begin(), end()   set, map, multiset, multimap，基于平衡二叉树（红黑树）动态维护有序序列  size(), empty(), clear() set/multiset:  insert(), find(), count(), erase() — 输入是一个数x, 删除所有x O(k+lgn)；输入是一个迭代器，删除这个迭代器 lower_bounder(x) — 返回大于等于x的最小的数的迭代器 upper_bound(x) — 返回大于x的最小的数的迭代器   mapmultimap  insert() — 插入的是pair, erase() — 输入是pair或者迭代器     unordered_set, unordered_map, unordered_multiset, unordered_multimap，哈希表  和上面类似，增删改查时间复杂度为O(1) 不支持lower_bounder和upper_bound，以及迭代器的++/- -   bitset，压位  可以省8倍空间 bitset\u0026lt;10000\u0026gt; s — \u0026lt;\u0026gt;里是长度 ~, \u0026amp;, |, ^ \u0026laquo;, \u0026raquo; ==, ! = count() — 返回有多少个1 any() — 判断是否至少有一个1, none() — 判断是否全为0 set() — 把所有位置成1, set(k, v) reset() — 把所有位置成0 flip() — 所有位取反    ","permalink":"https://michelia-zhx.github.io/posts/2021-05-12-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%843/","summary":"系统为某一程序分配空间所需时间，与空间大小无关，与申请次数有关！ 哈希 存储结构（一般只有添加和查找操作，如果要删，在点上打一个标记即可） 拉链法","title":"数据结构(三)"},{"content":"1 Trie树  高效存储字符串集合 存储：按字符串内容，从左到右依次设置结点，并标记结束位置 查找：查找字符串是否存在 \u0026amp; 出现几次  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  const int N=100010; int son[N][26], cnt[N], idx; void insert(char str[]){ int p = 0; for (int i=0; str[i]; i++){ int u = str[i] - \u0026#39;a\u0026#39;; if (!son[p][u]) son[p][u] = ++idx; p = son[p][u]; } cnt[p] ++ ; } int query(char str[]){ int p = 0; for (int i=0; str[i]; i++){ int u = str[i] - \u0026#39;a\u0026#39;; if (!son[p][u]) return 0; p = son[p][u]; } return cnt[p]; }   2 并查集 快速地处理（近乎O(1)）：\n 合并两个集合 询问两个元素是否在一个集合中  基本原理：\n 每个集合用一棵树来表示，树根的编号是集合的编号； 每个结点存储父结点，p表示其父结点  问题1：如何判断树根：if (p[x] == x);\n问题2：如何找到所属集合根结点：while(p[x] ≠ x) x = p[x]; （路径压缩）\n问题3：如何合并两个集合（px是x的集合编号，py是y的集合编号）：p[x] = py;\n问题4：维护集合元素的个数 size()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  const int N=100010; int n, m; int p[N]; int find(int x){ if (p[x] != x) p[x] = find(p[x]); return p[x]; } int main(){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n ,\u0026amp;m); for (int i=1; i\u0026lt;= n; i++){ p[i] =i; size[p[i]] = 1; } while (m--){ char op[2]; int a, b; scanf(\u0026#34;%s\u0026#34;, \u0026amp;op); if (op[0] == \u0026#39;C\u0026#39;){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;a, \u0026amp;b); p[find(a)] = find(b); size[find(b)] += size[find(a)]; } else if (op[0] == \u0026#39;1\u0026#39;){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;a, \u0026amp;b); if (find(a) == find(b)) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } else{ scanf(\u0026#34;%d\u0026#34;, \u0026amp;a); printf(\u0026#34;%d\\n\u0026#34;, size[find(a)]); } } return 0; }   3 堆   小根堆：每个点都小于等于左右儿子\n 插入一个数（STL） 1  heap[++size] = x; up(size);    求集合当中最小值（STL） 1  heap[1];    删除最小值（STL） 1  heap[1] = heap[size]; size--; down(1);    删除任意一个元素 1  heap[k] = heap[size]; size--; down(k); up(k);    修改任意一个元素 1  heap[k] = x; down(k); up(k);       堆是完全二叉树，用数组存储，1号位是根结点\n  一个结点是x，左孩子是2x，右孩子是2x+1\n  五个操作完全可以用以下两个函数组合起来操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  void down(int u){ int t = u; if (u * 2 \u0026lt;= size \u0026amp;\u0026amp; h[u * 2] \u0026lt; h[u]) t = u * 2; if (u * 2 + 1 \u0026lt;= size \u0026amp;\u0026amp; h[u * 2 + 1] \u0026lt; h[u]) t = u * 2 + 1; if (u != t){ swap(h[u], h[t]); down(t); } } void up(u){ while (u / 2 \u0026gt;= 1 \u0026amp;\u0026amp; h[u / 2] \u0026gt; h[u]){ swap(h[u / 2], h[u]); u /= 2; } }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-09-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%842/","summary":"1 Trie树 高效存储字符串集合 存储：按字符串内容，从左到右依次设置结点，并标记结束位置 查找：查找字符串是否存在 \u0026amp; 出现几次 1 2 3 4 5 6 7 8 9 10","title":"数据结构(二)"},{"content":"短视的行为\n区间选点 给N个闭区间, 要在数轴上选尽量少的点, 使每个区间至少包含一个选出的点 (answer ≤ count)\n区间贪心问题, 要么按左端点排序, 要么按右端点, 要么双关键字\n 每个区间按右端点从小到大排序 从前往后依次枚举每个区间   如果已经包含点, pass 否则选右端点 (answer ≥ count) =\u0026gt; answer = count  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return r \u0026lt; W.r; } }range[N]; int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); int res = 0, ed = -2e9; for (int i=0; i \u0026lt; n; i++){ if (range[i].l \u0026gt; ed){ res ++; ed = range[i].r; } } printf(\u0026#34;%d\u0026#34;, res); return 0; }   排课 (不冲突排尽量多的课) 和上题一样, 上题选点的个数(有点的区间相互没有相交) = 这题的课程数\n区间分组 区间分组, 每组内的区间两两没有交集, 要求组数尽量小. (合法的 → answer ≤ count)\n 每个区间按左端点从小到大排序 从前往后依次枚举每个区间   判断能否放入现有的组中 L[i] \u0026gt; max_r  不能, 新建一个组 存在, 随便挑一个, 将其放入, 更新当前组的max_r 最后分出的count个组, 每个都是相交的, 因此必须分开 → answer ≥ count =\u0026gt; answer = count    区间max_r可以用小根堆存储  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;queue\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return r \u0026lt; W.r; } }range[N]; int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;int\u0026gt; \u0026gt; heap; for (int i=0; i\u0026lt;n; i++){ auto r = range[i]; if (heap.empty() || heap.top() \u0026gt;= r.l) heap.push(r.r); else{ int t = heap.top(); heap.pop(); heap.push(r.r); } } printf(\u0026#34;%d\u0026#34;, heap.size()); return 0; }   区间覆盖 给定一些小区间和一个大区间[start, end], 要求用尽量少的小区间去完全覆盖大区间.\n 将所有区间按左端点从小到大排序 从前往后依次枚举每个区间, 在所有能覆盖start的区间中, 选择右端点最大的那一个, 将start更新成这个右端点  合法的 → answer ≤ count. 反证法 → answer ≥ count. 假设answer \u0026lt; count, 找到第一个不一样的, 替换掉, 保持个数不变且依然合法. answer = count.    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;queue\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return l \u0026lt; W.l; } }range[N]; int main(){ int st, ed; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;st, \u0026amp;ed); scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); int res = 0; bool success = false; for (int i=0; i \u0026lt; n; i++){ int j=i, r=-2e9; while (j\u0026lt;n \u0026amp;\u0026amp; range[j].l \u0026lt;= st){ r = max(r, range[j].r); j++; } if (r \u0026lt; st){ res = -1; break; } res ++; if (r \u0026gt;= ed) { success = true; break; } st = r; i = j-1; } if (!success) res = -1; printf(\u0026#34;%d\u0026#34;, res); return 0; }   哈夫曼树  合并果子 - 消耗体力最少  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;queue\u0026gt; using namespace std; int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;int\u0026gt;\u0026gt; heap; while(n--){ int x; scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); heap.push(x); } int res = 0; while (heap.size() \u0026gt; 1){ int a = heap.top(); heap.pop(); int b = heap.top(); heap.pop(); res += a + b; heap.push(a+b); } printf(\u0026#34;%d\u0026#34;, res); return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-27-%E8%B4%AA%E5%BF%83/","summary":"短视的行为 区间选点 给N个闭区间, 要在数轴上选尽量少的点, 使每个区间至少包含一个选出的点 (answer ≤ count) 区间贪心问题, 要么按左端点排序, 要么按右端点, 要","title":"贪心"}]