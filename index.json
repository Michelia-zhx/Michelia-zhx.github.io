[{"content":"Chap 1 计算机网络和因特网 1.1 什么是因特网 1.1.1 因特网的具体构成描述  各种各样的设备都被连接到因特网,这些设备都被称为主机或端系统. 端系统通过通信链路和分组交换机连接到一起. 分组: 一台端系统向另一台端系统发送数据时, 发送端系统将数据分组, 并为每段加上首部字节 分组交换机: 从入通信链路接受分组, 从出通信链路转发分组. 路由器和链路层交换机 一个分组从发送端到接收端所经历的一系列通信链路和分组交换机称为通过该网络的路径 (route 或 path) 端系统通过因特网服务提供商 (Internet Servise Provider, ISP) 接入因特网 端系统, 分组交换机和其他因特网部件, 都要运行控制因特网中信息接收和发送的一系列协议. TCP (Transmission Control Protocol, 传输控制协议), IP (Internet Protocol, 网际协议). IP 协议定义了在路由器和端系统中发送和接受的分组的格式. 因特网的主要协议统称为 TCP/IP.  1.1.2 服务描述  因特网也可以被描述为: 为应用程序提供服务的基础设施. 与因特网相连的端系统提供了一个应用程序编程接口 (Application Programming Interface, API), 规定了运行在一个端系统上的软件请求因特网基础设施向运行在另一个端系统上的特定目的的软件交付数据的方式. 一个协议定义了在两个或多个通信实体之间交换的报文格式和次序, 以及在报文传输和/或接受或其他事件方面做采取的动作.  1.2 网络边缘 1.2.1 客户机和服务器程序  客户器程序是运行在一个端系统上的程序, 它发出请求, 并从运行在另一个端系统上的服务器程序接受服务.  客户机-服务器因特网应用程序是分布式应用程序   P2P 应用程序: 其中的端系统互相作用并执行客户机和服务器功能的程序.  1.2.2 接入网  接入网: 将端系统连接到其边缘路由器的物理链路 (边缘路由器是端系统到任何其他远程端系统的路径上的第一台路由器)  网络接入大致分为三种类型: 住宅接入, 公司接入, 无线接入.   住宅接入: 将家庭端系统与边缘路由器相连接  通过普通模拟电话线用拨号调制解调器与住宅 ISP 相连, 家用调制解调器将 PC 输出的数字信号转换为模拟形式, 以便在模拟电话线上传输 宽带住宅区接入有两种常见类型: 数字用户线 (digital subscriber line, DSL) 和混合光纤同轴电缆 (hybrid fiber-coaxial cable, HFC)   公司接入: 局域网 (LAN) 通常被用于连接端用户与边缘路由器 无线接入: 在无线局域网中, 无线用户与位于几十米内的基站之间传输/接收分组. 在广域无线接入网中, 分组经用于蜂窝电话的相同无线基础设施进行发送.  ","permalink":"https://michelia-zhx.github.io/posts/2022-02-26-computer_networking_notes/","summary":"Chap 1 计算机网络和因特网 1.1 什么是因特网 1.1.1 因特网的具体构成描述 各种各样的设备都被连接到因特网,这些设备都被称为主机或端系统. 端系统通过通信链路和","title":"Notes - Computer Networking"},{"content":"Active Learning 也称为查询学习或者最优实验设计. 主动学习通过设计合理的查询函数, 不断从未标注的数据中挑选出数据标注后放入训练集. 有效的主动学习数据选择策略可以有效地降低训练的代价并同时提高模型的识别能力.\n一. 主动学习的场景   Membership Query Sythesis 生成一个询问, 并请求这个样本的标签, 这个样本可能是未标注数据中的任意一个, 甚至是从头生成的, 反正这些数据一般不是简单的服从一个自然分布的随机.\n  Stream-Based (Sequential) Selective Sampling 基于流的有选择性地选择目标, 这种通常是假设会有大量廉价的无标签数据. 它将采样一个无标签的数据, 然后决定是要询问它的标签还是忽略它 (如果数据是均匀分布, 那么和上一种情况一样), 通常基于下面的两个度量:\n 更大的信息量: 选择这些具有高信息量的数据 不确定性原则: 选择落在这种不确定域之中的数据    Pool Based Sampling 基于池的采样, 假设能够一次性获得大量未标注的数据, 并进行同时处理. 可以对数据池中的数据进行信息量排序, 直接采最有信息量的数据进行分析.\n  基于流的场景下, 数据是顺序来到的, 不会有全局的视野, 而基于池的则是更加常见的做法可以一次性对所有数据的信息量进行分析. 但是有的时候因为数据的生成情况或者计算带宽内存等的限制, 人们还是不得已还是要使用基于流的场景.\n二. Query Strategy Frameworks 用$x^*_A$表示某种采样算法$A$下最有信息量的样本.\nUncertainty Sampling  用熵作度量: $x^*_{ENT} = \\argmax_x -\\sum_iP(y_i|x;\\theta)\\log P(y_i|x;\\theta)$ 用置信度作度量: $x^_{LC} = \\argmin_x P(y^|x;\\theta), y^* = \\argmax_yP_{\\theta}(y|x)$, 选取最大置信度最小的样本 用最小间隔作度量: $x^{SM} = \\argmax_xP{\\theta}(y^1|x) - P{\\theta}(y^_2|x), y^_1, y^*_2$为可能性最高的两个样本.  Query-By-Committee (QBC)  选择一定数量的模型构成委员会$\\mathcal{C}={\\theta^{(1)},\u0026hellip;,\\theta^{(C)}}$, 对未标注的数据进行处理, 挑选出所有未标记数据中各个模型意见最不一致的样本. 不一致的度量:  用投票熵作度量: $$x^*_{VE} = \\argmax_x-\\sum_i\\dfrac{V(y_i)}{C}\\log\\dfrac{V(y_i)}{C}$$ 用KL散度(衡量两个分布的差异)作度量: $$x^*{KL} = \\argmax_x\\dfrac{1}{C}\\sum{c=1}^CD(P_{\\theta^{(c)}}|P_{\\mathcal{C}})$$ 其中 $$D(P_{\\theta^{(c)}}|P_{\\mathcal{C}}) = \\sum_iP(y_i|x;\\theta^{(c)})\\log\\dfrac{P(y_i|x;\\theta^{(c)})}{P(y_i|x;\\mathcal{C})}$$    Expected Model Change  去采的样本应当具备条件: 当它被赋予标记, 应当最大程度优化模型 —— 用训练梯度作为这种优化的衡量. $$x^*_{EGL} = \\argmax_x\\sum_iP(y_i|x;\\theta)|\\nabla \\mathcal{l}(\\mathcal{L}\\cup\\langle x, y_i\\rangle;\\theta)|$$  Variance Reduction and Fisher Information Ratio   最小化学习器的未来误差 (偏置-方差分解): $$E_T[(o-y)^2|x] = E[(y-E[y|x])^2] + (E_{\\mathcal{L}}[o] - E[y|x])^2 + E_{\\mathcal{L}}[(o-E_{\\mathcal{L}}[o])^2]$$\n  右边三项分别为噪音、偏置的平方、方差. 只适用于回归任务.\n  离散分类器使用Fisher Information: $$\\mathcal{I}(\\theta) = -\\int_xP(x)\\int_yP(y|x;\\theta)\\dfrac{\\partial^2}{\\partial\\theta^2}\\log P(y|x;\\theta)$$\n  最佳检索样本应最小化Fisher Information Ratio: $$x^*_{FIR} = \\argmin_xtr(\\mathcal{I}x(\\theta)^{-1}\\mathcal{I}{\\mathcal{U}}(\\theta))$$\n  $\\mathcal{I}x(\\theta)$不仅说明模型对样本$x$的不确定性有多大, 而且说明了是哪个参数造成了这种不确定性. $\\mathcal{I}{\\mathcal{U}}(\\theta)$表明了在整个数据集上的不确定性.\n  Estimated Error Reduction Density-Weight Methods 三. 主动学习分析 理论分析 四. Problem Setting Varients (变体) 结构化输出的主动学习  序列化模型 (CRF 或 HMM) 产生的输出. 树状输出.  批处理模式下的主动学习  适合并行处理. 每次选取最优的N个样本不一定能达到很好的效果, 因为没有考虑这些样本间信息的重合度.  主动学习的代价 多种访问类型 ","permalink":"https://michelia-zhx.github.io/posts/2021-08-31-active_learning/","summary":"Active Learning 也称为查询学习或者最优实验设计. 主动学习通过设计合理的查询函数, 不断从未标注的数据中挑选出数据标注后放入训练集. 有效的主动学习数据选择策","title":"Paper Notes - Active Learning"},{"content":" Learning from Multiple Teacher Networks http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf\n  loss:  teachers的softmax输出取平均和student的交叉熵 中间层表示的相对相异度(仅适用于MTKD), 三元组$(q_i,q_i^+,q_i^-)$, 其中$q$是中间层的输出, 偏序关系$q_i^+ \u0026gt; q_i^-$由两者和$q_i$的距离$d$决定, 参数$w_s$决定选取哪些层. 在不同teacher中, 中间层输出的偏序关系可能不同, 因此用投票法决定中间层输出应当的偏序关系, 设计和student对应层输出的loss, 以此鼓励student拥有和teacher中间层类似的相对相似(相异)关系. student和groudtruth的交叉熵   实验设置: 基于CIFAR-10, CIFAR-100, MNIST, SVHN的实验  CIFAR-10, 比较student不同层数和参数量(11/250K, 11/862K, 13/1.6M, 19/2.5M)时的表现(compression rate, acceleration rate and classification accuracy)(和Fitnets比较) CIFAR-10, student均为11层, 比较当student的参数为250K和862K时, teacher数量为1, 3, 5时, Teacher, RDL, FitNets, KD和他们的准确率 CIFAR-10, CIFAR-100, 比较不同方法(Teacher(5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法(19层))在两个数据集上的准确率 MNIST, 比较不同方法(Teacher(4层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法(7层))的准确率 SVHN, 比较不同方法(Teacher(5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法(19层))的准确率   @inproceedings{you2017learning, author={You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng}, booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, title={Learning from multiple teacher networks}, pages={1285\u0026ndash;1294}, year={2017} }   Semi-Supervised Knowledge Transfer for Deep Learning from Private Training Data. ICLR 2017 https://openreview.net/pdf?id=HkwoSDPgg\n  学习算法应当保护用户私人数据, 但模型会记住数据, 也会受到攻击(black/white box attack). 将样本分成n份, 分别训练teacher model, 将这些teachers聚合:  如果大多数teacher有相同的输出, 则输出不依赖于分别训练teacher的不相交集 如果有某两类票数相近, 则分歧可能会泄露私人信息(我不理解) 在投票中引入随机噪声   student是半监督, 一部分是利用private data通过teacher得到的label, 之后使用无标记的public data. 利用GAN训练student, discriminator增加1类(m+由生成器生成), 训练后只使用discriminator. 实验设置:    Dataset Teacher Student Student Public Data testing Data     MNIST 2 conv + 1 relu GANS(6 fc layers) test[:1000] test[1000:]   SVHN 2 conv + 2 relu GANS(7 conv + 2 NIN) test[:1000] test[1000:]   UCI Adult RF(100 trees) RF(100 trees) test[:500] test[500:]   UCI Diabetes RF(100 trees) RF(100 trees) test[:500] test[500:]     @article{Papernot2017SemisupervisedKT, title={Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data}, author={Nicolas Papernot and Mart{'i}n Abadi and {'U}lfar Erlingsson and Ian J. Goodfellow and Kunal Talwar}, journal={arXiv preprint}, pages = {}, year={2016} }   Knowledge Adaptation: Teaching to Adapt. ICLR, 2017 https://openreview.net/pdf?id=rJRhzzKxl\n  teachers的domain和student $\\mathcal{D}{T_i}, \\mathcal{D}S$不完全一致, 因此student对teacher的信任度取决于两者表示空间的相似度 $\\mathcal{L} = \\mathcal{H}(\\sum{i=1}sim(\\mathcal{D}{T_i}, \\mathcal{D}S)\\cdot D{T_i}, D_S)$ 定义MCD, MCD越大表示越远离分类边界, 输入teacher得到的结果置信度越高. 因此选取MCD最大(即置信度最高)的n个样本, 在teacher中得到的结果作为伪标记以训练student可以使无监督学习的性能得到提升. 实验设置: 基于Amazon product reviews sentiment analysis dataset. 包含Book, DVD, Electronics, Kitchen四类.  比较teacher, 由以相同类别样本训练的teacher训练出来的student, 由以所有样本训练的teacher训练出来的student, 结合以上两种teacher训练出来的student, 以及许多其他模型(SCL, SFA, SCL-com, SFA-com, SST, IDDIWP, DWHC, DAM, CP-MDA, SDAMS-SVM, SDAMS-Log)在四种类别上的性能. 比较分别以其中三类为domain的teacher训练以第四类为domain的student(B$\\rightarrow$D,E$\\rightarrow$D,K$\\rightarrow$D依次轮换)在不同方法下的性能   @article{ruder2017knowledge, title={Knowledge adaptation: Teaching to adapt}, author={Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G}, journal={arXiv preprint arXiv:1702.02052}, pages = {}, year={2017} }   Deep Model Compression: Distilling Knowledge from Noisy Teachers. Sau, Bharat Bhusan et al. arXiv:1610.09650 https://arxiv.org/pdf/1610.09650.pdf\n  在teacher的输出(student的目标)上加扰动, 等价于基于噪声的正则化. 可用于模型压缩. 实验设置: 基于MNIST, SVHN, CIFAR-10.  MNIST: teacher \u0026ndash; a modified network of LeNet([C5(S1P0)@20-MP2(S2)]- [C5(S1P0)@50-MP2(S2)]- FC500- FC10); student \u0026ndash; FC800-FC800-FC10 SVHN: Network-in-Network([C5(S1P2)@192]- [C1(S1P0)@160]- [C1(S1P0)@96-MP3(S2)]- D0.5- [C5(S1P2)@192]- [C1(S1P0)@192]- [C1(S1P0)@192- AP3(S2)]- D0.5- [C3(S1P1)@192]- [C1(S1P0)@192]- [C1(S1P0)@10]- AP8(S1)); student: LeNet([C5(S1P2)@32-MP3(S2)]- [C5(S1P2)@64-MP3(S2)]- FC1024-FC10) CIFAR-10: teacher: same as SVHN; student: a modified version of the LeNet([C5(S1P2)@64-MP2(S2)]- [C5(S1P2)@128- MP2(S2)]-FC1024-FC10).   @article{sau2016deep, title={Deep model compression: Distilling knowledge from noisy teachers}, author={Sau, Bharat Bhusan and Balasubramanian, Vineeth N}, journal={CoRR}, pages = {}, year={2016} }   Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Tarvainen, Antti and Valpola, Harri. NeurIPS 2017 https://papers.nips.cc/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf\n  算法:  构建一个普通的监督模型; copy一份监督学习模型, 原模型叫做student, 新的叫teacher; 每步使用同样的minibatch输入到student与teacher模型中, 但在输入数据前分别加入随机增强或者噪音; 加入student与teacher输出的一致性损失函数; 优化器只更新student的权重; 每步之后, 采用student权重的EMA更新teacher权重;   核心思想是: 模型既充当学生, 又充当老师. 作为老师, 用来产生学生学习时的目标; 作为学生, 则利用教师模型产生的目标来进行学习. 而教师模型的参数是由历史上(前几个step)几个学生模型的参数经过加权平均得到. 可以看成是П-model中的两次计算中模型换成了两个不同的模型, 一个叫teacher, 一个叫student; 另外, 也可以看成作Temporal ensembling的改进版, 在Temporal ensembling中, 采用的是每epoch的指数移动平均值来聚合历史数内容, 而Mean teacher则是在每训练步进行对Student的权重进指数移动平均. 实验设置: 基于SVHN和CIFAR-10  All the methods in the comparison use a similar 13-layer ConvNet architecture.   @inproceedings{10.5555/3294771.3294885, title = {Mean Teachers Are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results}, author = {Tarvainen, Antti and Valpola, Harri}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {1195–1204}, year = {2017} }   Born-Again Neural Networks. Furlanello, Tommaso et al. ICML 2018 https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf\n  基于新模型的输入和原模型的输入间的交叉熵, 使用KD项修改替代和正则化原来的loss Selves Born-Again Networks集成的学习顺序: $\\mathcal{L}(f(x, \\arg\\min_{\\theta_{k-1}}\\mathcal{L}(f(x, \\theta_{k-1}))),f(x,\\theta_k))$, 将上一个student学到的知识作为监督信息, 教导下一个学生. 实验设置:  CIFAR-10: Wide-ResNet with different depth and width (28-1, 28-2, 28-5, 28-10) and DenseNet of different depth and growth factor (112-33, 90-60, 80-80, 80-120) CIFAR-100: 与上同.   @inproceedings{Furlanello2018BornAN, title={Born Again Neural Networks}, author={Tommaso Furlanello and ZaKnowledge Adaptation: Teaching to Adaptchary Chase Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar}, booktitle={ICML}, year={2018} }   Deep Mutual Learning. Zhang, Ying et al. CVPR 2018 https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf\n  两个(多个?)students相互学习, 对于每个student, 损失为和groundtruth的交叉熵以及相对于另一个student的softmax输出的KL散度: $\\mathcal{L}{\\theta_1} = \\mathcal{L}{C_1} + D_{KL}(p_2|p_1), \\theta_1\\leftarrow\\theta_1+\\gamma_t\\dfrac{\\partial\\mathcal{\\theta_1}}{\\partial\\theta_1}$; $\\mathcal{L}{\\theta_2} = \\mathcal{L}{C_2} + D_{KL}(p_1|p_2), \\theta_2\\leftarrow\\theta_2+\\gamma_t\\dfrac{\\partial\\mathcal{\\theta_2}}{\\partial\\theta_2}$ 优点:  随着学生网络的增加其效率也得到提高 它可以应用在各种各样的网络中, 包括大小不同的网络 即使是非常大的网络采用相互学习策略, 其性能也能够得到提升   实验设置:  数据集: ImageNet, CIFAR-10, CIFAR-100, Market-1501 Networks:    ResNet-32 MobileNet InceptionV1 WRN-28-10     0.5M 3.3M 7.8M 36.5M       @inproceedings{8578552, title = {Deep Mutual Learning}, author = {Y. Zhang and T. Xiang and T. M. Hospedales and H. Lu}, booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages = {4320-4328}, year = {2018} }   Data Distillation: Towards Omni-Supervised Learning. Radosavovic, Ilija et al. CVPR 2018 https://openaccess.thecvf.com/content_cvpr_2018/papers/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.pdf\n  Model Distillation vs. Data Distillation: 前者ensemble同一样本在不同模型的输出, 后者ensemble同一样本经不同转换后在同一模型的输出. 方法:  用手动标注的数据训练模型A 用模型A去训练数据增广 (本文中为 scaling and horizontal flipping) 的未标注数据 将未标注数据的预测结果通过 ensembling 多个预测结果, 转化为 labels 在手动标注和自动标注的数据集重新训练模型   实验: 在COCO Keypoint Detection, Object Detection 验证. teacher和student都是Mask R-CNN keypoint detection variant @inproceedings{inproceedings, title = {Data Distillation: Towards Omni-Supervised Learning}, author = {Radosavovic, Ilija and Dollar, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming}, year = {2018}, doi = {10.1109/CVPR.2018.00433} pages = {4119-4128} }   Multilingual Neural Machine Translation with Knowledge Distillation. ICLR 2019 https://openreview.net/pdf?id=S1gUsoR9YX\n  方法很简单, 就是先针对每对语言训练单独的翻译模型作为teacher, 再用multi-teacher KD训练student, loss就是student和label的交叉熵以及和teacher的softmax输出的交叉熵. 实验设置: 数据集: IWSLT, WMT, Ted Talk; student和teacher均使用Transformer    Task model hidden size $d_{model}$ feed-forward hidden size $d_{ff}$ number of layer     IWSLT and Ted talk tasks 256 1024 2   WMT task 512 2048 6     @article{Tan2019MultilingualNM, title={Multilingual Neural Machine Translation with Knowledge Distillation}, author={Xu Tan and Yi Ren and Di He and Tao Qin and Zhou Zhao and Tie-Yan Liu}, journal={ICLR}, year={2019}, volume={abs/1902.10461} }   Unifying Heterogeneous Classifiers with Distillation. Vongkulbhisal et al. CVPR 2019 https://openaccess.thecvf.com/content_CVPR_2019/papers/Vongkulbhisal_Unifying_Heterogeneous_Classifiers_With_Distillation_CVPR_2019_paper.pdf\n  N个不同的模型$\\mathcal{C} = {C_i}_{i=1}^N$具有不同的结构和目标类别, $C_i$被训练以分别预测$p_i(Y=l_j)$, 并整合出样本在所有类中的概率$q(Y=i_j)$. 最后利用$q$训练student. 作者提出了基于交叉熵最小化和矩阵分解的方法，从未标记的样本中估计所有类别的soft-labels. 实验设置:  数据集: ImageNet, LSUN, Places365 $C_i$从AlexNet, VGG16, ResNet18, ResNet34中随机选择   @article{Vongkulbhisal2019UnifyingHC, title={Unifying Heterogeneous Classifiers With Distillation}, author={Jayakorn Vongkulbhisal and Phongtharin Vinayavekhin and Marco Visentini Scarzanella}, journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2019}, pages={3170-3179} }   Distilled Person Re-Identification: Towards a More Scalable System. Wu, Ancong et al. CVPR 2019 https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf\n  解决三个问题: 降低标签成本(减少标签的需求量); 降低跨数据库成本(利用一些先验知识); 降低测试成本(使用轻量级网络) 假设taregt domain包含10个类的图片, 先用多个个source domain分别训练多个teacher model, source domain之后并不会被用到(利用一些先验知识\u0026ndash;降低跨数据库成本); target domain可以只包含10个labelled sample(10类均有), 其余均为unlabeled sample, 对于N个unlabelled input, 定义相似度矩阵$A$, 其中第i行第j列表示第i个图像和第j个图像在同一个模型下输出的相似度. 为了将知识从teacher迁移到student, 需要最小化teacher的相似度矩阵$A_T$和student的相似度矩阵$A_S$的距离(这句话是学习single teacher). 分别利用teacher计算target domain中每一个x的特征向量, 并分别计算相似度矩阵$A$, 使用$L_{ver}$更新每一个老师模型的权重$a$(可以理解为，权重越大，该老师模型对应的source和target越相似) 计算出每一个老师模型和学生模型得到的相似矩阵的差异，并使用上述的权重加权，从而得到$L_{ta}$. 使用$L_{ta}$对学生模型进行更新, 循环训练. 实验设置: 数据集 \u0026ndash; Market-1501, DukeMTMC. 分别使用MSMT17, CUHK03, ViPER, DukeMTMC, Market-1501训练5个teacher model$T_1, T_2, T_3, T_4, T_5$; teacher \u0026ndash; an advanced Re-ID model PCB, student \u0026ndash; a lightweight mod- el MobileNetV2. @InProceedings{Wu_2019_CVPR, author = {Wu, Ancong and Zheng, Wei-Shi and Guo, Xiaowei and Lai, Jian-Huang}, title = {Distilled Person Re-Identification: Towards a More Scalable System}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2019} }   Diversity with Cooperation: Ensemble Methods for Few-Shot Classification. Dvornik, Nikita et al. ICCV 2019 https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf\n  meta learning \u0026ndash; learning to learn 模型间的关系有三种 - 合作(预测的结果无论是属于正确结果的概率还是属于错误结果的概率都是比较一致的), 独立(两个模型预测的结果之间不存在明显的关系), 多样性(除了正确结果，预测为其他结果的概率差异明显). 文章通过出了交叉熵损失外设计不同的损失函数$\\psi(y_i,f_{\\theta_j}(x_i),f_{\\theta_l}(x_i))$诱导模型的关系向不同方向发展, 例如基于cos或KL散度. 实验设置:  数据集: mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds (CUB) 2002011. ensemble of ResNet18 and WideResNet28   @INPROCEEDINGS{9010380, title={Diversity With Cooperation: Ensemble Methods for Few-Shot Classification},author={Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia}, booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, pages={3722-3730},\nyear={2019} }   Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System. Yang, Ze et al. WSDM 2020 https://arxiv.org/pdf/1910.08381.pdf\n  一种用于网络问答系统的两阶段多教师知识蒸馏(简称 TMKD)方法, 首先用一个通用的问答提炼任务对student进行预训练(貌似也是使用Multi-teacher), 并在下游任务(如 Web Q\u0026amp;A 任务, MNLI, SNLI, 来自 GLUE 的 RTE 任务)上使用Multi-Teacher KD进一步微调这个预训练的student. “early calibration” effect缓解了单个teacher造成的过拟合偏差. 实验设置:  数据集 - DeepQA, CommQA-Unlabeled, CommQA-Labeled, MNLI, SNLI, QNLI, RTE. Baseline: teacher - BERT-3, BERT_{large}, BERT_{large}Ensemble; student(Traditional Distillation Model) - Bi-LSTM(1-o-1, 1_{avg}-o-1, m-o-m), BERT3(1-o-1, 1_{avg}-o-1, m-o-m), student(TMKD) - Bi-LSTM(TMKD), TMKD_{base}, TMKD_{large}(后两者都是BERT-3 models).   @inproceedings{inproceedings, author = {Ze, Yang and Shou, Linjun and Gong, Ming and Lin, Wutao and Jiang, Daxin}, title = {Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System}, publisher = {Association for Computing Machinery}, doi = {10.1145/3336191.3371792}， pages = {690-698}, year = {2020} }   FEED: Feature-level Ensemble for Knowledge Distillation. Park, SeongUk and Kwak, Nojun. AAAI 2020 https://openreview.net/pdf?id=BJxYEsAqY7\n  实验方法就是让student直接学teacher的feature. 实验设置: 数据集: CIFAR-100; 选取模型: student \u0026ndash; ResNet-56, ResNet-110, WRN28-10, ResNext29-16x64d; 没说teacher是谁. @article{Park2019FEEDFE, title={FEED: Feature-level Ensemble for Knowledge Distillation}, author={Seonguk Park and Nojun Kwak}, journal={ECAI}, year={2019}, volume={abs/1909.10754} }   Stochasticity and Skip Connection Improve Knowledge Transfer. Lee, Kwangjin et al. ICLR 2020 https://openreview.net/pdf?id=HklA93NYwS\n  利用单个教师网络生成多个教师网络(加入stochastic blocks和skip connections)并训练学生网络, 分块并含有skip connections的网络可以看成树状网络, 从input到output有多条路径. 实验设置: 数据集 \u0026ndash; CIFAR-100 和 tiny imagenet, 并将这种方法应用到KD, AT(attention tranfer), ML. 实验中涉及到的teacher有ResNet 32, ResNet 110, WRN 28-10, MobileNet, WRN 40-4; 涉及到的student有VGG 13, ResNet 20, ResNet 32, WRN 40-4. @INPROCEEDINGS{9287227, author={Nguyen, Luong Trung and Lee, Kwangjin and Shim, Byonghyo}, title={Stochasticity and Skip Connection Improve Knowledge Transfer}, booktitle={2020 28th European Signal Processing Conference (EUSIPCO)}, pages={1537-1541}, year={2021} }   Hydra: Preserving Ensemble Diversity for Model Distillation. Tran, Linh et al. arXiv:2001.04694 http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-026.pdf\n  普通multi-teacher KD对teacher的预测值取平均, 这样会丧失多teacher结果包含的不确定性信息(?), 本文将student拆分成body和多个head, 每个head对应一个teacher, 以保留多teacher输出的多样性 假设有M个teacher, 首先训练一个head直至其收敛至teacher的平均, 再添加其他M-1个head, M个head一起训练, 实验证明如果没有第一个head会很难收敛. 作者定义了一个模型不确定性, 由数据不确定性和总不确定性组成(我不理解为什么是这个顺序). 实验设置:  数据集: a spiral toy dataset(用于可视化并解释模型不确定性), MNIST(测试时用了它的测试集和Fashion-MNIST), CIFAR-10(测试时用了它的测试集, cyclic translated test set, 80 different corrupted test sets 和 SVHN). 模型: toy dataset - 两层MLP, 每层100个结点; MNIST - MLP; CIFAR-10 - ResNet-20 V1. 在回归问题中, 所有数据集均使用MLP.   @article{DBLP:journals/corr/abs-2001-04694, author = {Linh Tran, Bastiaan S. Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V. Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Sebastian Nowozin, Rodolphe Jenatton}, title = {Hydra: Preserving Ensemble Diversity for Model Distillation}, journal = {CoRR}, year = {2020} }   Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition. Gao, Yan et al. arXiv:2005.09310 https://arxiv.org/pdf/2005.09310v1.pdf\n  @article{DBLP:journals/corr/abs-2005-09310, author = {Yan Gao, Titouan Parcollet, Nicholas D. Lane}, title = {Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition}, journal = {CoRR}, year = {2020} }   Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection. Chen, Cong et al. IEEE 2020 [code]\n  Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation. MICCAI 2020\n  Knowledge Distillation for Multi-task Learning. Li, WeiHong \u0026amp; Bilen, Hakan. arXiv:2007.06889 [project]\n  Adaptive Multi-Teacher Multi-level Knowledge Distillation. Liu, Yuang et al. Neurocomputing 2020 [code] https://arxiv.org/pdf/2103.04062.pdf\n  loss: $\\mathcal{L} = \\mathcal{L}{KD}+\\alpha\\mathcal{L}{angle}+\\beta\\mathcal{L}_{HT}$  $\\mathcal{L}{KD}$: 对于每个样本, student需要赋予teachers的输出不同的权重, student中fc之前的表示经过maxpooling后和每个teacherfc前的表示分别做点积作为权重, teacher的加权和作为weighted target, 将weighted target与student的soft-target间的KL散度和student输出与groungtruth的交叉熵作为$\\mathcal{L}{KD}$. $\\mathcal{L}{angle}$: 对于样本组成的三元组, 计算它们的teacher和student表示的空间相对位置, 计算二者的Huber loss作为$\\mathcal{L}{angle}$. $\\mathcal{L}_{HT}$: 计算teacher和student中间层表示的差的二范式, student的中间层需要经过一个单层FitNet使其规模等于teacher的中间层表示.   实验设置: 数据集有CIFAR-10, CIFAR-100和Tiny-ImageNet.  CIFAR-10, CIFAR-100: teacher使用ResNet110, VGG-19, DenseNet121, student为ResNet20; 比较不同baseline (OKD, FitNet, RKD, AvgMKD, DML) 在数据集上的表现; 比较不同baseline(OKD, AvgMKD, DML)在teacher数量为2,3,5时的表现. Tiny-ImageNet: teacher(ResNet110, ResNet56, ResNet32), student - ResNet20.   @article{LIU2020106, author={Yuang Liu and W. Zhang and Jijie Wang}, title = {Adaptive multi-teacher multi-level knowledge distillation}, journal = {Neurocomputing}, pages = {106-113}, year = {2020} }  ","permalink":"https://michelia-zhx.github.io/posts/2022-02-23-multi_teacher_knowledge_distillation-1/","summary":"Learning from Multiple Teacher Networks http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf loss: teachers的softmax输出取平均和student的交叉熵 中间层表示的相对相异度(仅适用于MTKD), 三元组$(q_i","title":"Paper Notes - Multi-Teacher Knowledge Distillation - 1"},{"content":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks https://arxiv.org/pdf/2004.05937.pdf\n Learning from Multiple Teacher Networks, KDD 2017 Efficient knowledge distillation from an ensemble of teachers. Interspeech 2017: 对teacher的logits取加权平均, 加权平均和student的logits计算交叉熵作为损失函数; 另外没有说权是怎么分配的 (提前设置好的). A Two-Teacher Framework for Knowledge Distillation. ISNN 2019 Feature-Level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks. ECAI 2020 Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation: 多教师知识蒸馏中如何整合多教师的知识的问题依然没有得到很好的解决. 不同的教师具有不同的重要性, 且有些教师会对学生的泛化性能产生负面影响. 本文提出了基于多任务学习的自适应方法 (没看懂). Ensemble Knowledge Distillation for Learning Improved and Efficient Networks. ECAI 2020 Knowledge Distillation based Ensemble Learning for Neural Machine Translation. ICLR 2021: 机器翻译方向的文章, 主要提出了新的损失函数. A Simple Ensemble Learning Knowledge Distillation. MLIS 2020: 只有一个损失函数 ($\\mathcal{L}{CL}+\\mathcal{L}{KD}$) (???) Stochasticity and Skip Connection Improve Knowledge Transfer. ICLR 2020 Amalgamating Knowledge towards Comprehensive Classification. AAAI 2019 Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification. ECCV 2020: 主要用于解决长尾问题, 每个teacher对应几类, 用样本数相近的几类数据去训练的效果会优于从长尾分布的数据中学习. Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning. IJCAI 2019  Semi-Supervised Knowledge Amalgamation for Sequence Classification. AAAI 2021   Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation. CVPR 2019  Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning. ECCV 2020   Highlight Every Step: Knowledge Distillation via Collaborative Teaching Hydra: Preserving Ensemble Diversity for Model Distillation  Diversity Matters When Learning From Ensembles. NIPS 2021   Knowledge flow: Improve upon your teachers. ICLR 2019  Learning from Multiple Teacher Networks. You, Shan et al. KDD 2017   http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf 这是第一篇提出Multi-teacher KD的文章\n  问题: 如何进行多教师知识蒸馏? 方法: 主要改进了loss function, 由三部分组成:  teachers的softmax输出取平均和student的交叉熵 中间层表示的相对相异度 (仅适用于MTKD), 三元组 $(q_i,q_i^+,q_i^-)$, 其中$q_i$是样本$i$在中间层的表示, 偏序关系$q_i^+ \u0026gt; q_i^-$由两者和$q_i$的距离$d$决定, 参数$w_s$决定选取哪层. 在不同teacher中, 输入的三个样本的中间层表示的偏序关系可能不同, 因此用投票法决定正确的偏序关系. 设计和student对应层输出的loss, 以此鼓励student的中间层的表示空间拥有和teacher近似的结构. student和groudtruth的交叉熵.   实验设置: 基于CIFAR-10, CIFAR-100, MNIST, SVHN的实验  CIFAR-10, 比较student不同层数和参数量 (11/250K, 11/862K, 13/1.6M, 19/2.5M) 时的表现 (compression rate, acceleration rate and classification accuracy) (和Fitnets比较) CIFAR-10, student均为11层, 比较当student的参数为250K和862K时, teacher数量为1, 3, 5时, Teacher, RDL, FitNets, KD和他们的准确率 CIFAR-10, CIFAR-100, 比较不同方法 (Teacher (5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法 (19层)) 在两个数据集上的准确率 MNIST, 比较不同方法 (Teacher (4层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法 (7层)) 的准确率 SVHN, 比较不同方法 (Teacher (5层), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networks和此方法 (19层)) 的准确率   @inproceedings{you2017learning, author={You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng}, booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, title={Learning from multiple teacher networks}, pages={1285\u0026ndash;1294}, year={2017} }  A Two-Teacher Framework for Knowledge Distillation. ISNN 2019   https://link.springer.com/chapter/10.1007%2F978-3-030-22796-8_7 (找不到pdf, 只有网页版)\n  问题: single-teacher KD不行. 方法: 该框架由两个以不同策略训练的教师网络组成, 一个被严格地训练以指导学生网络学习复杂的特征 (loss为每一层表示的差), 另一个指导学生网络学习基于学到的特征进行的决策 (loss为教师和学生logits的交叉熵). 其中用到了adversarial learning的方法, 用一个discriminator分辨教师和学生的表示. 实验设置: 实验做得又少又拉, 也没跟single-teacher比, 就不写了. @InProceedings{10.1007/978-3-030-22796-8_7, title = {A Two-Teacher Framework for Knowledge Distillation}, author = {Chen, Xingjian, Su Jianbo, Zhang Jun\u0026quot;, booktitle = {Advances in Neural Networks \u0026ndash; ISNN 2019}, pages = {58\u0026ndash;66}, year = {2019} }  Feature-Level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks. ECAI 2020   https://ecai2020.eu/papers/405_paper.pdf (这篇和上次的FEED是同一篇)\n  问题: 多教师知识蒸馏在使用基于feature-map的蒸馏任务中不方便. 方法: 加入一些非线性转换层. loss function由两部分组成: student和groudtruth的交叉熵; student的feature经过n种非线性转换并归一化, 分别和teacher的feature (归一化后) 做差并求一范式. 实验设置: 数据集: CIFAR-100; 选取模型: student \u0026ndash; ResNet-56, ResNet-110, WRN28-10, ResNext29-16x64d; 没说teacher是谁. @article{Park2019FEEDFE, title={FEED: Feature-level Ensemble for Knowledge Distillation}, author={Seonguk Park and Nojun Kwak}, journal={ECAI}, year={2020} }  Ensemble Knowledge Distillation for Learning Improved and Efficient Networks. ECAI 2020   https://arxiv.org/pdf/1909.08097v1.pdf\n  问题: 由CNN组成的集成模型在模型泛化方面表现出显着改进，但代价是计算量大和内存需求大. (机翻的, 摘要提出来的问题貌似和他后面做的事没啥关系, 且这篇和上次看的倒数第二篇结构一样) 方法: 学生由多个branch组成, 每个branch和teacher一一对应, loss由三部分组成: teacher (多teacher输出相加) 和groundtruth的交叉熵, student (多branch的输出相加) 和groungtruth的交叉熵, teacher和student对应branch表示的KL散度和MSE. 结构:  实验设置: teacher \u0026ndash; ResNet14, ResNet20, ResNet26, ResNet32, ResNet44, ResNet56, and ResNet110; student \u0026ndash; a CNN with dense connections, a medium capacity CNN with 6 dense layers (DenseNet6), a large capacity CNN with 12 dense layers (DenseNet12); 数据集: EKD, CIFAR-10, CIFAR-100. @article{asif2019ensemble, title={Ensemble knowledge distillation for learning improved and efficient networks}, author={Asif, Umar and Tang, Jianbin and Harrer, Stefan}, journal={ECAI}, year={2020} }  Stochasticity and Skip Connection Improve Knowledge Transfer. Lee, Kwangjin et al. ICLR 2020   https://openreview.net/pdf?id=HklA93NYwS\n  问题: 部署多个教师网络有利于学生网络的学习, 但在一定程度上造成资源浪费. 方法: 利用单个教师网络生成多个教师网络 (加入add stochastic blocks和skip connections) 并训练学生网络, 在没有额外资源的情况下为学生网络提供足够的知识. 实验设置: 数据集 \u0026ndash; CIFAR-100 和 tiny imagenet, 并将这种方法应用到KD, AT(attention tranfer), ML. 实验中涉及到的teacher有ResNet 32, ResNet 110, WRN 28-10, MobileNet, WRN 40-4; 涉及到的student有VGG 13, ResNet 20, ResNet 32, WRN 40-4. @INPROCEEDINGS{9287227, author={Nguyen, Luong Trung and Lee, Kwangjin and Shim, Byonghyo}, title={Stochasticity and Skip Connection Improve Knowledge Transfer}, booktitle={2020 28th European Signal Processing Conference (EUSIPCO)}, pages={1537-1541}, year={2021} }  Amalgamating Knowledge towards Comprehensive Classification. AAAI 2019   https://arxiv.org/pdf/1811.02796v1.pdf\n  问题: 重用已经过训练的模型可以显著降低降低从头开始训练新模型的成本, 因为用于训练原始网络的注释通常不向公众公开. 方法: 使用multi-teacher KD的方法对多个模型进行合并, 得到轻量级的student模型. 方法分为两步: The feature amalgamation step \u0026ndash; 将teacher的每一个中间层表示都合并, 得到student对应的中间层表示. 一种简单的方式是直接concat, 但会导致student变成teacher的4倍大小. 因此在concat多个teacher的中间层表示后经过一个auto-encoder, 压缩student\u0026rsquo;s feature的同时保留重要信息; The parameter learning step \u0026ndash; 根据student相邻两层表示学习中间层参数. 损失函数由几部分组成: feature amalgamation \u0026ndash; teacher的中间层表示concat之后经过$1\\times 1$卷积得到压缩的表示, 再经过$1\\times 1$卷积试图复原, 和concat的表示之差 (的模) 作为loss; parameter learning: student前一层表示 $F_a^{l-1}$ 经过中间层后的表示 $\\hat{F}_a^l = conv(pool(activation(F_a^{l-1})))$ 和由teacher生成的下一层 $F_a^l$ 表示的差. 结构:  实验设置: 数据集 \u0026ndash; CUB-200- 2011, Stanford Dogs, FGVC-Aircraft, Cars; teacher \u0026ndash; AlexNet (在ImageNet上fine-tune) @inproceedings{shen2019amalgamating, title={Amalgamating knowledge towards comprehensive classification}, author={Shen, Chengchao and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, pages={3068\u0026ndash;3075}, year={2019} }  Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning. IJCAI 2019   https://arxiv.org/pdf/1906.10546.pdf\n  问题: 教师网络结构不一, 各自擅长解决不同的任务. 方法: student学习teacher经过转化的表示 (Common Feature Learning), 同时学习teacher输出的soft target. 结构:  实验设置: networks: alexnet, vgg-16, resnet-18, resnet-34, resnet-50. datasets: (classification) Stanford Dog, Stanford Car, CUB200-2011, FGVC-Aircraft, Catech 101; (face) CASIA, MS-Celeb-1M, CFP-FP, LFW, AgeDB-30. @inproceedings{10.5555/3367243.3367468, author = {Luo Sihui, Wang Xinchao, Fang Gongfan, Hu Yao, Tao Dapeng, Song Mingli}, title = {Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning}, booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence}, pages = {3087–3093}, year = {2019} }  Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation. CVPR 2019   https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.pdf (这篇是多教师, 但主要针对的是从多任务的教师集合中进行选择性学习, 其中只需了解第三条, 针对的是问题中的“学生选择某个老师特征？”)\n  问题: 如何利用多个针对不同任务, 在不同数据集上优化的教师模型, 来训练可以定制的可以处理选择性任务的学生? 方法: 假设没有可用的人工注释，并且每个老师可能是单任务或多任务. 首先从共享相同子任务的异构教师(Source Net)中提取特定的知识(Component Net), 合并提取的知识以构建学生网络(Target Net). 为了促进训练，作者采用了选择性学习方案，对于每个未标记的样本，学生仅从具有最小预测歧义的教师那里自适应地学习. Selective Learning: $I(p^t(x_i)) = -\\sum_ip^t(x_i)\\log(p^t(x_i))$, 针对每个样本学生只学置信度最大的教师. 实验设置: 数据集: CelebFaces Attributes Dataset (CelebA), Stanford Dogs, FGVC-Aircraft, CUB-200-2011, Cars. Source net: resnet-18; component net and target net adopt resnet-18-like network architectures. @inproceedings{shen2019customizing, title={Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation}, author={Shen, Chengchao and Xue, Mengqi and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={3504\u0026ndash;3513}, year={2019} }  Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning. ECCV 2020   https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510630.pdf (作者和\u0026quot;7\u0026quot;是同一个. 这篇文章主要针对多任务知识蒸馏, student的多个head针对不同任务, 但其中的Competi-Collaboration策略可以学习)\n  问题: 和\u0026quot;7\u0026quot;一样. 方法: 训练分为两步, 第一步整合teacher的多模态信息, 训练student的共享参数部分; 第二步用基于梯度的竞争-平衡策略, 训练student的multi-head部分, 每个部分针对不同的任务. 令$\\Omega, \\Theta_i, \\Theta_j$表示student的共享参数部分 (encoder) 和两个针对特定任务的head参数, Competi-Collaboration策略迭代如下:  Competition step: 固定student的共享参数部分, 最小化multi-head的输出和multi-teacher输出logits的差异. Collaborative step: 固定multi-head部分参数, 最小化$head_i, head_j$输出的logits和对应teacher的差, 以及student的encoder输出和teacher的encoder输出的差. 最后更新分配给$head_i, head_j$的权重 (计算loss时用到), 并归一化更新后的权重向量.   结构:  实验设置: 使用的数据集是Taskonomy dataset. teacher均使用taskonomy models, student的encoder使用ResNet-50加一个卷积层, decoder的结构根据具体任务各不相同. @InProceedings{10.1007/978-3-030-58539-6_38, author = {Luo Sihui, Pan Wenwen, Wang Xinchao, Wang Dazhou, Tang Haihong, Song Mingli}, title = {Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning}, booktitle = {Computer Vision \u0026ndash; ECCV 2020}, pages = {631\u0026ndash;646}, year = {2020} }  Highlight Every Step: Knowledge Distillation via Collaborative Teaching   https://arxiv.org/pdf/1907.09643v1.pdf (这篇里用到的attention loss还行)\n  问题: 现有的知识蒸馏方法时常忽略训练过程中与训练结果有关的有价值的信息. 方法: 本文一共使用两个teacher, 一个是从头开始训练的 (scratch teacher), 指导student走能到达最终logits的最佳路径 (loss包含和groundtruth的交叉熵, 和student soft target的L2 loss); 另一个是已经预训练好的 (expert teacher), 引导学生专注于对任务更有用的关键区域 (会计算和student中间层表示的attention loss). 结构:  实验设置: 数据集采用CIFAR-10, CIFAR-100, SVHN和Tiny-ImageNet. teachers和student均使用Wide-ResNet, 其中teachers为WRN-40-1, student采用WRN-16-1. @ARTICLE{9151346, author={Zhao Haoran, Sun Xin, Dong Junyu, Chen Changrui, Dong Zihe}, title={Highlight Every Step: Knowledge Distillation via Collaborative Teaching}, journal={IEEE Transactions on Cybernetics}, pages={1-12}, year={2020} }  Knowledge flow: Improve upon your teachers. ICLR 2019   https://openreview.net/pdf?id=BJeOioA9Y7\n  问题: 如今几乎对于所有给定的任务都可以使用现有的深度学习网络, 并且人们越来越不清楚在处理新任务时应从哪个网络开始, 或者选择哪个网络进行微调. 本文将\u0026quot;知识\u0026quot;从多个深度网络 (称为教师) 移动到一个新的深度网络模型 (称为学生). 教师和学生的结构可以任意不同, 他们也可以在具有不同输出空间的完全不同的任务上进行训练. 方法: 教师的中间层表示, 经过可训练的矩阵$Q$, 加权组成学生的中间层表示, 权重$w$也可学习. 实验设置: 主要是强化学习. @inproceedings{liu2018knowledge, title={Knowledge Flow: Improve Upon Your Teachers}, author={Iou-Jen Liu and Jian Peng and Alexander Schwing}, booktitle={International Conference on Learning Representations}, year={2019} }  Semi-Supervised Knowledge Amalgamation for Sequence Classification. AAAI 2021   https://www.aaai.org/AAAI21Papers/AAAI-1292.ThadajarassiriJ.pdf\n  问题: 每个教师在不同的训练集上训练, 导致他们对于未知的种类样本的输出是不可预测的, 并且和其他教师的输出无关. 因此在融合多教师的知识时, 一些教师有可能会给出很高置信度的错误分类. 方法: 包含两个部分, 一个是 Teacher Trust Learner (TTL), 在有标注训练集上训练对于给定输入样本, 每个教师的可信度有多高 $P(y_j\\in\\mathcal{Y}_k|X)$; 另一个是 Knowledge Amalgamator, 用于将多个教师给出的概率分布整合成在最终类别集合上的概率分布. 实验设置: Datasets: SyntheticControl (SYN), MelbournePedestrian (PED), Human Activity Recognition Using Smartphones (HAR), ElectricDevices (ELEC). Baselines: Original Teachers, SupLSTM, SelfTrain. @article{Sun2021CollaborativeTL, title={Collaborative Teacher-Student Learning via Multiple Knowledge Transfer}, author={Liyuan Sun, Jianping Gou, Lan Du, Dacheng Tao}, journal={ArXiv}, year={2021} }  Hydra: Preserving Ensemble Diversity for Model Distillation   https://arxiv.org/pdf/2001.04694.pdf\n  普通multi-teacher KD对teacher的预测值取平均, 这样会丧失多teacher结果包含的不确定性信息, 本文将student拆分成body和多个head, 每个head对应一个teacher, 以保留多teacher输出的多样性 假设有M个teacher, 首先训练一个head直至其收敛至teacher的平均, 再添加其他M-1个head, M个head一起训练, 实验证明如果没有第一个head会很难收敛. 作者定义了一个模型不确定性, 由数据不确定性和总不确定性组成(我不理解为什么是这个顺序). 结构:  实验设置:  数据集: a spiral toy dataset(用于可视化并解释模型不确定性), MNIST(测试时用了它的测试集和Fashion-MNIST), CIFAR-10(测试时用了它的测试集, cyclic translated test set, 80 different corrupted test sets 和 SVHN). 模型: toy dataset - 两层MLP, 每层100个结点; MNIST - MLP; CIFAR-10 - ResNet-20 V1. 在回归问题中, 所有数据集均使用MLP.   @article{DBLP:journals/corr/abs-2001-04694, author = {Linh Tran, Bastiaan S. Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V. Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Sebastian Nowozin, Rodolphe Jenatton}, title = {Hydra: Preserving Ensemble Diversity for Model Distillation}, journal = {CoRR}, year = {2020} }  Diversity Matters When Learning From Ensembles. NIPS 2021   https://papers.nips.cc/paper/2021/file/466473650870501e3600d9a1b4ee5d44-Paper.pdf\n  问题: 作者的假设是，一个蒸馏模型应该尽可能多地吸收集成模型内部的功能多样性, 而典型的蒸馏方法不能有效地传递这种多样性, 尤其是实现接近零训练误差的复杂模型. 方法: 首先证明了上述猜想, 随后作者提出了一种蒸馏扰动策略, 通过寻找使得集成成员输出不一致的输入来揭示多样性. 作者发现用这种扰动样本蒸馏出的模型确实表现出更好的多样性.  对于多个教师模型, 作者用ODS (输出多样性采样) 对输入样本添加扰动, 最大程度地提高集成的输出在所生成样本之间的多样性. 教师分别对添加扰动后的样本生成预测, 学生模型可以最大程度地学习教师模型的多样性. 设计的损失函数也很简单, CE + KD   结构:  实验设置: 数据集: CIFAR-10, CIFAR-100, TinyImageNet. 在CIFAR-10上用的是ResNet-32, 在CIFAR-100, TinyImageNet上用的是WideResNet-28x10. @inproceedings{nam2021diversity, title={Diversity Matters When Learning From Ensembles}, author={Nam, Giung and Yoon, Jongmin and Lee, Yoonho and Lee, Juho}, booktitle={Thirty-Fifth Conference on Neural Information Processing Systems}, year={2021} }  Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective  https://arxiv.org/abs/2102.00650v1\n  $L_{ce}=-y_k\\log \\hat{y}{k,1}^{s}, L{kd}=-\\tau^2\\sum_k\\hat{y}{k,\\tau}^t\\log \\hat{y}{k,\\tau}^s$对两种期望错误率$\\text{error}{ce},\\text{error}{kd}$进行偏置-方差分解. $L_{kd}=L_{kd}-L_{ce}+L_{ce}$, 根据上一条, $L_{kd}-L_{ce}$导致方差减小, $L_{ce}$导致偏置减小. If a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. 定义了 regularization sample: 定义两个量$a=\\dfrac{\\partial L_{ce}}{\\partial z_i}, b=\\dfrac{\\partial(L_{kd}-L_{ce})}{\\partial z_i}$, 对于一个样本, 当$b \u0026gt; a$时, 方差主导了优化的方向, 因此将这个样本定义为regularization sample. 实验表明, regularization sample的数量和训练的效果有一定的关系 (区别在于有没有 label smoothing的效果相差很大, 同时 regularization sample 的数量也相差很大; 但是$\\tau$取不同值时训练效果和 regularization sample 的数量没有明显的关系)  ","permalink":"https://michelia-zhx.github.io/posts/2022-02-24-multi_teacher_knowledge_distillation-2/","summary":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks https://arxiv.org/pdf/2004.05937.pdf Learning from Multiple Teacher Networks, KDD 2017 Efficient knowledge distillation from an ensemble of teachers. Interspeech 2017: 对teacher的logits取加权平均, 加权平均和student的logit","title":"Paper Notes - Multi-Teacher Knowledge Distillation - 2"},{"content":"Definition  Image Classification: 输入图片, 输出图中目标物体的类别. Object Localization: 输入图片, 输出图中物体的 bounding box. Object Detection: 输入图片, 输出图中物体的 bounding box 和类别.  R-CNN Model Family 采用region proposal methods, 首先生成潜在的bounding boxes, 然后采用分类器 识别这些bounding boxes区域. 最后通过post-processing来去除重复bounding boxes来进行优化.\n这类方法流程复杂, 存在速度慢和训练困难的问题.\nR-CNN R-CNN 由三个部分组成:\n Region Proposal: Generate and extract category independent region proposals. Feature Extractor: Extract feature from each candidate region. Classifier: Classify features as one of the known class.  The feature extractor used by the model was the AlexNet deep CNN that won the ILSVRC-2012 image classification competition. The output of the CNN was a 4,096 element vector that describes the contents of the image that is fed to a linear SVM for classification, specifically one SVM is trained for each known class. 问题是运行很慢, test阶段CNN要从约2000个proposed region上提取特征.\n![]({{ site.url }}/assets/object_detection/Summary-of-the-R-CNN-Model-Architecture.png) {: style=\u0026ldquo;width: 500px;\u0026rdquo; class=\u0026ldquo;center\u0026rdquo;}\nFast R-CNN 提出了R-CNN的三个限制:\n Training is a multi-stage pipeline. Training is expensive in space and time. Object detection is slow.  Fast R-CNN只训练一个模型去学习物体位置和分类, 而不是一个pipeline, 输入是一个region proposal的集合, 经过deep CNN, 进行特征提取. CNN的结尾是 Region of Interest Pooling Layer (ROI Pooling), 针对输入的candidate 进行特征提取. CNN的输出送入一个FC, 得到两个输出, 一个用于softmax layer预测类别, 另一个用于regression用于生成bounding box.\n这个过程针对每一个region of interest进行循环.\n![]({{ site.url }}/assets/object_detection/Summary-of-the-Fast-R-CNN-Model-Architecture.png) {: style=\u0026ldquo;width: 500px;\u0026rdquo; class=\u0026ldquo;center\u0026rdquo;}\nFaster R-CNN 由两部分组成:\n Region Proposal Network: Convolutional neural network for proposing regions and the type of object to consider in the region. Fast R-CNN: Convolutional neural network for extracting features from the proposed regions and outputting the bounding box and class labels.  ![]({{ site.url }}/assets/object_detection/Summary-of-the-Faster-R-CNN-Model-Architecture.png) {: style=\u0026ldquo;width: 500px;\u0026rdquo; class=\u0026ldquo;center\u0026rdquo;}\nRPN网络接受CNN的输出, feature map送入一个小型网络得到许多region proposals, 每个对应一个分类. Region proposals是bounding boxes, 或者说anchor boxes, 后续优化. Class prediction is binary, indicating the presence of an object, or not, so-called \u0026ldquo;objectness\u0026rdquo; of the proposed region.\nYOLO Model Family YOLO 训练单独一个神经网络, 端到端, 接受一个图片直接预测bounding box. 准确率不高但是速度快.\nYOLO首先将图像分为 $$S\\times S$$ 的格子 (grid cell). 如果一个目标的中心落入格子, 该格子就负责检测该目标. (即使一个对象跨越多个网格, 它也只会被分配到其中点所在的单个网格. 可以通过增加更多网格来减少多个对象出现在同一网格单元中的几率.)\n每一个格子 (grid cell) 预测bounding boxes(B)和该boxes的置信值(confidence score). 置信值代表box包含一个目标的置信度. 然后, 我们定义置信值为 $$Pr(Object)*IOU^{truth}_{pred}$$ . 如果没有目标, 置信值为零. 另外, 我们希望预测的置信值和ground truth的intersection over union (IOU)相同.\n每一个bounding box包含5个值: $$x, y, w, h$$ 和confidence. $$(x, y)$$ 代表与格子相关的box的中心. $$(w, h)$$ 为与全图信息相关的box的宽和高. confidence代表预测boxes的IOU和gound truth. (IOU = 交叉面积/联合的面积)\n每个格子(grid cell)预测条件概率值C $$Pr(Class_i|Object)$$ , 概率值C代表了格子包含一个目标的概率, 每一格子只预测一类概率. 在测试时, 每个box通过类别概率和box置信度相乘来得到特定类别置信分数: $$Pr(Class_i|Object)*Pr(Object)*IOU^{truth}{pred} = Pr(Class_i)*IOU^{truth}{pred}$$\n这个分数代表该类别出现在box中的概率和box和目标的合适度. 例子讲解\n![]({{ site.url }}/assets/object_detection/Summary-of-Predictions-made-by-YOLO-Model.png) {: style=\u0026ldquo;width: 500px;\u0026rdquo; class=\u0026ldquo;center\u0026rdquo;}\n当一个目标不止一次被识别, 非极大值抑制可以显着提高YOLO的效果.\nYOLO相对于传统方法有如下有优点：\n 非常快. YOLO预测流程简单, 速度很快. 我们的基础版在Titan X GPU上可以达到45帧/s; 快速版可以达到150帧/s. 因此, YOLO可以实现实时检测. YOLO采用全图信息来进行预测. 与滑动窗口方法和region proposal-based方法不同, YOLO在训练和预测过程中可以利用全图信息. Fast R-CNN检测方法会错误的将背景中的斑块检测为目标, 原因在于Fast R-CNN在检测中无法看到全局图像. 相对于Fast R-CNN, YOLO背景预测错误率低一半. YOLO可以学习到目标的概括信息 (generalizable representation), 具有一定普适性. 我们采用自然图片训练YOLO, 然后采用艺术图像来预测. YOLO比其它目标检测方法 (DPM和R-CNN) 准确率高很多.  YOLOv2 (YOLO9000) and YOLOv3 YOLOv2 model makes use of anchor boxes, pre-defined bounding boxes with useful shapes and sizes that are tailored during training.\nThe choice of bounding boxes for the image is pre-processed using a k-means analysis on the training dataset.\n重要的是, 更改了边界框的预测表示形式, 以允许较小的更改对预测产生较小的影响, 从而产生更稳定的模型. 不是直接预测位置和大小, 而是预测偏移量, 以相对于网格单元移动和重塑预定义的锚框, 并通过逻辑函数对其进行阻尼.\n[YOLOF: You Only Look One-level Feature](CVPR2021: https://arxiv.org/pdf/2103.09460.pdf) Problem: Address optimization problem by utilizing only one-level feature for detection. Two key components, Dilated Encoder and Uniform Matching are proposed and bring considerable improvements.\nPerformance: YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being $$2. 5\\times$$ faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with $$7\\times$$ less training epochs.\nSSD: Single Shot MultiBox Detector  问题: RCNN系列为two-stage方法, 先预先回归一次边框, 再进行骨干网络训练, 所以精度更高, 但速度方面有待提升. YOLO为one-stage方法, 只做一次边框回归和打分, 速度快但对小目标效果差, 对尺寸敏感. 使用one-stage思想, 融入Faster R-CNN中的anchor思想, 做了特征分层提取并以此计算边框回归和分类操作, 因此可以适应多尺度目标的训练和检测任务. 在每个stage中根据feature map的大小按照固定的ratio和scale生成default boxes. 例如conv9的输出feature map为5*5, 每个点默认生成6个box, 因此一张feature map上有5*5*6=150个default boxes, 而后每个default box将生成(c+1+4)维的特征向量, 其中c是类别数, 1代表背景, 4是box的偏移和缩放尺度. SSD的backbone是VGG16, 将最后的fc6和fc7转化成conv6和conv7, 再在之后加上不同尺度的conv8, 9, 10, 11四个卷积网络层. 联合损失函数 $L(x,c,l,g) = \\dfrac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))$, 其中$L_{conf}$代表分类误差, 使用softmax; $L_{loc}$代表回归误差, 采用smooth L1 loss.  $L_{loc}(x,j,g) = \\sum_{i\\in Pos}^N\\sum_{m\\in{cx,cy,w,h}}x_{ij}^ksmooth_{L1}(l_i^m - \\hat{g}_j^m)$ $L_{conf}(x,c) = -\\sum_{x\\in Pos}x_{ij}^p\\log(\\hat{c}{i}^p) - \\sum{i\\in Neg}\\log(\\hat{c}i^0), where\\ \\hat{c}{i}^p = \\dfrac{\\exp(c_i^p)}{\\sum_p\\exp(c_i^p)}$   训练策略:  匹配策略: 第一步是根据最大的overlap将ground truth和default box进行匹配, 第二步是将default boxes与overlap大于某个阈值的fround truth进行匹配. Default Boxes生成器: $S_k = S_{min}+\\dfrac{S_{max}-S_{min}}{m-1}(k-1), k\\in[1,m], ratio: a_r\\in{1,2,\\dfrac{1}{2}, 3, \\dfrac{1}{3}}$ Hard Negative Mining: 根据confidence loss对所有box进行排序, 选取置信度误差较大的top-k作为负样本, 使得正负样本比例控制在1:3之内.    PS: 正负样本怎么用啊啊啊啊\n","permalink":"https://michelia-zhx.github.io/posts/2021-09-12-object_detection/","summary":"Definition Image Classification: 输入图片, 输出图中目标物体的类别. Object Localization: 输入图片, 输出图中物体的 bounding box. Object Detection: 输入图片, 输出图中物体的 bounding box 和类别. R-CNN Model Family 采用region proposal methods, 首","title":"Paper Notes - Object Detection"},{"content":" Why self-supervised learning: 主要的问题在于获取数据及其标注部分. Definition:  Self-supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one: \u0026ldquo;Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?\u0026rdquo; Replace the human annotation block by creatively exploiting some property of data to set up a pseudo-supervised task. 学到这些特征后, 利用迁移学习将特征在下游的监督任务上进行微调, 从而利用更少的数据进行训练.    Self-Supervised Learning from Image Pattern 1: Reconstruction  Image Colorization: 生成彩色和黑白的图片对, 黑白图片经过encoder-decoder生成预测的图片, 和真是图片计算L2 loss. 如此可以学到图片中的物体及其关联部分以正确上色. Image Superresolution: 基于GAN的SRGAN, 低分辨率的图片经过卷积网络生成高分辨率图片, 计算和原图的MSE. 同时将生成的图片和真实图片送入二分类器, 判断哪个是真实图片, 以此使生成器学到如何生成具有细节的高分辨率图片. Image Impainting: 将图片扣掉一块, 由生成器生成完整的图片, 后续步骤同上. Cross-Channel Prediction: 一张图分成灰度图和颜色信道, 用灰度图预测颜色信道, 用颜色信道预测灰度图, 将预测的两个channel复合成预测的图片, 和原图对比计算loss以改进模型.  Pattern 2: Common Sense Tasks  Image Jigsaw Puzzle: 图像分成9个patch, 按照汉明距离最大的64种打乱方式打乱, 输入是9个patch, 需要预测是按照哪一种方式 (1-64) 打乱的. 这一任务要求模型学习物体里的各部分是如何组合的, 以及物体的形状. Context Prediction: 图中取9个相邻的patch选取中心的patch和周围随意一个patch, 输入和前一条类似的网络, 预测两者位置关系. Geometric Transformation Recognition: 将旋转后的图片输入, 预测旋转了多少度 (0, 90, 180, 270). 这个任务要求模型学习物体的地点, 类型和造型.  Pattern 3: Automatic Label Generation  Image Clustering: 在无标注数据集上对图片进行聚类. Deep Clustering, 图片首先被聚类, 然后这些类才被作为classes. Synthetic Imagery: 用game engine生成人造图片, 和真实的图片送入convnet, 并判断生成的特征是属于人造的or真实存在的场景图片.  Self-Supervised Learning from Video  Frame Order Verification: (不)打乱帧的顺序, 送入convnet, 预测输入顺序是否是正确的.  SimCLR  Contrastive Learning: It attempts to teach machines to distinguish between similar and dissimilar images. 需要构造相似的图片对和不相似的图片对. 一张图片被经过随机变换后和原图构成相似图片对 $(x_i,x_j)$, 两张图片分别送入encoder以获取表示 $(h_i,h_j)$, 在经过非线性全连接层以获得新的表示 $(z_i,z_j)$, 最后判断 $(z_i,z_j)$ 的相似性.  步骤  Self-supervised Formulation  Batch Size = N Transformation function T = random(crop + flip + color jitter + grayscale).   Getting Representation  针对一个batch里的所有图片做随机变换, 共获得2N张图片. 增强后的图片对经过共享参数的encoder生成表示$(h_i, h_j)$. 作者使用ResNet-50, 输出的特征为2048维向量.   Projection Head  Projection Head g(·) = Dense + Relu + Dense $(h_i, h_j)$经过 g 生成新的表示$(z_i, z_j)$, 在他们之间计算相似度.   Tuning Model, 针对N张图片生成的增强后的2N个表示, 进行如下操作  计算cosine相似度, 同一张图增强后相似性高, 其余很低 将2N张图片组成很多对, 用softmax计算两张图片相似的概率, 除了同一张图生成的两个表示很相似, 其余的图片对全部是负例. 且这个方法不需要memory bank, queue等特别的结构. 计算损失: $l(i,j) = -\\log\\dfrac{\\exp(s_{i,j})}{\\sum_{k=1}^{2N}l_{[k!=i]}\\exp(s_{i,k})}$ 一个batch的损失: $L = \\dfrac{1}{2N}\\sum_{k=1}^M[l(2k-1, 2k) + l(2k, 2k-1)]$ 优化这个损失函数可以提升图片表示能力, 是模型学会区分图片是否相似 (如果两张图片属于同一类是否需要更高的相似度? 但是没有标注, 无法判断两张照片是否属于同一类.)    Downstream Tasks PIRL: Pretext-Invariant Representation Learning  存在的问题: 随机变换有可能导致不同的图生成相似的新图. 解决方法: 使同一张图变换后的新图尽可能相似, 不同图变换后的新图尽可能不相似.  PIRL Framework  原图经过一个转换后生成新图, 两张图经过共享参数的convnet $\\theta$ 后得到两个表示$V_I, V_{I^T}$, $V_I$经过projection head $f$后得到新的表示 $f(V_I)$, $V_{I^T}$经过projection head $g$后得到新的表示 $g(V_{I^T})$. 优化损失函数使得两个表示尽量相似, 而$f(V_I)$和memory bank里其他图生成的表示尽量不相似. 用memory bank存储别的图像生成的表示, 以避免更大的batch size 损失函数: $h(f(V_I), g(V_{I^T})) = \\dfrac{\\exp(\\frac{s(f(V_I), g(V_{I^T}))}{\\tau})}{\\exp(\\frac{s(f(V_I), g(V_{I^T})}{\\tau}) + \\sum_{I'\\in D_N}\\exp(\\frac{g(V_{I^T}), s(f(V_{I'})}{\\tau})}$ $L_{NCE}(I, I^t) = -\\log[h(m_I, g(V_{I^t}))] - \\sum_{I'\\in D_N}\\log[1-h(g(V_{I^t}), m_{I'})]$  Self-Labelling Combining clustering and representation learning together to learn both features and labels simultaneously.\n 生成标签然后用标签训练一个模型 由训练好的模型生成新的标签 重复以上操作 以上形成鸡蛋问题, 最初用随机初始化的Alexnet, 在Imageet上评估.  Self-Labelling Pipeline  利用一个随机初始化的模型为增强后的无标签数据生成标签 使用Sinkhorn-Knopp算法将无标签图片聚类, 得到新的标签 利用新生成的标签训练模型上, 利用交叉熵损失优化 重复以上步骤  Sinkhron-Knopp Algorithm: 源于最优运输问题 (仓库, 商店, 运输距离) - 原问题: 将N个样本分到K个类中 - 限制条件: N个样本被均分到K个类中 - 代价矩阵: 将 使用这种分类方式训练出的模型的模型表现 作为这种划分方式的代价. 如果代价很高, 说明需要调整划分方式以接近理想效果.\nFixMatch for Semi-Supervised Learning  Intuition: 数据集中只有少部分标注数据和大部分未标注数据, 先用标注数据有监督训练一个模型. 对于未标注数据, 将所有图片做两种变换, 分别送入之前训练的模型, 由于是同一张图片生成的输入, 因此模型的输出应该相同. 模型结构:  将已标记数据用于训练, 交叉熵损失作为监督学习的损失函数: $l_s = \\dfrac{1}{B}\\sum_{b=1}^BH(p_b, p_m(y|\\alpha(x_b)))$ 伪标记: 对于一张未标记图片, 先对它进行weak augmentation, 输入模型得到伪标签, 再和strongly augmented image的输出对比. 针对未标记数据, 半监督部分损失函数: $l_u = \\dfrac{1}{\\mu B}\\sum_{b=1}^{\\mu B}1(max(q_b\\geq \\tau)H(\\hat{q}_b, p_m(y|A(u_b))))$, $loss = l_s + \\mu l_u$, 其中$\\tau$代表伪标记生成的阈值, 当weakly augmented image的softmax超过阈值则生成伪标记, 并和strongly augmented image的结果计算较差熵. 在最终loss的计算中, 随着训练的进度$\\mu$逐渐增大, 具体可以解释为前期对用少量样本训练出的模型不信任, 后期会逐渐加入对未标记数据的考量.    DeepCluster  主要思想: 图片经过augmentation之后送入convnetm 取分类之前的特征, 经过pca降维, 用kmeans聚类得到各个类别. 将kmeans的结果作为伪标记, 和convnet输出的表示经过分类层得到的结果计算交叉熵.  ","permalink":"https://michelia-zhx.github.io/posts/2021-09-05-self_supervised_learning/","summary":"Why self-supervised learning: 主要的问题在于获取数据及其标注部分. Definition: Self-supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one: \u0026ldquo;Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?\u0026rdquo; Replace","title":"Paper Notes - Self-Supervised Learning"},{"content":"Mining Typhoon Knowledge with Neural Networks \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999  需解决的问题: 神经网络的两个缺点 \u0026ndash; 数据量大, 训练时间长; 神经网络对知识的学习果不能直接用于决策. Fast neural model - FTART (Firld Theory based Adaptive Resonance Theory): 隐层的激活函数是Sigmoid函数, 输入层和第二层之间使用Gaussian权重, 并更新. 第二层用于分类输入,第三层用于分类输出, 在这两层之间建立关系来进行有监督学习. Rule extraction algorithm - SPT (Statistic based Producing and Testing):  用大量实例来训练一个神经网络 结合输入和神经网络的输出来构造一个虚拟示例集, 如果存在多个输入分量的组合, 且投影到它时等价的示例属于某个类的概率为$\\lambda$, 则通过将组合作为前因, 类别作为后继来构造规则. 如果没有这样的组合, 则选择具有最佳聚类效果的连续输入组件并离散化. (?) 如何去噪: (?).    FANRE: A Fast Adaptive Neural Regression Estimator \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999  Adaptive Resonance Theory: 像是一个KNN网络, 可以实现神经元的动态扩充. 对于新来的样本, 和之前的聚类中心进行比较, 如果符合阈值设定(形成共振), 则单独训练与之匹配的聚类中心对应的神经元的相关链接权重, 其他神经元保持不变; 如果所有神经元都不匹配, 那就创造一个新的神经元分配给这个数据形成新的一类. FANRE的结构: 输入层, 输出层, 中间两层隐层. 最初隐层为空, 随输入动态添加隐层结点. 增量学习, 每个样例只过一遍.  两个参数$\\theta_{ij}, \\alpha_{ij}$, 代表从第一层的$unit_i$到第二层的$unit_j$的高斯权重的响应中心和响应特征宽度. $Err_{max}$: 最大容错值, $Vig_1$: first-degree vigilance, $Vig_2$: second-degree vigilance. 且有$Err_{max} \u0026lt; Vig_1 \u0026lt; Vig_2$   FANRE的学习流程: 输入新样例 $\\Rightarrow$ 第二层竞争 $\\Rightarrow$ 第三层竞争 $\\Rightarrow$ 计算网络输出:(*)  $Err \u0026lt; Err_{max}$ [已有的attracting basin能覆盖当前样例, 不需要调整] $\\Rightarrow$ 下一个样例(**) $Err \\geq Err_{max}$:  $Err_{max} \\leq Err \u0026lt; Vig_1$ [虽然总体的近似表现不尽如人意, 但内部对输入输出模式的近似还是可用的] $\\Rightarrow$ 调整$\\theta_{ij}'$和$\\alpha_{ij}$, 回到(*) $Vig_1 \\leq Err \u0026lt; Vig_2$ [由结点$u$表示的对输出的近似可用, 但由第二层结点表示的对输入的近似不合适] $\\Rightarrow$ 第二层添加一个结点, 回到(**) $Err \\geq Vig_2$ [由结点$u$表示的对输出的近似, 和由第二层结点表示的对输入的近似均不合适] $\\Rightarrow$ 第二层和第三层各添加一个结点, 回到(**)      Ensemble of GA based Selective Neural Network Ensembles \u0026ndash; Jian-Xin Wu, Zhi-Hua Zhou, Zhao-Qian Chen - 2002 $\\textbf{GASEN}$\n $N$个基学习器$f_i:\\mathbf{R}^m\\rightarrow\\mathbf{R}^n$加权: $\\overline{f}(x) = \\sum_{i=1}^Nw_if_i(x)$ 设对于输入$x$, 期望输出为$d(x)$, 则基学习器和ensemble的误差为 $$E_i(x) = (f_i(x)-d(x))^2, E(x) = (\\overline{f}(x)-d(x))^2$$ 设$x$服从分布$p(x)$, 则基学习器和ensemble在分布上的泛化误差为 $$E_i = \\int p(x)E_i(x)\\rm{d}x, E = \\int p(x)E(x)\\rm{d}x$$ 平均误差为$\\overline{E}(x) = \\sum_{i=1}^Nw_iE_i(x)$, 平均泛化误差为$\\overline{E} = \\int p(x)\\overline{E}(x)\\rm{d}x$ Ambiguity of the i-th learner on input $x$: $$A_i(x) = (f_i(x)-\\overline{f}(x))^2, A_i = \\int p(x)A_i(x)\\rm{d}x$$ $$\\overline{A}(x)=\\sum_{i=1}^Nw_iA_i(x), \\overline{A} = \\int p(x)\\overline{A}(x)\\rm{d}x$$ 定义ensemble的泛化性能: $E = \\overline{E} - \\overline{A}$ 两个基学习器的相关性: $$C_{ij} = \\int p(x)(f_i(x)-d(x))(f_j(x)-d(x))\\rm{d}x$$ $$E = \\sum_{i=1}^N\\sum_{j=1}^Nw_iw_jC_{ij}$$ 当对于第$k$个基学习器, 满足 $$(2N-1)\\sum_{i=1,i\\neq k}^N\\sum_{j=1,j\\neq k}^NC_{ij} \u0026lt; 2(N-1)^2\\sum_{i=1,i\\neq k}^NC_{ik} + (N-1)^2E_k$$  $\\textbf{e-GASEN}$ 先用GASEN算法训几个ensembles, 再用简单的集成算法把这几个ensembles集成起来.\nHybrid Decision Tree \u0026ndash; Zhi-Hua Zhou, Zhao-Qian Chen, 2002  处理有序属性 \u0026ndash; 定量分析; 无序属性 \u0026ndash; 定性分析. 结合symbolic leanring (无序属性) 和 neural learning (有序属性). 树的扩展: 将属性集分为无序属性$\\mathcal{L}_0$和有序属性$\\mathcal{L}_1$, 先将HDT按照无序属性扩展, 当树的分支因为结点里的样本属于同一类别而无法扩展时, 终止扩展; 当因为结点中无序属性均被使用过而无法扩展时, 将这个叶结点标记为neural node. Neural Processing: 落入neural node的样本被连续属性集重新表示, 并被归一化. 然后使用FANNC做分类 (增量学习) 几种增量学习:  E-IL (Example-Incremental Learning) —— 新的样本到来时, 保证学到新知识的同时不要牺牲过多的旧知识. 非增量学习方法会有灾难性遗忘的缺陷. C-IL (Class-Incremental Learning) —— 当新样例属于新类时, 学到新的知识且不用牺牲太多旧的知识 (例如重新学习整个系统) A-IL (Attribute-Incremental Learning) —— 当新样例带有新属性时, 学到新的知识且不用牺牲太多旧的知识 (例如重新学习整个系统)    Face recognition with one training image per person \u0026ndash; Jian-Xin Wu, Zhi-Hua Zhou, 2002  人脸识别算法主要有两种类别: geometric feature-based and template-based techniques. PCA属于后者, 但没有考虑标签信息; 而考虑标签信息的每个类别至少需要两张图片 (LDA). $(PC)^2A$ —— projection-combined principal component analysis: 针对每张图片对其进行变换  $x\\in [1,N_1], y\\in [1,N_2], P(x,y)\\in [0,1]$, $P(x,y)$是灰度图 $V_P(x) = \\sum_{y=1}^{N_2}P(x,y), H_P(y) = \\sum_{x=1}^{N_1}P(x,y)$ $\\overline{P} = \\dfrac{\\sum_{x=1}^{N_1}\\sum_{y=1}^{N_2}P(x,y)}{N_1N_2}, M_P(x,y) = \\dfrac{V_P(x)H_P(y)}{N_1N_2\\overline{P}}$ $P_{\\alpha}(x,y) = \\dfrac{P(x,y)+\\alpha M_P(x,y)}{1+\\alpha}$ $P_{\\alpha}'(x,y) = \\dfrac{P_{\\alpha}(x,y) - \\min(P_{\\alpha}(x,y))}{\\max(P_{\\alpha}(x,y)) - \\min(P_{\\alpha}(x,y))}$   最后在projection-combined version of image $P_{\\alpha}(x,y)$上使用$PCA$.  Learning a Rare Event Detection Cascade by Direct Feature Selection \u0026ndash; Jianxin Wu, James M.Rehg, Matthew D.Mullin, 2003  人脸检测是稀有事件检测的典型例子, 给一些人脸大小的图片, 其中很少的部分会包含人脸, target patterns occur with much lower frequency than non-targets. 搜索-分类: 搜索图片中可能的区域, 再判别是否包含脸. Viola-Jones framework 包含三个元素: 层叠式结构, 一些长方形特征, 基于AdaBoost的算法 - 在每个分类器中构造长方形特征的ensemble. 每个分类器拒绝一部分不包含人脸的区域, 并使包含人脸的通过. 在每个结点, 给定一个训练集${x_i,y_i}$, 训练目标是从总共$F$个特征中选出一些弱分类器${h_t}$, 集成的分类器$H_i$需要有很高的检测率$d_i$和中等的假正率$f_i$, 则整个层叠式模型的检测率$d = \\prod_{i=1}^nd_i$和假正率$f = \\prod_{i=1}^nf_i$, 可以保证有较高的检测率和很低的假正率. 第$t$轮boosting后, ensemble表示为 $$H(x) = \\left{\\begin{aligned} 1\\quad \u0026amp; \\sum_{t=1}^T\\alpha_th_t(x) \\geq \\theta\\ 0\\quad \u0026amp; otherwise \\end{aligned}\\right.$$ 训练一个结点的过程:  训练所有的弱分类器, (*)判断是否$d \u0026gt; D?$  yes $\\Rightarrow$ 添加这个特征以最小化ensemble的假正率 no $\\Rightarrow$ 添加这个特征以最大化ensemble的检测率 (以上的最大化和最小化均通过穷举法完成, 选择加入ensemble后能给ensemble带来最大提升的classifier)   如果 $f \\geq F or d \\leq D$ $\\Rightarrow$ 返回(*)   和Viola-Jones相比, 本算法在每个结点每个弱学习器之训练一次, 而Viola-Jones算法中每个弱学习器每针对一个特征就要训练一次.(?)  A Scalable Approach to Activity Recognition based on Object Use \u0026ndash; Jianxin-Wu, Adebola Osuntogun, Tanzeem Choudhury, ICCV 2006  使用动态贝叶斯网络 (Dynamic Bayesian Network), 从视频中稀疏且有噪声的RFID传感器数据和一些活动的常识学习训练模型. Object-use Based Activity Recognition:  $A^t, O^t, R^t, V^t$分别代表活动, 使用的物体, RFID和视频帧. DBN具有的参数: 先验$P(A^1)$, 观测模型$P(O^1|A^1), P(O^{t+1}|O^{t}, A^{t+1})$, 状态转移模型$P(A^{t+1}|A^{t})$, 输出模型$P(V^t|O^t), P(R^t|O^t)$ 如何确定正在使用的物体 —— 除了RFID传感器数据, 借助视频, 将像素组成$8\\times 8$的superpixels, 对比当前帧$t$和$t-3$$t+3$两帧的superpixels, 计算差距, 若差距均超过阈值则将其中的物体认定为正在使用的物体. 将segmented area中提取SIFT特征, 将视频帧看成的集合$V^t = (v^t_1,v^t_2,\u0026hellip;,v^t_{n^t})$, 其中任两个SIFT特征相互独立, 且$P(V^t|O^t) = \\prod_{i=1}^{n^t}P(v^t_i|O^t) = \\prod_{i=1}^{n^t}\\mathbf{h}_{O^t}(v_i^t)$   Learning object models w/o human labeling  在EM算法中使用RFID readings和common knowledge去学习object models. E步: 估计给定$R^t, V^t, \\mathbf{h}_{O^t}(v_i^t)$时$O^t$的边际概率 用standard junction tree算法估计每一个时刻$O^t$的边际概率 给定$O^t$的边际概率, $V^t$和$A^t$独立 M步: 计数, 更新$\\mathbf{h}_{O^t}(v_i^t)$   Specify parameters from domain knowledge  不想细看了    P.S. without = w/o\n","permalink":"https://michelia-zhx.github.io/posts/2021-07-15-pros_paper_notes/","summary":"Mining Typhoon Knowledge with Neural Networks \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999 需解决的问题: 神经网络的两个缺点 \u0026ndash; 数据量大, 训练时间长; 神经网络对知识的学习果不能直接用于决策. Fast neural model - FTART (Firld Theory","title":"Paper Notes - Week 1"},{"content":"Multi-Instance Multi-Label Learning with Application to Scene Classification \u0026ndash; Zhi-Hua Zhou, Min-Ling Zhang, NIPS 2006  Multi-instance: 一个example包含多个instance, example只对应1个label; Multi-label: 一个example对应多个label. 以Multi-instance或Multi-label为桥梁, 将multi-instance multi-label learning ($f_{MIML}: 2^{\\mathcal{X}}\\rightarrow 2^{\\mathcal{Y}}$) 转化为传统机器学习任务.  方法一 (通过Multi-instance): 先转化为$f_{MIL}:2^{\\mathcal{X}}\\times \\mathcal{Y}\\rightarrow {-1. +1}$, 再转化为$f_{SISL}: \\mathcal{X}\\times\\mathcal{Y}\\rightarrow{-1, +1}$, $f_{MIL}(X_i, y) = sign[\\sum_{i=1}^{n_i}f_{SISL}(x_{j}^{(i)}, y)]$ 方法二 (通过Multi-label): 先转化为$f_{MLL}:\\mathcal{Z}\\rightarrow 2^{\\mathcal{Y}}$, 对于任意$z_i\\in \\mathcal{Z}, f_{MLL}(z_i) = f_{MIML}(X_i)$ if $z_i\\in\\phi(X_i), \\phi: 2^{\\mathcal{X}}\\rightarrow\\mathcal{Z}$, 再转化为$f_{SISL}:\\mathcal{Z}\\times\\mathcal{Y}\\rightarrow{-1, +1}$, $f_{MLL}(z_i) = {y|\\arg_{y\\in\\mathcal{Y}}[f_{SISL}(z_i,y)=+1}$   MINIBOOST (在每轮boosting中试图将$\\mathcal{F}(B)$扩展为$\\mathcal{F}(B)+cf(B)$)  将每个MIML样本 $(X_u, Y_u)$ 转化为 $|\\mathcal{Y}|$ 个multi-instance bags ${[(X_u,y_1), \\Psi(X_u,y_1)],\u0026hellip;,[(X_u,y_{|\\mathcal{Y}|}), \\Psi(X_u,y_{|\\mathcal{Y}|})]}$ 初始化每个bag的权重$W^{(i)} = \\dfrac{1}{m\\times |\\mathcal{Y}|}$ 对于迭代次数$t=1,2,\u0026hellip;,m\\times |\\mathcal{Y}|$:  令$W_j^{(i)} = W^{(i)}/n_i$, 将bag 的label $\\Psi(X^{(i)},y^{(i)})$赋给其中的instance $(x_j^{(i)}, y^{(i)})$， 训练一个 instance-level的学习器 $h_t[(x_j^{(i)}, y^{(i)})]$. 对第 $i$个bag, 计算其错误率, $e^{(i)} = \\dfrac{\\sum_{i=1}^{n_i}\\llbracket{h_t[(x_j^{(i)}, y^{(i)})]\\neq\\Psi(X^{(i)},y^{(i)})}\\rrbracket}{n_i}$ 若 $e^{(i)} \u0026lt; 0.5$ 对所有 $i\\in [1,2,\u0026hellip;,m\\times|\\mathcal{Y}| ]$, 则跳出循环 计算$c_t=\\argmin_{c_t}\\sum_{i=1}^{m\\times|\\mathcal{Y}}W^{(i)}\\exp[(2e^{(i)}-1)c_t]$ 如果$c_t \u0026lt; 0$, 则跳出循环 令 $W^{(i)} = W^{(i)}\\exp[(2e^{(i)}-1)c_t]$, 重新正则化使得 $0\\leq W^{(i)}\\leq 1$且$\\sum_{i=1}^{m\\times|\\mathcal{Y}| }W^{(i)} = 1$   返回 $Y^* = {y|\\arg_{y\\in\\mathcal{Y}}sign(\\sum_j\\sum_tc_th_t[(x_j^,Y)])=+1}$ ($x_j^$是$X^*$的instance) P.S. 损失函数为: $$\\begin{aligned} E_{\\mathcal{B}}E_{\\mathcal{G}|\\mathcal{B}}[\\exp(-g\\mathcal{F}(B)+c(-g\\mathcal{F}(B)))] \u0026amp;= \\sum_i W^{(i)}\\exp[c\\left(-\\dfrac{g^{(i)}\\sum_jh(\\mathbf{b}_j^{(i)})}{n_i}\\right)]\\ \u0026amp;= \\sum_iW^{(i)}\\exp[(2e^{(i)}-1)c] \\end{aligned}$$   MINISVM  对MIML样本 $(X_u, Y_u), \\Gamma = {X_u|u=1,2,\u0026hellip;,m}$ 从$\\Gamma$中随即选择$k$个元素初始化$M_t$, 重复以下直到$M_t$不再变化:  $\\Gamma_t = {M_t}$ (t=1,2,\u0026hellip;,k) 对于每一个 $X_u\\in (\\Gamma-{M_t|t=1,2,\u0026hellip;,k})$:  $index = \\argmin_{t\\in{1,\u0026hellip;,k}}d_H(X_u, M_t), \\Gamma_{index} = \\Gamma_{index} \\cup {X_u}$   $M_t = \\argmin_{A\\in\\Gamma_t}\\sum_{B\\in\\Gamma_t}d_H(A, B)$ (t=1,2,\u0026hellip;,k) 将 $(X_u, Y_u)$ 转化为 multi-instance 样本 $(z_u, Y_u), z_u = (z_{u1}, z_{u2}, \u0026hellip;, z_{uk}) = (d_H(X_u, M1), d_H(X_u, M_2),\u0026hellip;,d_H(X_u,M_k))$ 对于每个$y\\in\\mathcal{Y}$, 生成一个数据集 $\\mathcal{D} = {(z_u,\\Phi(z_u,y))|u=1,2,\u0026hellip;,m}$, 然后训练一个SVM $h_y = SVMTrain(\\mathcal{D}_y)$ 返回 $Y^* = {\\argmax_{y\\in\\mathcal{Y}}h_y(z^)}\\cup{y|h_y(z^) \\geq 0, y\\in\\mathcal{Y}}$ P.S. Hausdorff distance: $d_H(A,B) = \\max{\\max_{a\\in A}\\min_{b\\in B}|\\mathbf{a} - \\mathbf{b}|, \\max_{b\\in B}\\min_{a\\in A}|\\mathbf{b} - \\mathbf{a}|}$      On Multi-Class Cost-Sensitive Learning \u0026ndash; Zhihua Zhou, Xuying Liu, AAAI 2006  rescale classes 针对二分类问题有很好的效果, 但是针对多分类代价敏感问题效果不好. 原理: rescale让代价不敏感的算法变得代价敏感, 对于二分类问题, $p = P(class=1|\\mathbf{x})$, 作出最优选择的阈值$p^$满足 $$p^\\times\\epsilon_{11}+(1-p^)\\times\\epsilon_{21} = p^\\times\\epsilon_{12}+(1-p^*)\\times\\epsilon_{22}$$ Elkan Theorem: 对应原先的概率阈值$p_0$设置新的阈值$p^$, 则第二类的个数应乘以$\\dfrac{p^}{1-p^*}\\dfrac{1-p_0}{p_0}$. 推广到多分类, rescale时$i$类相比于$j$的比例是$\\tau_{opt}(i,j)=\\dfrac{\\epsilon_{ij}}{\\epsilon_{ji}}, \\epsilon_i = \\sum_{j=1}^c\\epsilon_{ij}, w_i = \\dfrac{(n\\times\\epsilon_i)}{\\sum_{k=1}^cn_k\\times\\epsilon_k}, w_i$是赋给每个类样本个数的权重. 传统rescale方法赋的比例$\\tau(i,j) = \\dfrac{w_i}{w_j} = \\dfrac{\\epsilon_i}{\\epsilon_j}$, 和上一条相比只在$c=2$时一样, 其余情况不等, 因此传统rescale方法针对多分类情况会不适用. The $RESCALE_{new}$ Approach $$\\begin{aligned} \\dfrac{w_1}{w_2} = \\dfrac{\\epsilon_{12}}{\\epsilon_{21}}, \\dfrac{w_1}{w_3} = \\dfrac{\\epsilon_{13}}{\\epsilon_{31}}, \u0026hellip;, \\dfrac{w_1}{w_c} \u0026amp;= \\dfrac{\\epsilon_{1c}}{\\epsilon_{c1}}\\ \\dfrac{w_2}{w_3} = \\dfrac{\\epsilon_{23}}{\\epsilon_{32}}, \u0026hellip;, \\dfrac{w_2}{w_c} \u0026amp;= \\dfrac{\\epsilon_{2c}}{\\epsilon_{c2}}\\ \u0026hellip;\\ \\ \\ \\ \\ \\ \\ \\ \u0026hellip;\\ \\ \\ \\ \\ \\ \\ \u0026amp;\\ \u0026hellip;\\ \\dfrac{w_{c-1}}{w_c} \u0026amp;= \\dfrac{\\epsilon_{c-1,c}}{\\epsilon_{c,c-1}} \\end{aligned}$$ 可以构造出一个$c$元一次方程组.  Where am I: Place instance and category recognition using spatial $\\tt{PACT}$ \u0026ndash; Jianxin Wu, James M.Rehg, 2008, CVPR  \u0026ldquo;Where am I\u0026rdquo; 问题: 经典的机器人问题, recognize instances and categories of places or scenes. 在视觉中常被处理为场景识别问题 (场景的类别而不是具体位置) PACT: Principal component Analysis of Census Transform histograms.  CT: 一个没有参数的局部转换方法, 可以建立局部的相关性. 将图片转换成灰度图, 每九个像素($3\\times 3$), 将中间的像素灰度和周围8个比较(比较结果用0/1)表示, 构成一个8位的二进制数, 转换成十进制数后作为CT代替原来的像素. CT的传递性保证了相距很远的像素对应的CT也有相关性. 再构造直方图统计不同CT值的数量 最后使用PCA计算对应特征值最大的特征向量, 对应主成分, 即图片中最主要的形状.   Spatial PACT: 将图片分成小份, 在区域中其中计算相关性. 构造 \u0026ldquo;spatial pyramid\u0026rdquo;, 0,1,2-level对应不同的划分方式.  Semi-Supervised Learning with Very Few Labeled Training Examples  传统半监督学习算法需要先通过少量标注数据训练一个初始弱学习器$h$, 再利用$h$和无标注数据. 但是在现实中很多任务能获得的标注数据很少, 比如基于内容的图像检索(搜索相似图片)或在线网页推荐(只拥有一个用户感兴趣的页面) OLTV (learning with One Labeled example and Two Views):  CCV (Canonical correlation analysis): 定义两个视角间的相关投影. $X=(x_0,x_1,\u0026hellip;,x_{l-1}), Y=(y_0,y_1,\u0026hellip;,y_{l-1})$, 寻找两个基向量集合, 以最大化两个视角向量分别在基向量$w_x,w_y$上投影的相关性: $$\\mathop{\\arg\\min}\\limits_{w_x,w_y} \\left(\\dfrac{w^T_xC_{xy}w^T_y}{\\sqrt{w^T_xC_{xx}w^T_x\\cdot w^T_yC_{yy}w^T_y}}\\right)\\ w^T_xC_{xx}w^T_x=1, w^T_yC_{yy}w^T_y=1$$ 最终可以得到一个线性关系$C_{xy}C_{yy}^{-1}C){yx}w_x=\\lambda^2C_{xx}w_x$. 也可用核函数映射到高维, 目标函数变为 $$\\mathop{\\arg\\min}\\limits_{\\alpha, \\beta}\\dfrac{\\alpha^TS_xS_x^TS_yS_y^T\\beta}{\\sqrt{\\alpha^TS_xS_x^TS_xS_x^T\\alpha\\cdot\\beta^TS_yS_y^TS_yS_y^T\\beta}}$$ 两个核矩阵为$K_x=S_xS_x^T, K_y=S_yS_y^T$, $\\alpha, \\beta$可以由$(K_x+\\kappa I)^{-1}K_y(K_y+\\kappa I)^{-1}K_x\\alpha = \\lambda^2\\alpha, \\beta = \\dfrac{1}{\\lambda}(K_y+\\kappa I)^{-1}K_x\\alpha$算出. 因此对于所有的$(x^,y^)$, 投影可以利用$\\alpha, \\beta$算出. 继而算出新样本和旧样本的投影的相似度, 将最像的作为新的正例, 最不像的作为负例.    ","permalink":"https://michelia-zhx.github.io/posts/2021-07-22-pros_paper_notes/","summary":"Multi-Instance Multi-Label Learning with Application to Scene Classification \u0026ndash; Zhi-Hua Zhou, Min-Ling Zhang, NIPS 2006 Multi-instance: 一个example包含多个instance, example只对应1个label; Multi-label: 一个example对应多个","title":"Paper Notes - Week 2"},{"content":"1 快排 1.1 快排 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 1000010; int a[N]; void quicksort(int a[], int l, int r){ if (l \u0026gt;= r) return; int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1]; while (i \u0026lt; j){ do i ++ ; while (a[i] \u0026lt; x); do j -- ; while (a[j] \u0026gt; x); if (i \u0026lt; j) swap(a[i], a[j]); } quicksort(a, l, j); quicksort(a, j+1, r); } int main(){ int n; cin \u0026gt;\u0026gt; n; for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); } quicksort(a, 0, n-1); for(int i=0; i\u0026lt;n; i++) printf(\u0026#34;%d \u0026#34;, a[i]); return 0; }   1.2 第k大的数  执行完partition操作后，设左边的长度为L , 枢轴点的位置为p (将枢轴元素视为属于左部即p==L)  若L == k，则枢轴点v 即为第k 大数 若k \u0026lt; L, 则在左部求其第k大数 若k \u0026gt; L, 则在右部求其第k − L大数    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 1000010; int a[N]; int quicksort(int a[], int l, int r, int k){ if (l \u0026gt;= r){ return a[l]; } int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1]; while (i \u0026lt; j) { do i ++ ; while (a[i] \u0026lt; x); do j -- ; while (a[j] \u0026gt; x); if (i \u0026lt; j) swap(a[i], a[j]); } if (j - l + 1 \u0026gt;= k){ return quicksort(a, l, j, k); } else return quicksort(a, j + 1, r, k - (j - l + 1)); } int main(){ int n, k; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; for(int i=0; i\u0026lt;n; i++) cin \u0026gt;\u0026gt; a[i]; cout \u0026lt;\u0026lt; quicksort(a, 0, n-1, k) \u0026lt;\u0026lt; endl; return 0; }   2 归并排序 2.1 归并排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 1e6 + 10; int a[N], tmp[N]; void merge_sort(int q[], int l, int r){ if (l \u0026gt;= r) return; int mid = l + r \u0026gt;\u0026gt; 1; merge_sort(q, l, mid), merge_sort(q, mid + 1, r); int k = 0, i = l, j = mid + 1; while (i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= r) if (q[i] \u0026lt;= q[j]) tmp[k ++ ] = q[i ++ ]; else tmp[k ++ ] = q[j ++ ]; while (i \u0026lt;= mid) tmp[k ++ ] = q[i ++ ]; while (j \u0026lt;= r) tmp[k ++ ] = q[j ++ ]; for (i = l, j = 0; i \u0026lt;= r; i ++, j ++ ) q[i] = tmp[j]; } int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 0; i \u0026lt; n; i ++ ) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); merge_sort(a, 0, n - 1); for (int i = 0; i \u0026lt; n; i ++ ) printf(\u0026#34;%d \u0026#34;, a[i]); return 0; }   2.2 逆序对的数量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; const int N = 100010; int a[N]; int nums; unsigned long result = 0; void merge_sort(int a[], int l, int r){ if (l \u0026gt;= r) return; int mid = (l+r) \u0026gt;\u0026gt; 1; merge_sort(a, l, mid); merge_sort(a, mid + 1, r); int temp[r - l + 1]; int lptr = l; int rptr = mid + 1; int tempptr = 0; while(lptr \u0026lt;= mid \u0026amp;\u0026amp; rptr \u0026lt;= r){ if(a[lptr] \u0026lt;= a[rptr]) temp[tempptr++] = a[lptr++]; else { temp[tempptr++] = a[rptr++]; result += (mid - lptr + 1); } } while (lptr \u0026lt;= mid) temp[tempptr++] = a[lptr++]; while (rptr \u0026lt;= r) temp[tempptr++] = a[rptr++]; for (int i = l, j = 0; i \u0026lt;= r; i ++, j ++){ a[i] = temp[j]; } } int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;nums); for(int i = 0; i \u0026lt; nums; i++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); merge_sort(a, 0, nums-1); cout \u0026lt;\u0026lt; result; return 0; }   3 二分 3.1 数的范围  对于每个查询，返回一个元素 k 的起始位置和终止位置  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #include \u0026lt;iostream\u0026gt; using namespace std; const int maxn = 100005; int n, q, x, a[maxn]; int main() { scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n, \u0026amp;q); for (int i = 0; i \u0026lt; n; i++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); while (q--) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); int l = 0, r = n - 1; while (l \u0026lt; r) { int mid = l + r \u0026gt;\u0026gt; 1; if (a[mid] \u0026lt; x) l = mid + 1; else r = mid; } if (a[l] != x) { printf(\u0026#34;-1 -1\\n\u0026#34;); continue; } int l1 = l, r1 = n; while (l1 + 1 \u0026lt; r1) { int mid = l1 + r1 \u0026gt;\u0026gt; 1; if (a[mid] \u0026lt;= x) l1 = mid; else r1 = mid; } printf(\u0026#34;%d %d\\n\u0026#34;, l, l1); } return 0; }   3.2 数的三次方根 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #include\u0026lt;iostream\u0026gt; using namespace std; double n,l,r,mid; double q(double a){return a*a*a;} int main(){ cin \u0026gt;\u0026gt; n; l=-100, r=100; while(r - l \u0026gt;= 1e-7){ mid=(l+r)/2; if (q(mid)\u0026gt;=n) r=mid; else l=mid; } printf(\u0026#34;%.06f\u0026#34;, l); return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-01-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%951/","summary":"1 快排 1.1 快排 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 1000010; int a[N]; void quicksort(int a[], int l, int r){ if (l \u0026gt;= r) return; int i = l-1, j = r+1, x =","title":"基础算法(一)"},{"content":"1 双指针  两个指针指向两个序列: 归并排序 两个指针指向一个序列: 快排  1.1 最长不重复子序列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; const int maxn = 1e6+10; int a[maxn], q[maxn]; int n, ans=0; int main(){ cin \u0026gt;\u0026gt; n; for(int i=0;i\u0026lt;n;i++) cin \u0026gt;\u0026gt; a[i]; for(int i=0,j=0;i\u0026lt;n;i++){ q[a[i]]++; while(q[a[i]]\u0026gt;1){ q[a[j]]--; j++; } ans=max(ans,i-j+1); } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; endl; }   2 位运算  n的二进制表示中第k位是几  把第k位移到最后一位 n \u0026raquo; k   lowbit(x): 返回x二进制的最后一位1 (返回值是一个二进制数)  x \u0026amp; (-x) = x \u0026amp; (~x+1) x = 1010\u0026hellip;10000, ~x=0101\u0026hellip;01111, ~x+1=0101\u0026hellip;10000   n的二进制表示中有多少个1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include\u0026lt;iostream\u0026gt; using namespace std; int lowbit(int x){ return x \u0026amp; -x; // 返回x二进制的最后一位1 (返回值是一个二进制数) } int main(){ int n; cin \u0026gt;\u0026gt; n; // 多次测试 while(n--){ int x; cin \u0026gt;\u0026gt; x; int res = 0; while (x){ x -= lowbit(x); // 把最后一位1去掉 res ++; } printf(\u0026#34;%d \u0026#34;, res); } return 0; }   3 离散化 3.1 离散化  整数序列a, 值域大但个数少, 将他们映射到从0开始的数  a中可能有重复元素(去重) 如何算出a[i]映射(离散化后)的数(二分)    1 2 3 4 5 6 7 8 9 10 11 12 13  vector\u0026lt;int\u0026gt; alls; sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); int find(int x){ int i=0, r=alls.size()-1; while (l \u0026lt; r){ int mid = (l+r)/2; if (alls[mid] \u0026gt;= x) r = mid; else l = mid+1; } return r+1; // }   3.2 区间和  首先进行 n 次操作, 每次操作将某一位置 x 上的数加 c. 接下来, 进行 m 次询问, 每个询问包含两个整数 l 和 r, 你需要求出在区间 [l,r] 之间的所有数的和.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; typedef pair\u0026lt;int, int\u0026gt; PII; const int N=300100; int a[N], s[N]; vector\u0026lt;int\u0026gt; alls; vector\u0026lt;PII\u0026gt; add, query; int find(int x){ int l=0, r=alls.size()-1; while (l \u0026lt; r){ int mid = (l+r)/2; if (alls[mid] \u0026gt;= x) r = mid; else l = mid+1; } return r+1; // } int main(){ int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; while(n--){ int x, c; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; c; add.push_back({x, c}); alls.push_back(x); } while(m--){ int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; query.push_back({l, r}); alls.push_back(l); alls.push_back(r); } //去重 sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); for (auto item: add){ int x = find(item.first); a[x] += item.second; } for(int i = 1; i \u0026lt;= alls.size(); i ++ ) s[i] = s[i - 1] + a[i]; for (auto item: query){ int l = find(item.first), r = find(item.second); cout \u0026lt;\u0026lt; s[r] - s[l-1] \u0026lt;\u0026lt; endl; } return 0; }   4 区间合并  返回n个区间合并之后的个数  按左端点排序 分新区间与前一个区间关系的三种情况更新区间列表    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; typedef pair\u0026lt;int, int\u0026gt; PII; vector\u0026lt;PII\u0026gt; p1, p2; int main(){ int n; cin \u0026gt;\u0026gt; n; while(n--){ int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; p1.push_back({l,r}); } sort(p1.begin(), p1.end()); int res = 1; for(auto item: p1){ if (p2.size() == 0) p2.push_back(p1[0]); int l = item.first, r = item.second; if (l \u0026lt;= p2[p2.size()-1].second){ if (r \u0026gt;= p2[p2.size()-1].second) p2[p2.size()-1].second = r; } else{ res += 1; p2.push_back({l,r}); } } cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; endl; return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-03-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%952/","summary":"1 双指针 两个指针指向两个序列: 归并排序 两个指针指向一个序列: 快排 1.1 最长不重复子序列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; const","title":"基础算法(二)"},{"content":"1 链表与邻接表 1 2 3 4 5  struct Node{ int val; Node *next; } new Node();   1.1 用数组模拟链表  用数组模拟单链表(静态链表): 邻接表(存储图和树)  O(1)时间找下一个点, O(n)时间找上一个点    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 100010; int head, e[N], ne[N], idx; void init(){ head = -1; } void add_to_head(int x){ ne[idx] = head; head = idx; e[idx] = x; idx ++; } void add(int k, int x){ e[idx] = x; ne[idx] = ne[k]; ne[k] = idx; idx ++; } void del(int k){ ne[k] = ne[ne[k]]; }   1.2 用数组模拟双链表  每个节点有两个指针, 指向前后 l[N], r[N], 0对应head, 1对应tail, 两个边界不包含实质内容  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 100010; int head, e[N], l[N], r[N], idx; void init(){ r[0] = 1; l[1] = 0; idx = 2; } void add(int k, int x){ e[idx] = x; r[idx] = r[k]; l[idx] = k; l[r[k]] = idx; r[k] = idx; idx ++; } void del(int k){ r[l[k]] = r[k]; l[r[k]] = l[k]; }   2 栈与队列 2.1 模拟栈 1 2 3 4 5 6 7  int stk[N], tt; // 插入 stk[ ++t] = x; // 弹出 t -- ; //判断是否为空 return t \u0026gt; 0; // 不空   2.2 单调栈  求每一个数左边离它最近且比它小的数, 没有返回-1 事实上对于这样的要求, 只需要保留单调上升序列即可, 逆序对的第一个元素删掉  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 100010; int n; int stk[N], tt; int main(){ ios::sync_with_stdio(false); scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i\u0026lt;n; i++){ int x; scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); while (tt \u0026amp;\u0026amp; stk[tt] \u0026gt;= x) t --; if (tt) printf(\u0026#34;%d \u0026#34;, stk[tt]); else printf(\u0026#34;-1 \u0026#34;); stk[ ++t] = x; } return 0; }   2.3 滑动窗口里的最大值和最小值 (单调队列)  在找最小值时, 只要前面的元素比后面的元素大, 那么大的元素一定没有用 找最大值是完全对称的写法  3 kmp  暴力怎么做 如何优化  next[i] = j表示p[1,\u0026hellip;,j] = p[i-j+1,\u0026hellip;,i]    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 10010, M=100010; int n, m; int p[N], s[M]; int ne[N]; int main(){ cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; p + 1 \u0026gt;\u0026gt; m \u0026gt;\u0026gt; s + 1; // 求next过程 for (int i=2, j=0; i\u0026lt;=m; i++){ while (j \u0026amp;\u0026amp; p[i] != p[j+1]) j = ne[j]; if (p[i] == p[j+1]) j ++; ne[i] = j; } // 匹配过程 for (int i=1, j=0; i\u0026lt;=m; i++){ while (j \u0026amp;\u0026amp; s[i] != p[j+1]) j = ne[j]; if (s[i] != p[j+1]) j ++; if (j == n){ // 匹配成功 } } }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-07-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841/","summary":"1 链表与邻接表 1 2 3 4 5 struct Node{ int val; Node *next; } new Node(); 1.1 用数组模拟链表 用数组模拟单链表(静态链表): 邻接表(存储图和树) O(1)时间找下一个点, O(n)","title":"数据结构(一)"},{"content":"系统为某一程序分配空间所需时间，与空间大小无关，与申请次数有关！\n哈希  存储结构（一般只有添加和查找操作，如果要删，在点上打一个标记即可）  拉链法（槽+链表） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  #include \u0026lt;iostream\u0026gt;using namespace std; const int N=100003; int h[N], e[N], ne[N], idx; void insert(int x){ int k = (x % N + N) % N; // 让余数为正  e[idx] = x; //新点的真正存储位置  ne[idx] = h[k]; //新点的next指针指向h[k](指向的点)  h[k] = idx ++; //h[k]指向新点 } bool find(int x){ int k = (x % N + N) % N; for (int i = h[k]; i != -1; i = ne[i]){ if (e[i] == x) return true; } return false; } int main(){ bool flag = true; for (int i = 100000;; i++){ for (int j=2; j*j\u0026lt;=i; j++){ if (i % j == 0){ flag = false; break; } } if (flag == false) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; break; } } int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); memset(h, -1, sizeof h ); while(n--){ char op[2]; int x; int scanf(\u0026#34;%s%d\u0026#34;, op, \u0026amp;x); if (*op == \u0026#39;I\u0026#39;) insert(x); else{ if (find(x)) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } } return 0; }    开放寻址法（一般初始化两倍大小的数组）：插入——从第k个坑位开始，如果有人，则往后，遇到没人的插入；查找——从第k个坑位开始，如果有人，就判断；如果没人，说明查找失败 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \u0026lt;iostream\u0026gt;using namespace std; const int N=200003, null = 0x3f3f3f3f; int h[N]; int find(int x){ int k = (x % N + N) % N; while(h[k] != null \u0026amp;\u0026amp; h[k] != x){ k ++; if (k == N) k = 0; } return k; } int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); memset(h, 0x3f, sizeof h); while(n--){ char op[2]; int x; scanf(\u0026#34;%s%d\u0026#34;, op, \u0026amp;x); int k = find(x); if (*op == \u0026#39;I\u0026#39;) h[k] = x; else{ if (h[k] != null) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } } }      字符串前缀哈希法  预处理出所有前缀的哈希，h[i]=str(0:i-1)的哈希值 讲字符串表示成一个p进制的数 \u0026ldquo;ABDC\u0026quot;→(1,2,3,4)_p = (p^3+2p^2+3p+4) mod Q 一般不把字母映射成0 $$ \\text{L到R这段的哈希值: }h[R]-h[L-1]*p^{R-L+1} $$ O(1)时间求某一段的哈希值    C++ STL  vector，变长数组，倍增的思想  size(), empty(), clear() front(), back() push_back(), a.pop_back() begin(), end()   pair\u0026lt;int, int\u0026gt;  first(), second(), 支持比较运算，以first为第一关键字，second为第二关键字   string，字符串，substr(), c_str()  size(), length(), empty(), clear()   queue，队列  push(), front(), back(), pop() size(), empty()   qriority_queue，优先队列（默认是大根堆）  push(), top(), pop(), clear() 定义小根堆 priority_queue\u0026lt;int, vector, greater\u0026gt; heap;   stack，栈  push(), top(), pop() size(), empty()   deque，双端队列（效率很低，比数组慢）  size(), empty(), clear(); front(), back() push_back(), pop_back() push_front(), pop_front() begin(), end()   set, map, multiset, multimap，基于平衡二叉树（红黑树）动态维护有序序列  size(), empty(), clear() set/multiset:  insert(), find(), count(), erase() — 输入是一个数x, 删除所有x O(k+lgn)；输入是一个迭代器，删除这个迭代器 lower_bounder(x) — 返回大于等于x的最小的数的迭代器 upper_bound(x) — 返回大于x的最小的数的迭代器   mapmultimap  insert() — 插入的是pair, erase() — 输入是pair或者迭代器     unordered_set, unordered_map, unordered_multiset, unordered_multimap，哈希表  和上面类似，增删改查时间复杂度为O(1) 不支持lower_bounder和upper_bound，以及迭代器的++/- -   bitset，压位  可以省8倍空间 bitset\u0026lt;10000\u0026gt; s — \u0026lt;\u0026gt;里是长度 ~, \u0026amp;, |, ^ \u0026laquo;, \u0026raquo; ==, ! = count() — 返回有多少个1 any() — 判断是否至少有一个1, none() — 判断是否全为0 set() — 把所有位置成1, set(k, v) reset() — 把所有位置成0 flip() — 所有位取反    ","permalink":"https://michelia-zhx.github.io/posts/2021-05-12-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%843/","summary":"系统为某一程序分配空间所需时间，与空间大小无关，与申请次数有关！ 哈希 存储结构（一般只有添加和查找操作，如果要删，在点上打一个标记即可） 拉链法","title":"数据结构(三)"},{"content":"1 Trie树  高效存储字符串集合 存储：按字符串内容，从左到右依次设置结点，并标记结束位置 查找：查找字符串是否存在 \u0026amp; 出现几次  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  const int N=100010; int son[N][26], cnt[N], idx; void insert(char str[]){ int p = 0; for (int i=0; str[i]; i++){ int u = str[i] - \u0026#39;a\u0026#39;; if (!son[p][u]) son[p][u] = ++idx; p = son[p][u]; } cnt[p] ++ ; } int query(char str[]){ int p = 0; for (int i=0; str[i]; i++){ int u = str[i] - \u0026#39;a\u0026#39;; if (!son[p][u]) return 0; p = son[p][u]; } return cnt[p]; }   2 并查集 快速地处理（近乎O(1)）：\n 合并两个集合 询问两个元素是否在一个集合中  基本原理：\n 每个集合用一棵树来表示，树根的编号是集合的编号； 每个结点存储父结点，p表示其父结点  问题1：如何判断树根：if (p[x] == x);\n问题2：如何找到所属集合根结点：while(p[x] ≠ x) x = p[x]; （路径压缩）\n问题3：如何合并两个集合（px是x的集合编号，py是y的集合编号）：p[x] = py;\n问题4：维护集合元素的个数 size()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  const int N=100010; int n, m; int p[N]; int find(int x){ if (p[x] != x) p[x] = find(p[x]); return p[x]; } int main(){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n ,\u0026amp;m); for (int i=1; i\u0026lt;= n; i++){ p[i] =i; size[p[i]] = 1; } while (m--){ char op[2]; int a, b; scanf(\u0026#34;%s\u0026#34;, \u0026amp;op); if (op[0] == \u0026#39;C\u0026#39;){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;a, \u0026amp;b); p[find(a)] = find(b); size[find(b)] += size[find(a)]; } else if (op[0] == \u0026#39;1\u0026#39;){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;a, \u0026amp;b); if (find(a) == find(b)) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } else{ scanf(\u0026#34;%d\u0026#34;, \u0026amp;a); printf(\u0026#34;%d\\n\u0026#34;, size[find(a)]); } } return 0; }   3 堆   小根堆：每个点都小于等于左右儿子\n 插入一个数（STL） 1  heap[++size] = x; up(size);    求集合当中最小值（STL） 1  heap[1];    删除最小值（STL） 1  heap[1] = heap[size]; size--; down(1);    删除任意一个元素 1  heap[k] = heap[size]; size--; down(k); up(k);    修改任意一个元素 1  heap[k] = x; down(k); up(k);       堆是完全二叉树，用数组存储，1号位是根结点\n  一个结点是x，左孩子是2x，右孩子是2x+1\n  五个操作完全可以用以下两个函数组合起来操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  void down(int u){ int t = u; if (u * 2 \u0026lt;= size \u0026amp;\u0026amp; h[u * 2] \u0026lt; h[u]) t = u * 2; if (u * 2 + 1 \u0026lt;= size \u0026amp;\u0026amp; h[u * 2 + 1] \u0026lt; h[u]) t = u * 2 + 1; if (u != t){ swap(h[u], h[t]); down(t); } } void up(u){ while (u / 2 \u0026gt;= 1 \u0026amp;\u0026amp; h[u / 2] \u0026gt; h[u]){ swap(h[u / 2], h[u]); u /= 2; } }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-09-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%842/","summary":"1 Trie树 高效存储字符串集合 存储：按字符串内容，从左到右依次设置结点，并标记结束位置 查找：查找字符串是否存在 \u0026amp; 出现几次 1 2 3 4 5 6 7 8 9 10","title":"数据结构(二)"},{"content":"短视的行为\n区间选点 给N个闭区间, 要在数轴上选尽量少的点, 使每个区间至少包含一个选出的点 (answer ≤ count)\n区间贪心问题, 要么按左端点排序, 要么按右端点, 要么双关键字\n 每个区间按右端点从小到大排序 从前往后依次枚举每个区间   如果已经包含点, pass 否则选右端点 (answer ≥ count) =\u0026gt; answer = count  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return r \u0026lt; W.r; } }range[N]; int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); int res = 0, ed = -2e9; for (int i=0; i \u0026lt; n; i++){ if (range[i].l \u0026gt; ed){ res ++; ed = range[i].r; } } printf(\u0026#34;%d\u0026#34;, res); return 0; }   排课 (不冲突排尽量多的课) 和上题一样, 上题选点的个数(有点的区间相互没有相交) = 这题的课程数\n区间分组 区间分组, 每组内的区间两两没有交集, 要求组数尽量小. (合法的 → answer ≤ count)\n 每个区间按左端点从小到大排序 从前往后依次枚举每个区间   判断能否放入现有的组中 L[i] \u0026gt; max_r  不能, 新建一个组 存在, 随便挑一个, 将其放入, 更新当前组的max_r 最后分出的count个组, 每个都是相交的, 因此必须分开 → answer ≥ count =\u0026gt; answer = count    区间max_r可以用小根堆存储  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;queue\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return r \u0026lt; W.r; } }range[N]; int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;int\u0026gt; \u0026gt; heap; for (int i=0; i\u0026lt;n; i++){ auto r = range[i]; if (heap.empty() || heap.top() \u0026gt;= r.l) heap.push(r.r); else{ int t = heap.top(); heap.pop(); heap.push(r.r); } } printf(\u0026#34;%d\u0026#34;, heap.size()); return 0; }   区间覆盖 给定一些小区间和一个大区间[start, end], 要求用尽量少的小区间去完全覆盖大区间.\n 将所有区间按左端点从小到大排序 从前往后依次枚举每个区间, 在所有能覆盖start的区间中, 选择右端点最大的那一个, 将start更新成这个右端点  合法的 → answer ≤ count. 反证法 → answer ≥ count. 假设answer \u0026lt; count, 找到第一个不一样的, 替换掉, 保持个数不变且依然合法. answer = count.    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;queue\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return l \u0026lt; W.l; } }range[N]; int main(){ int st, ed; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;st, \u0026amp;ed); scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); int res = 0; bool success = false; for (int i=0; i \u0026lt; n; i++){ int j=i, r=-2e9; while (j\u0026lt;n \u0026amp;\u0026amp; range[j].l \u0026lt;= st){ r = max(r, range[j].r); j++; } if (r \u0026lt; st){ res = -1; break; } res ++; if (r \u0026gt;= ed) { success = true; break; } st = r; i = j-1; } if (!success) res = -1; printf(\u0026#34;%d\u0026#34;, res); return 0; }   哈夫曼树  合并果子 - 消耗体力最少  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;queue\u0026gt; using namespace std; int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;int\u0026gt;\u0026gt; heap; while(n--){ int x; scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); heap.push(x); } int res = 0; while (heap.size() \u0026gt; 1){ int a = heap.top(); heap.pop(); int b = heap.top(); heap.pop(); res += a + b; heap.push(a+b); } printf(\u0026#34;%d\u0026#34;, res); return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-27-%E8%B4%AA%E5%BF%83/","summary":"短视的行为 区间选点 给N个闭区间, 要在数轴上选尽量少的点, 使每个区间至少包含一个选出的点 (answer ≤ count) 区间贪心问题, 要么按左端点排序, 要么按右端点, 要","title":"贪心"}]