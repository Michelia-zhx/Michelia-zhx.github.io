[{"content":"","permalink":"https://michelia-zhx.github.io/about/","summary":"about","title":"About"},{"content":"Biasâ€“variance tradeoff The biasâ€“variance dilemma or biasâ€“variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: - The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). - The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).\nThe biasâ€“variance decomposition is a way of analyzing a learning algorithm\u0026rsquo;s expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\nBiasâ€“variance decomposition of mean squared error å¯¹äºæ•°æ®é›†$D = {(x_1,y_1),\u0026hellip;,(x_n,y_n)}$, å‡è®¾æ ‡ç­¾$y = f(x) + \\epsilon$, å…¶ä¸­$\\epsilon$ä¸ºå™ªå£°, æœŸæœ›ä¸º $0$, æ–¹å·®ä¸º$\\sigma^2$. å¸Œæœ›$\\hat{f}(x;D)$èƒ½å°½é‡æ‹Ÿåˆ$f(x)$.\n$$E_{D,\\epsilon}\\left[(y-\\hat{f}(x;D))^2\\right] = \\left(\\text{Bias}_D[\\hat{f}(x;D)])^2\\right) + \\text{Var}_D[\\hat{f}(x;D)] + \\text{intrinsic noise}$$\nå…¶ä¸­ $$\\text{Bias}_D[\\hat{f}(x;D)] = E_D[\\hat{f}(x;D)] - f(x)$$\n$$\\text{Var}_D[\\hat{f}(x;D)] = E_D\\left[\\left(E_D(\\hat{f}(x;D)) - \\hat{f}(x;D)\\right)^2\\right]$$\nBiasâ€“variance decomposition of Kullback-Leibler divergence $$K(q, \\hat{p}) = \\int\\rm{d}yq(y)\\log\\left[\\dfrac{q(y)}{\\hat{p}(y)}\\right]$$\n$$\\text{variance} = \\min_{a: \\int\\rm{d}ya(y)=1}EK(a,\\hat{p}) = EK(\\overline{p}, \\hat{p}), \\overline{p}(y) = \\dfrac{1}{Z}\\exp[E\\log\\hat{p}(y)]$$\n$$\\text{bias} = K(q,\\overline{p}) = EK(q,\\overline{p}) + \\log Z$$\n$$\\text{error} = EK(q,\\hat{p}) = K(q,\\overline{p}) + EK(q,\\hat{p})$$\n$$-E\\log\\hat{p}(t) = -\\log\\overline{p}(t) + EK(\\overline{p}, \\hat{p})$$\n$$error = -E\\left[\\int\\rm{d}q(t)\\log\\hat{p}(t)\\right] = -\\int\\rm{d}q(t)\\log q(t) + K(q,\\overline{p}) + EK(\\overline{p}, \\hat{p})$$\nReference   Fortmann-Roe, Scott (2012). \u0026ldquo;Understanding the Biasâ€“Variance Tradeoff\u0026rdquo;\n  Heskes T. Bias/variance decompositions for likelihood-based estimators[J]. Neural Computation, 1998, 10(6): 1425-1433.\n  ","permalink":"https://michelia-zhx.github.io/posts/2022-03-03-bias-variance-decomposition/","summary":"Biasâ€“variance tradeoff The biasâ€“variance dilemma or biasâ€“variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: - The","title":"Bias-Variance Decomposition"},{"content":"1. å®‰è£… Hugo è‹¹æœç”¨æˆ·æœ‰å®‰è£… HomeBrew å·¥å…·çš„è¯å¯ä»¥ç›´æ¥è¾“å…¥ brew install hugo è¿›è¡Œå®‰è£….\nä¸‹è½½å®Œæˆåæ£€æŸ¥æ˜¯å¦å®‰è£…æˆåŠŸï¼Œè¾“å…¥: hugo version, è‹¥å‡ºç°ç‰ˆæœ¬ä¿¡æ¯åˆ™è¡¨ç¤ºå®‰è£…æˆåŠŸ.\n2. æ–°å»ºç«™ç‚¹ è¾“å…¥ hugo new site {your site's name}, å³å¯ç”Ÿæˆä»¥ {your site's name} å‘½åçš„ç«™ç‚¹æ–‡ä»¶å¤¹. å…·æœ‰ archetypes, content, data, layouts, static, themes æ–‡ä»¶å¤¹å’Œ config.yml é…ç½®æ–‡ä»¶.\n3. ä¸‹è½½ä¸»é¢˜ å…ˆå» Hugo ä¸»é¢˜å®˜ç½‘ æ‰¾åˆ°è‡ªå·±å–œæ¬¢çš„ä¸»é¢˜, ç„¶åç‚¹å‡»ä¸‹è½½ä¼šè·³è½¬åˆ°ä¸»é¢˜çš„github, æŠŠç»ˆç«¯çš„è·¯å¾„è°ƒæ•´åˆ°åšå®¢æ–‡ä»¶å¤¹çš„themesç›®å½•ä¸‹, è¾“å…¥ git clone https://github.com/adityatelange/hugo-PaperMod.git. ä¹Ÿå¯ä»¥åœ¨ github ä¸‹è½½å¯¹åº” repo çš„å‹ç¼©åŒ…, è§£å‹åˆ°themesç›®å½•ä¸‹.\nä½¿ç”¨è¯¥ä¸»é¢˜çš„æ–¹æ³•å°±æ˜¯åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶è¾“å…¥ä¸»é¢˜çš„åå­—:\n1  theme: PaperMod # ä¸»é¢˜åå­—ï¼Œå’Œthemesæ–‡ä»¶å¤¹ä¸‹çš„ä¸€è‡´   4. é…ç½®æ–‡ä»¶ æˆ‘çš„ config.yml å†…å®¹å¦‚ä¸‹:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202  baseURL: https://michelia-zhx.github.io languageCode: zh-cn # en-us title: Michelia\u0026#39;Log theme: PaperMod # ä¸»é¢˜åå­—ï¼Œå’Œthemesæ–‡ä»¶å¤¹ä¸‹çš„ä¸€è‡´ enableInlineShortcodes: true enableEmoji: true # å…è®¸ä½¿ç”¨ Emoji è¡¨æƒ…ï¼Œå»ºè®® true enableRobotsTXT: true # å…è®¸çˆ¬è™«æŠ“å–åˆ°æœç´¢å¼•æ“ï¼Œå»ºè®® true hasCJKLanguage: true # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦åŒ…å« ä¸­æ–‡æ—¥æ–‡éŸ©æ–‡ å¦‚æœæ–‡ç« ä¸­ä½¿ç”¨äº†å¾ˆå¤šä¸­æ–‡å¼•å·çš„è¯å¯ä»¥å¼€å¯ buildDrafts: false buildFuture: false buildExpired: false # googleAnalytics: UA-123-45 # è°·æ­Œç»Ÿè®¡ # Copyright: Michelia paginate: 5 # é¦–é¡µæ¯é¡µæ˜¾ç¤ºçš„æ–‡ç« æ•° minify: disableXML: true # minifyOutput: true permalinks: post: \u0026#34;/:title/\u0026#34; # post: \u0026#34;/:year/:month/:day/:title/\u0026#34; defaultContentLanguage: en # æœ€é¡¶éƒ¨é¦–å…ˆå±•ç¤ºçš„è¯­è¨€é¡µé¢ defaultContentLanguageInSubdir: true languages: en: languageName: \u0026#34;English\u0026#34; # contentDir: content/english weight: 1 avatarURL: \u0026#34;images/Cheese.ico\u0026#34; profileMode: enabled: true title: ğŸ§€ Welcome to Michelia\u0026#39;Log imageUrl: \u0026#34;images/goujuan.png\u0026#34; imageTitle: imageWidth: 130 imageHeight: 130 menu: main: - identifier: home name: ğŸ§€Home url: / weight: 1 - identifier: posts name: ğŸ¥‘Posts url: posts weight: 2 - identifier: archives name: ğŸ‘Archives url: archives weight: 4 - identifier: tags name: ğŸTags url: tags weight: 5 - identifier: about name: ğŸ‰About url: about weight: 6 - identifier: search name: ğŸ³Search url: search weight: 7 outputs: home: - HTML - RSS - JSON params: env: production # to enable google analytics, opengraph, twitter-cards and schema. author: Michelia-zhx # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors defaultTheme: auto # defaultTheme: light or dark  disableThemeToggle: false DateFormat: \u0026#34;2006-01-02\u0026#34; ShowShareButtons: true ShowReadingTime: true # disableSpecialistPost: true displayFullLangName: true ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: true hideFooter: false # éšè—é¡µè„š ShowWordCounts: true VisitCount: true ShowLastMod: true #æ˜¾ç¤ºæ–‡ç« æ›´æ–°æ—¶é—´ ShowToc: true # æ˜¾ç¤ºç›®å½• TocOpen: true # è‡ªåŠ¨å±•å¼€ç›®å½• comments: true socialIcons: - name: github url: \u0026#34;https://github.com/Michelia-zhx\u0026#34; - name: twitter url: \u0026#34;images/twitter.png\u0026#34; - name: facebook url: \u0026#34;https://www.facebook.com/\u0026#34; - name: instagram url: \u0026#34;images/instagram.png\u0026#34; - name: QQ url: \u0026#34;images/qq.png\u0026#34; - name: WeChat url: \u0026#34;images/wechat.png\u0026#34; - name: email url: \u0026#34;mailto: zhanghanxiao000@gmail.com\u0026#34; - name: RSS url: \u0026#34;index.xml\u0026#34; # editPost: # URL: \u0026#34;https://github.com/adityatelange/hugo-PaperMod/tree/exampleSite/content\u0026#34; # Text: \u0026#34;Suggest Changes\u0026#34; # edit text # appendFilePath: true # to append file path to Edit link # label: # text: \u0026#34;Home\u0026#34; # icon: icon.png # iconHeight: 35 # analytics: # google: # SiteVerificationTag: \u0026#34;XYZabc\u0026#34; assets: favicon: \u0026#34;images/Cheese.ico\u0026#34; favicon16x16: \u0026#34;images/Cheese.ico\u0026#34; favicon32x32: \u0026#34;images/Cheese.ico\u0026#34; apple_touch_icon: \u0026#34;images/Cheese.ico\u0026#34; safari_pinned_tab: \u0026#34;images/Cheese.ico\u0026#34; # cover: # hidden: true # hide everywhere but not in structured data # hiddenInList: true # hide on list pages and home # hiddenInSingle: true # hide on single page fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 1 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;] twikoo: version: 1.4.11 taxonomies: category: categories tag: tags series: series markup: goldmark: renderer: unsafe: true # HUGO é»˜è®¤è½¬ä¹‰ Markdown æ–‡ä»¶ä¸­çš„ HTML ä»£ç ï¼Œå¦‚éœ€å¼€å¯çš„è¯ highlight: codeFences: true guessSyntax: false hl_Lines: \u0026#34;\u0026#34; lineNoStart: 1 lineNos: true lineNumbersInTable: true noClasses: true style: monokai tabWidth: 4 privacy: vimeo: disabled: false simple: true twitter: disabled: false enableDNT: true simple: true instagram: disabled: false simple: true youtube: disabled: false privacyEnhanced: true services: instagram: disableInlineCSS: true twitter: disableInlineCSS: true   5. å¯åŠ¨åšå®¢ åœ¨ç«™ç‚¹ç›®å½•ä¸‹è¾“å…¥ hugo server -D å°±å¯ä»¥åœ¨æœ¬åœ°é¢„è§ˆäº†, æœ¬åœ°é¢„è§ˆç½‘å€ä¸º localhost:1313, è¾“å…¥ hugo å°±å¯ä»¥ç”Ÿæˆ public æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªæ–‡ä»¶å¤¹å¯ä»¥éƒ¨ç½²åˆ°äº‘æœåŠ¡å™¨æˆ–è€…æ‰˜ç®¡åˆ° github ä¸Š.\næ³¨æ„: è¾“å…¥ hugo çš„ç”Ÿæˆæ–¹å¼åªä¼šå¾€ public æ–‡ä»¶å¤¹é‡Œæ·»åŠ å†…å®¹, ä½†æ˜¯ä¸ä¼šåˆ é™¤å¤–éƒ¨å·²ç»ä¸å­˜åœ¨è€Œ public é‡Œé¢è¿˜å­˜åœ¨çš„æ–‡ä»¶.\n6. å†™åšå®¢ è¾“å…¥ hugo new æ–‡ç« åç§°.md å°±ä¼šåœ¨ content ç›®å½•ä¸‹çš„åˆ° \u0026ldquo;æ–‡ç« åç§°.md\u0026rdquo; å½¢å¼çš„æ–‡ä»¶, æ‰€æœ‰æ–‡ç« éƒ½æ˜¯æ”¾åœ¨ content è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œ.\næ–‡ç« å†…éƒ¨çš„å¤´éƒ¨é…ç½®ä¿¡æ¯:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  --- title: Hugo åšå®¢æ­å»º | PaperMod ä¸»é¢˜ author: \u0026#34;Michelia-zhx\u0026#34; # æ–‡ç« ä½œè€… date: 2022-03-01 15:20 # æ–‡ç« ç¼–å†™æ—¥æœŸ lastmod: 2022-03-01 15:20 # æ–‡ç« ä¿®æ”¹æ—¥æœŸ tags: [ # æ–‡ç« æ‰€å±æ ‡ç­¾ \u0026#34;Hugo\u0026#34;, \u0026#34;Blog\u0026#34; ] keywords: [ # æ–‡ç« å…³é”®è¯ \u0026#34;Hugo\u0026#34;, \u0026#34;static\u0026#34;, \u0026#34;generator\u0026#34;, ] ---   è¿™é‡Œæœ‰ä¸€äº›åŠŸèƒ½æ¯ä¸ªä¸»é¢˜ä¸ä¸€å®šæœ‰, éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µåšä¸€äº›é€‚é…, ä»…ä¾›å‚è€ƒ.\n7. æ•°å­¦å…¬å¼ åœ¨ ./themes/PaperMod/layouts/partials/footer.htmlæœ«å°¾æ·»åŠ ä»¥ä¸‹å†…å®¹ (å› ä¸ºæ¯ä¸ªé¡µé¢éƒ½æœ‰ footer éƒ¨åˆ†):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  \u0026lt;script type=\u0026#34;text/javascript\u0026#34; async src=\u0026#34;https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0026#34;\u0026gt; MathJax.Hub.Config({ tex2jax: { inlineMath: [[\u0026#39;$\u0026#39;,\u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;,\u0026#39;\\\\)\u0026#39;]], displayMath: [[\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026#39;\\[\\[\u0026#39;,\u0026#39;\\]\\]\u0026#39;]], processEscapes: true, processEnvironments: true, skipTags: [\u0026#39;script\u0026#39;, \u0026#39;noscript\u0026#39;, \u0026#39;style\u0026#39;, \u0026#39;textarea\u0026#39;, \u0026#39;pre\u0026#39;], TeX: { equationNumbers: { autoNumber: \u0026#34;AMS\u0026#34; }, extensions: [\u0026#34;AMSmath.js\u0026#34;, \u0026#34;AMSsymbols.js\u0026#34;] } } }); MathJax.Hub.Queue(function() { // Fix \u0026lt;code\u0026gt; tags after MathJax finishes running. This is a  // hack to overcome a shortcoming of Markdown. Discussion at  // https://github.com/mojombo/jekyll/issues/199  var all = MathJax.Hub.getAllJax(), i; for(i = 0; i \u0026lt; all.length; i += 1) { all[i].SourceElement().parentNode.className += \u0026#39; has-jax\u0026#39;; } }); \u0026lt;/script\u0026gt; \u0026lt;style\u0026gt; code.has-jax { font: inherit; font-size: 100%; background: inherit; border: inherit; color: #515151; } \u0026lt;/style\u0026gt;   8. æ’å…¥å›¾ç‰‡ ä¸¾ä¾‹: å°†å›¾ç‰‡æ”¾åœ¨ ./static/images/multi-teacher_kd/1.png, æ–‡ç« å†…ä½¿ç”¨ \u0026lt;img src=\u0026quot;/images/multi-teacher_kd/1.png\u0026quot; width = \u0026quot;500\u0026quot; div align=center /\u0026gt;, å»ºç«™æ—¶ public æ–‡ä»¶å¤¹å†… images æ–‡ä»¶å¤¹ä¸‹ä¼šç”Ÿæˆ multi-teacher_kdæ–‡ä»¶å¤¹, å­˜æ”¾ 1.png, å¯¹åº”çš„ html æ–‡ä»¶å†…ä¹Ÿä¼šé€šè¿‡ \u0026lt;img src=\u0026quot;/images/multi-teacher_kd/11.png\u0026quot; width = \u0026quot;500\u0026quot; div align=center /\u0026gt;\u0026lt;/li\u0026gt;æ’å…¥å›¾ç‰‡.\n","permalink":"https://michelia-zhx.github.io/posts/2022-03-01-%E5%BB%BA%E7%AB%99/","summary":"1. å®‰è£… Hugo è‹¹æœç”¨æˆ·æœ‰å®‰è£… HomeBrew å·¥å…·çš„è¯å¯ä»¥ç›´æ¥è¾“å…¥ brew install hugo è¿›è¡Œå®‰è£…. ä¸‹è½½å®Œæˆåæ£€æŸ¥æ˜¯å¦å®‰è£…æˆåŠŸï¼Œè¾“å…¥: hugo version, è‹¥å‡ºç°ç‰ˆæœ¬ä¿¡æ¯åˆ™è¡¨ç¤ºå®‰è£…æˆåŠŸ. 2. æ–°å»ºç«™ç‚¹ è¾“","title":"Hugo åšå®¢æ­å»º | PaperMod ä¸»é¢˜"},{"content":"Chap 1 è®¡ç®—æœºç½‘ç»œå’Œå› ç‰¹ç½‘ 1.1 ä»€ä¹ˆæ˜¯å› ç‰¹ç½‘ 1.1.1 å› ç‰¹ç½‘çš„å…·ä½“æ„æˆæè¿°  å„ç§å„æ ·çš„è®¾å¤‡éƒ½è¢«è¿æ¥åˆ°å› ç‰¹ç½‘,è¿™äº›è®¾å¤‡éƒ½è¢«ç§°ä¸ºä¸»æœºæˆ–ç«¯ç³»ç»Ÿ. ç«¯ç³»ç»Ÿé€šè¿‡é€šä¿¡é“¾è·¯å’Œåˆ†ç»„äº¤æ¢æœºè¿æ¥åˆ°ä¸€èµ·. åˆ†ç»„: ä¸€å°ç«¯ç³»ç»Ÿå‘å¦ä¸€å°ç«¯ç³»ç»Ÿå‘é€æ•°æ®æ—¶, å‘é€ç«¯ç³»ç»Ÿå°†æ•°æ®åˆ†ç»„, å¹¶ä¸ºæ¯æ®µåŠ ä¸Šé¦–éƒ¨å­—èŠ‚ åˆ†ç»„äº¤æ¢æœº: ä»å…¥é€šä¿¡é“¾è·¯æ¥å—åˆ†ç»„, ä»å‡ºé€šä¿¡é“¾è·¯è½¬å‘åˆ†ç»„. è·¯ç”±å™¨å’Œé“¾è·¯å±‚äº¤æ¢æœº ä¸€ä¸ªåˆ†ç»„ä»å‘é€ç«¯åˆ°æ¥æ”¶ç«¯æ‰€ç»å†çš„ä¸€ç³»åˆ—é€šä¿¡é“¾è·¯å’Œåˆ†ç»„äº¤æ¢æœºç§°ä¸ºé€šè¿‡è¯¥ç½‘ç»œçš„è·¯å¾„ (route æˆ– path) ç«¯ç³»ç»Ÿé€šè¿‡å› ç‰¹ç½‘æœåŠ¡æä¾›å•† (Internet Servise Provider, ISP) æ¥å…¥å› ç‰¹ç½‘ ç«¯ç³»ç»Ÿ, åˆ†ç»„äº¤æ¢æœºå’Œå…¶ä»–å› ç‰¹ç½‘éƒ¨ä»¶, éƒ½è¦è¿è¡Œæ§åˆ¶å› ç‰¹ç½‘ä¸­ä¿¡æ¯æ¥æ”¶å’Œå‘é€çš„ä¸€ç³»åˆ—åè®®. TCP (Transmission Control Protocol, ä¼ è¾“æ§åˆ¶åè®®), IP (Internet Protocol, ç½‘é™…åè®®). IP åè®®å®šä¹‰äº†åœ¨è·¯ç”±å™¨å’Œç«¯ç³»ç»Ÿä¸­å‘é€å’Œæ¥å—çš„åˆ†ç»„çš„æ ¼å¼. å› ç‰¹ç½‘çš„ä¸»è¦åè®®ç»Ÿç§°ä¸º TCP/IP.  1.1.2 æœåŠ¡æè¿°  å› ç‰¹ç½‘ä¹Ÿå¯ä»¥è¢«æè¿°ä¸º: ä¸ºåº”ç”¨ç¨‹åºæä¾›æœåŠ¡çš„åŸºç¡€è®¾æ–½. ä¸å› ç‰¹ç½‘ç›¸è¿çš„ç«¯ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ (Application Programming Interface, API), è§„å®šäº†è¿è¡Œåœ¨ä¸€ä¸ªç«¯ç³»ç»Ÿä¸Šçš„è½¯ä»¶è¯·æ±‚å› ç‰¹ç½‘åŸºç¡€è®¾æ–½å‘è¿è¡Œåœ¨å¦ä¸€ä¸ªç«¯ç³»ç»Ÿä¸Šçš„ç‰¹å®šç›®çš„çš„è½¯ä»¶äº¤ä»˜æ•°æ®çš„æ–¹å¼. ä¸€ä¸ªåè®®å®šä¹‰äº†åœ¨ä¸¤ä¸ªæˆ–å¤šä¸ªé€šä¿¡å®ä½“ä¹‹é—´äº¤æ¢çš„æŠ¥æ–‡æ ¼å¼å’Œæ¬¡åº, ä»¥åŠåœ¨æŠ¥æ–‡ä¼ è¾“å’Œ/æˆ–æ¥å—æˆ–å…¶ä»–äº‹ä»¶æ–¹é¢åšé‡‡å–çš„åŠ¨ä½œ.  1.2 ç½‘ç»œè¾¹ç¼˜ 1.2.1 å®¢æˆ·æœºå’ŒæœåŠ¡å™¨ç¨‹åº  å®¢æˆ·å™¨ç¨‹åºæ˜¯è¿è¡Œåœ¨ä¸€ä¸ªç«¯ç³»ç»Ÿä¸Šçš„ç¨‹åº, å®ƒå‘å‡ºè¯·æ±‚, å¹¶ä»è¿è¡Œåœ¨å¦ä¸€ä¸ªç«¯ç³»ç»Ÿä¸Šçš„æœåŠ¡å™¨ç¨‹åºæ¥å—æœåŠ¡.  å®¢æˆ·æœº-æœåŠ¡å™¨å› ç‰¹ç½‘åº”ç”¨ç¨‹åºæ˜¯åˆ†å¸ƒå¼åº”ç”¨ç¨‹åº   P2P åº”ç”¨ç¨‹åº: å…¶ä¸­çš„ç«¯ç³»ç»Ÿäº’ç›¸ä½œç”¨å¹¶æ‰§è¡Œå®¢æˆ·æœºå’ŒæœåŠ¡å™¨åŠŸèƒ½çš„ç¨‹åº.  1.2.2 æ¥å…¥ç½‘  æ¥å…¥ç½‘: å°†ç«¯ç³»ç»Ÿè¿æ¥åˆ°å…¶è¾¹ç¼˜è·¯ç”±å™¨çš„ç‰©ç†é“¾è·¯ (è¾¹ç¼˜è·¯ç”±å™¨æ˜¯ç«¯ç³»ç»Ÿåˆ°ä»»ä½•å…¶ä»–è¿œç¨‹ç«¯ç³»ç»Ÿçš„è·¯å¾„ä¸Šçš„ç¬¬ä¸€å°è·¯ç”±å™¨)  ç½‘ç»œæ¥å…¥å¤§è‡´åˆ†ä¸ºä¸‰ç§ç±»å‹: ä½å®…æ¥å…¥, å…¬å¸æ¥å…¥, æ— çº¿æ¥å…¥.   ä½å®…æ¥å…¥: å°†å®¶åº­ç«¯ç³»ç»Ÿä¸è¾¹ç¼˜è·¯ç”±å™¨ç›¸è¿æ¥  é€šè¿‡æ™®é€šæ¨¡æ‹Ÿç”µè¯çº¿ç”¨æ‹¨å·è°ƒåˆ¶è§£è°ƒå™¨ä¸ä½å®… ISP ç›¸è¿, å®¶ç”¨è°ƒåˆ¶è§£è°ƒå™¨å°† PC è¾“å‡ºçš„æ•°å­—ä¿¡å·è½¬æ¢ä¸ºæ¨¡æ‹Ÿå½¢å¼, ä»¥ä¾¿åœ¨æ¨¡æ‹Ÿç”µè¯çº¿ä¸Šä¼ è¾“ å®½å¸¦ä½å®…åŒºæ¥å…¥æœ‰ä¸¤ç§å¸¸è§ç±»å‹: æ•°å­—ç”¨æˆ·çº¿ (digital subscriber line, DSL) å’Œæ··åˆå…‰çº¤åŒè½´ç”µç¼† (hybrid fiber-coaxial cable, HFC)   å…¬å¸æ¥å…¥: å±€åŸŸç½‘ (LAN) é€šå¸¸è¢«ç”¨äºè¿æ¥ç«¯ç”¨æˆ·ä¸è¾¹ç¼˜è·¯ç”±å™¨ æ— çº¿æ¥å…¥: åœ¨æ— çº¿å±€åŸŸç½‘ä¸­, æ— çº¿ç”¨æˆ·ä¸ä½äºå‡ åç±³å†…çš„åŸºç«™ä¹‹é—´ä¼ è¾“/æ¥æ”¶åˆ†ç»„. åœ¨å¹¿åŸŸæ— çº¿æ¥å…¥ç½‘ä¸­, åˆ†ç»„ç»ç”¨äºèœ‚çªç”µè¯çš„ç›¸åŒæ— çº¿åŸºç¡€è®¾æ–½è¿›è¡Œå‘é€.  1.3 ç½‘ç»œæ ¸å¿ƒ  é€šè¿‡ç½‘ç»œé“¾è·¯å’Œäº¤æ¢æœºç§»åŠ¨æ•°æ®æœ‰ä¸¤ç§åŸºæœ¬æ–¹æ³•: ç”µè·¯äº¤æ¢å’Œåˆ†ç»„äº¤æ¢  ç”µè·¯äº¤æ¢: æ²¿ç€ç³»ç»Ÿé€šä¿¡è·¯å¾„, ä¸ºç«¯ç³»ç»Ÿä¹‹é—´é€šä¿¡æ‰€æä¾›çš„èµ„æºåœ¨é€šä¿¡ä¼šè¯æœŸé—´ä¼šè¢«é¢„ç•™ \u0026ndash; ç”µè¯ç½‘ç»œ åˆ†ç»„äº¤æ¢: è¿™äº›èµ„æºä¸ä¼šè¢«é¢„ç•™, ä¼šè¯çš„æŠ¥æ–‡æŒ‰éœ€ä½¿ç”¨è¿™äº›èµ„æº, å¯¼è‡´å¯èƒ½ä¸å¾—ä¸ç­‰å¾… (æ’é˜Ÿ) \u0026ndash; å› ç‰¹ç½‘ ä½œè€…ç”¨å»é¥­åº—åƒé¥­çš„ä¾‹å­è§£é‡Šäº†ä»¥ä¸Šä¸¤æ¡: éœ€è¦é¢„çº¦çš„é¥­åº—, å’Œä¸éœ€è¦é¢„çº¦ä½†æ˜¯ä¸ä¸€å®šæœ‰ä½ç½®çš„é¥­åº—.   ç”µè·¯äº¤æ¢å’Œå…¶ä¸­çš„å¤šè·¯å¤ç”¨  é¢‘åˆ†å¤šè·¯å¤ç”¨ (FDM) æ—¶åˆ†å¤šè·¯å¤ç”¨ (TDM)   åˆ†ç»„äº¤æ¢:  æŠ¥æ–‡ (message): åŒ…å«åè®®è®¾è®¡è€…éœ€è¦çš„å…¨éƒ¨ä¿¡æ¯ åˆ†ç»„: æºä¸»æœºå°†é•¿æŠ¥æ–‡åˆ’åˆ†ä¸ºè¾ƒå°çš„æ•°æ®å— åˆ†ç»„äº¤æ¢æœº: äº¤æ¢æœºä¸»è¦æœ‰è·¯ç”±å™¨å’Œé“¾è·¯å±‚äº¤æ¢æœºä¸¤ç±» å­˜å‚¨è½¬å‘ä¼ è¾“æœºåˆ¶: åœ¨äº¤æ¢æœºèƒ½å¤Ÿå¼€å§‹å‘è¾“å‡ºé“¾è·¯ä¼ è¾“è¯¥åˆ†ç»„çš„ç¬¬ä¸€ä¸ªæ¯”ç‰¹ä¹‹å‰, å¿…é¡»æ¥æ”¶åˆ°æ•´ä¸ªåˆ†ç»„ \u0026ndash;\u0026gt; å­˜å‚¨è½¬å‘æ—¶å»¶. è¾“å‡ºç¼“å­˜ (è¾“å‡ºé˜Ÿåˆ—): å¦‚æœåˆ°è¾¾çš„åˆ†ç»„éœ€è¦è·¨è¶Šé“¾è·¯ä¼ è¾“, ä½†å‘ç°è¯¥é“¾è·¯æ­£åœ¨å¿™äºä¼ è¾“å…¶ä»–åˆ†ç»„ \u0026ndash;\u0026gt; æ’é˜Ÿæ—¶å»¶; åˆ°è¾¾çš„åˆ†ç»„å‘ç°è¯¥ç¼“å­˜è¢«ç­‰å¾…ä¼ è¾“çš„åˆ†ç»„å……æ»¡äº† \u0026ndash;\u0026gt; åˆ†ç»„ä¸¢å¤±/ä¸¢åŒ… æŒ‰éœ€å…±äº«èµ„æºæœ‰æ—¶è¢«ç§°ä¸ºèµ„æºçš„ç»Ÿè®¡å¤šè·¯å¤ç”¨ åˆ†ç»„åˆ°è¾¾è·¯ç”±å™¨å, æ¯ä¸ªè·¯ç”±å™¨æœ‰ä¸€ä¸ªè½¬å‘è¡¨, ç”¨äºå°†åˆ†ç»„å¤´éƒ¨çš„ç›®çš„åœ°å€æ˜ å°„åˆ°è¾“å‡ºé“¾è·¯.   ISP å’Œå› ç‰¹ç½‘ä¸»å¹²  ç¬¬ä¸€å±‚ ISP:  ç›´æ¥ä¸å…¶ä»–æ¯ä¸€ä¸ªç¬¬ä¸€å±‚ ISP ç›¸è¿ ä¸å¤§é‡çš„ç¬¬äºŒå±‚ ISP å’Œå…¶ä»–å®¢æˆ·ç½‘ç»œç›¸è¿ \u0026ndash; å®¢æˆ·, æä¾›å•† è¦†ç›–å›½é™…åŒºåŸŸ   å› ç‰¹ç½‘ç”±å‡ åä¸ªç¬¬ä¸€å±‚ ISP å’Œç¬¬äºŒå±‚ ISP ä¸æ•°ä»¥åƒè®¡çš„è¾ƒä½å±‚ ISP ç»„æˆ. ISP è¦†ç›–çš„åŒºåŸŸä¸åŒ, è¾ƒä½å±‚çš„ ISP ä¸è¾ƒé«˜å±‚çš„ ISP ç›¸è¿, è¾ƒé«˜å±‚ ISP å½¼æ­¤äº’è”. ç”¨æˆ·å’Œå†…å®¹æä¾›å•†æ˜¯è¾ƒä½å±‚ ISP çš„å®¢æˆ·, è¾ƒä½å±‚çš„ ISP æ˜¯è¾ƒé«˜å±‚çš„ ISP çš„å®¢æˆ·.    1.4 åˆ†ç»„äº¤æ¢ä¸­çš„æ—¶å»¶ã€ä¸¢åŒ…å’Œååé‡  æ—¶å»¶çš„ç±»å‹  å¤„ç†æ—¶å»¶: æ£€æŸ¥åˆ†ç»„é¦–éƒ¨ä»¥å†³å®šå»å‘, æ£€æŸ¥æ¯”ç‰¹çº§å·®é”™ æ’é˜Ÿæ—¶å»¶: åˆ†ç»„åœ¨é“¾è·¯ä¸Šç­‰å¾…ä¼ è¾“ ä¼ è¾“æ—¶å»¶: $L$è¡¨ç¤ºåˆ†ç»„çš„é•¿åº¦, $R\\ bps$è¡¨ç¤ºä»è·¯ç”±å™¨ A åˆ°è·¯ç”±å™¨ B çš„é“¾è·¯ä¼ è¾“é€Ÿç‡. $L/R$æ˜¯å°†åˆ†ç»„çš„æ‰€æœ‰æ¯”ç‰¹æ¨å‘é“¾è·¯æ‰€éœ€è¦çš„æ—¶é—´ ä¼ æ’­æ—¶å»¶: è¯¥é“¾è·¯çš„èµ·ç‚¹åˆ°è·¯ç”±å™¨ B ä¼ æ’­æ‰€éœ€è¦çš„æ—¶é—´ $t_{nodal} = t_{proc} + t_{queue} + t_{trans} + t_{prop}$   æ’é˜Ÿæ—¶å»¶å’Œä¸¢åŒ…  æµé‡å¼ºåº¦: $a$è¡¨ç¤ºåˆ†ç»„åˆ°è¾¾é˜Ÿåˆ—çš„å¹³å‡é€Ÿç‡, åˆ™æ¯”ç‰¹åˆ°è¾¾é˜Ÿåˆ—çš„å¹³å‡é€Ÿç‡ä¸º$La$, $\\dfrac{La}{R}$å®šä¹‰ä¸ºæµé‡å¼ºåº¦. (è®¾è®¡ç³»ç»Ÿæ—¶æµé‡å¼ºåº¦ä¸èƒ½å¤§äº 1). æµé‡å¼ºåº¦æ¥è¿‘äº 0 æ—¶, å¹³å‡æ’é˜Ÿæ—¶å»¶å°†æ¥è¿‘äº 0; æµé‡å¼ºåº¦æ¥è¿‘äº 1 æ—¶, å¹³å‡æ’é˜Ÿæ—¶å»¶å°†è¿…é€Ÿå¢åŠ ; $t_{queue} = \\dfrac{I}{1-I}\\dfrac{L}{R}$, $I=\\dfrac{La}{R}$è¡¨ç¤ºæµé‡å¼ºåº¦, $\\dfrac{L}{R}$æ˜¯ä¸€ä¸ªåˆ†ç»„çš„ä¼ è¾“æ—¶é—´   ååé‡: ç¬æ—¶ååé‡ (bps) å’Œå¹³å‡ååé‡.  å¯¹äºç®€å•çš„ä¸¤é“¾è·¯ç½‘ç»œ, å‡è®¾$R_S$ä¸ºæœåŠ¡å™¨å’Œè·¯ç”±å™¨ä¹‹é—´çš„é“¾è·¯é€Ÿç‡, $R_C$ä¸ºè·¯ç”±å™¨å’Œå’Œå®¢æˆ·æœºä¹‹é—´çš„é“¾è·¯é€Ÿç‡. ååé‡æ˜¯$\\min{R_S, R_C}$, å³ç“¶é¢ˆé“¾è·¯çš„ä¼ è¾“é€Ÿç‡.    1.5 åè®®å±‚æ¬¡å’Œå®ƒä»¬çš„æœåŠ¡ç±»å‹  åˆ†å±‚çš„ä½“ç³»ç»“æ„: æŸå±‚å¯¹å…¶ä¸Šé¢çš„å±‚æä¾›ç›¸åŒçš„æœåŠ¡, å¹¶ä¸”ä½¿ç”¨æ¥è‡ªä¸‹é¢å±‚æ¬¡çš„æœåŠ¡ (æœåŠ¡ç±»å‹). å½“æŸå±‚çš„å®ç°å˜åŒ–æ—¶, è¯¥ç³»ç»Ÿçš„å…¶ä½™éƒ¨åˆ†å¯ä»¥ä¿æŒä¸å˜. å±‚ n çš„åè®®é€šå¸¸åˆ†å¸ƒåœ¨æ„æˆç½‘ç»œçš„ç«¯ç³»ç»Ÿ, åˆ†ç»„äº¤æ¢æœºå’Œå…¶ä»–ç»„ä»¶ä¸­. å„å±‚çš„æ‰€æœ‰åè®®è¢«ç§°ä¸ºåè®®æ ˆ: ç‰©ç†å±‚, é“¾è·¯å±‚, ç½‘ç»œå±‚, è¿è¾“å±‚, åº”ç”¨å±‚.  åº”ç”¨å±‚: HTTP, SMTP, FTP\u0026hellip; åè®®åˆ†å¸ƒåœ¨ç«¯ç³»ç»Ÿä¸Š, ä¸€ä¸ªç«¯ç³»ç»Ÿä¸­çš„åº”ç”¨ç¨‹åºä½¿ç”¨åè®®ä¸å¦ä¸€ä¸ªç«¯ç³»ç»Ÿä¸­çš„åº”ç”¨ç¨‹åºäº¤æ¢ä¿¡æ¯åˆ†ç»„ (æŠ¥æ–‡). è¿è¾“å±‚: TCP, UDP. è¿è¾“å±‚åˆ†ç»„ç§°ä¸ºæŠ¥æ–‡æ®µ. ç½‘ç»œå±‚: å°†ç§°ä¸ºæ•°æ®æŠ¥çš„ç½‘ç»œå±‚åˆ†ç»„ä»ä¸€å°ä¸»æœºç§»åŠ¨åˆ°å¦ä¸€å°ä¸»æœº. é“¾è·¯å±‚: é€šè¿‡ä¸€ç³»åˆ—è·¯ç”±å™¨åœ¨æºå’Œç›®çš„åœ°ä¹‹é—´å‘é€åˆ†ç»„. é“¾è·¯å±‚åˆ†ç»„ç§°ä¸ºå¸§. ç‰©ç†å±‚: å°†æ•´ä¸ªå¸§ä»ä¸€ä¸ªç½‘ç»œå…ƒç´ ç§»åŠ¨åˆ°é‚»è¿‘çš„ç½‘ç»œå…ƒç´ . åœ¨æ¯ä¸€å±‚, åˆ†ç»„å…·æœ‰ä¸¤ç§ç±»å‹çš„å­—æ®µ: é¦–éƒ¨å­—æ®µå’Œæœ‰æ•ˆè½½è·å­—æ®µ, æœ‰æ•ˆè½½è·é€šå¸¸æ¥è‡ªä¸Šä¸€åˆ†ç»„.    ","permalink":"https://michelia-zhx.github.io/posts/2022-02-26-computer_network_chap1/","summary":"Chap 1 è®¡ç®—æœºç½‘ç»œå’Œå› ç‰¹ç½‘ 1.1 ä»€ä¹ˆæ˜¯å› ç‰¹ç½‘ 1.1.1 å› ç‰¹ç½‘çš„å…·ä½“æ„æˆæè¿° å„ç§å„æ ·çš„è®¾å¤‡éƒ½è¢«è¿æ¥åˆ°å› ç‰¹ç½‘,è¿™äº›è®¾å¤‡éƒ½è¢«ç§°ä¸ºä¸»æœºæˆ–ç«¯ç³»ç»Ÿ. ç«¯ç³»ç»Ÿé€šè¿‡é€šä¿¡é“¾è·¯å’Œ","title":"Notes - Computer Networking -- Chap 1"},{"content":"Chap 2 åº”ç”¨å±‚ 2.1 åº”ç”¨å±‚åŸç†  ç½‘ç»œåº”ç”¨çš„ä½“ç³»ç»“æ„:  å®¢æˆ·-æœåŠ¡å™¨ (C/S) æ¨¡å¼ (å¯æ‰©å±•æ€§å·®, æ€§èƒ½éšç”¨æˆ·æ•°é‡å¢åŠ ä¼šæœ‰æ–­å´–å¼ä¸‹é™) å¯¹ç­‰ä½“ (P2P) ä½“ç³»ç»“æ„ æ··åˆä½“   è¿›ç¨‹é€šä¿¡: åŒä¸€ä¸»æœºå†…, ä½¿ç”¨è¿›ç¨‹é—´é€šä¿¡æœºåˆ¶é€šä¿¡ (æ“ä½œç³»ç»Ÿå®šä¹‰); ä¸åŒä¸»æœº, é€šè¿‡äº¤æ¢æŠ¥æ–‡æ¥é€šä¿¡  å¯¹è¿›ç¨‹è¿›è¡Œç¼–å€ addressing:  è¿›ç¨‹ä¸ºäº†æ¥å—æŠ¥æ–‡, å¿…é¡»æœ‰ä¸€ä¸ªæ ‡è¯† SAP: IP åœ°å€, æ‰€é‡‡ç”¨çš„ä¼ è¾“å±‚åè®®æ˜¯ TCP è¿˜æ˜¯ UDP, TCP/UDP çš„ç«¯å£å·; æœ¬è´¨ä¸Š, ä¸€å¯¹ä¸»æœºçš„è¿›ç¨‹ä¹‹é—´çš„é€šä¿¡ç”± 2 ä¸ªç«¯èŠ‚ç‚¹æ„æˆ.     ä¼ è¾“å±‚æä¾›çš„æœåŠ¡ - å±‚é—´ä¿¡æ¯çš„ä»£è¡¨: å¦‚æœ socket api æ¯æ¬¡ä¼ è¾“æŠ¥æ–‡éƒ½è¦æºå¸¦åŒæ–¹åœ°å€å’Œå†…å®¹, ä¼šè¿‡äºç¹ç.  ç”¨ä¸ªä»£å·æ ‡ç¤ºé€šä¿¡çš„åŒæ–¹æˆ–å•æ–¹: socket; å°±æƒ³ OS æ‰“å¼€æ–‡ä»¶è¿”å›çš„å¥æŸ„ä¸€æ · \u0026ndash; å¯¹å¥æŸ„çš„æ“ä½œå°±æ˜¯å¯¹æ–‡ä»¶çš„æ“ä½œ; TCP socket: ç›¸å½“äºæœ¬åœ° IP æœ¬åœ° TCP ç«¯å£, å¯¹æ–¹ IP å¯¹æ–¹ TCP ç«¯å£çš„æœ¬åœ°æ ‡è¯† (ç”¨äºæŒ‡æ˜åº”ç”¨è¿›ç¨‹ä¼šè¯çš„æœ¬åœ°æ ‡è¯†); UDP socket: ä¸¤ä¸ªè¿›ç¨‹ä¹‹é—´çš„é€šä¿¡éœ€è¦ä¹‹å‰æ— éœ€å»ºç«‹è¿æ¥ (æ¯ä¸ªæŠ¥æ–‡éƒ½æ˜¯ç‹¬ç«‹ä¼ è¾“çš„). ç”¨ä¸€ä¸ªæ•´æ•°è¡¨ç¤ºæœ¬åº”ç”¨å®ä½“çš„æ ‡ç¤º: æœ¬ IP, æœ¬ç«¯å£. ä½†æ˜¯ä¼ è¾“æŠ¥æ–‡æ—¶, å¿…é¡»æä¾›å¯¹æ–¹ IP, port; æ¥å—æŠ¥æ–‡æ—¶, ä¼ è¾“å±‚éœ€è¦ä¸Šä¼ å¯¹æ–¹çš„ IP, port.   åº”ç”¨å±‚éœ€è¦ä¼ è¾“å±‚æä¾›çš„æœåŠ¡? å¦‚ä½•æè¿°ä¼ è¾“å±‚çš„æœåŠ¡?  æ•°æ®ä¸¢å¤±ç‡ å»¶è¿Ÿ åå å®‰å…¨æ€§   TCP æœåŠ¡  å¯é çš„ä¼ è¾“æœåŠ¡ æµé‡æ§åˆ¶: å‘é€æ–¹ä¸ä¼šæ·¹æ²¡æ¥å—æ–¹ æ‹¥å¡æ§åˆ¶: å½“ç½‘ç»œå‡ºç°æ‹¥å¡æ—¶, èƒ½æŠ‘åˆ¶å‘é€æ–¹ ä¸èƒ½æä¾›çš„æœåŠ¡: æ—¶é—´ä¿è¯ã€æœ€å°ååä¿è¯å’Œå®‰å…¨ é¢å‘è¿æ¥: è¦æ±‚åœ¨å®¢æˆ·ç«¯è¿›ç¨‹å’ŒæœåŠ¡å™¨è¿›ç¨‹ä¹‹é—´å»ºç«‹è¿æ¥   UDP æœåŠ¡:  ä¸å¯é çš„æ•°æ®ä¼ è¾“ ä¸æä¾›çš„æœåŠ¡: å¯é , æµé‡æ§åˆ¶, æ‹¥å¡æ§åˆ¶, æ—¶é—´å¸¦å®½ä¿è¯, å»ºç«‹è¿æ¥ å­˜åœ¨çš„å¿…è¦æ€§:  èƒ½å¤ŸåŒºåˆ†ä¸åŒè¿›ç¨‹ æ— éœ€å»ºç«‹è¿æ¥ ä¸åšå¯é æ€§çš„å·¥ä½œ æ²¡æœ‰æµé‡æ§åˆ¶, æ‹¥å¡æ§åˆ¶, åº”ç”¨èƒ½å¤ŸæŒ‰ç…§è®¾å®šçš„é€Ÿåº¦å‘é€æ•°æ®      2.2 Web and HTTP  Web é¡µ: ç”±ä¸€äº›å¯¹è±¡ç»„æˆ. å«æœ‰ä¸€ä¸ªåŸºæœ¬çš„ html æ–‡ä»¶, å…¶ä¸­åŒ…å«è‹¥å¹²å¯¹è±¡çš„å¼•ç”¨ (ä»¥ url çš„å½¢å¼è¡¨ç¤º: åè®®å+ç”¨æˆ·:å£ä»¤+ä¸»æœºå+è·¯å¾„å+ç«¯å£) HTTP: è¶…æ–‡æœ¬ä¼ è¾“åè®®  Web çš„åº”ç”¨å±‚åè®® å®¢æˆ·/æœåŠ¡å™¨æ¨¡å¼ ä½¿ç”¨ TCP  å®¢æˆ·å‘èµ·ä¸€ä¸ªä¸æœåŠ¡å™¨çš„ TCP è¿æ¥, ç«¯å£å·ä¸º 80 æœåŠ¡å™¨æ¥å—å®¢æˆ·çš„ TCP è¿æ¥ (æœåŠ¡å™¨æœ‰ waiting socket å’Œ connection socket, æ¯å»ºç«‹ä¸€ä¸ªè¿æ¥, å¼¹å‡ºä¸€ä¸ª connection socket, åŒæ—¶ waiting socket å§‹ç»ˆå­˜åœ¨) åœ¨æµè§ˆå™¨ä¸ Web æœåŠ¡å™¨äº¤æ¢ HTTP æŠ¥æ–‡ TCP è¿æ¥å…³é—­   æ— çŠ¶æ€æœåŠ¡å™¨: æœåŠ¡å™¨ä¸éœ€è¦ç»´æŠ¤å®¢æˆ·ç«¯çš„çŠ¶æ€ å»ºç«‹è¿æ¥éœ€è¦ä¸€æ¬¡å¾€è¿” (å»ºç«‹è¿æ¥è¯·æ±‚ + å»ºç«‹è¿æ¥ç¡®è®¤), http è¯·æ±‚å¯¹è±¡, è¿”å›å¯¹è±¡ (å“åº”æŠ¥æ–‡)    éæŒä¹… (æœ€å¤šåªæœ‰ä¸€ä¸ªå¯¹è±¡åœ¨ TCP è¿æ¥ä¸Šå‘é€, ä¸‹è½½å¤šä¸ªå¯¹è±¡éœ€è¦å¤šä¸ª TCP è¿æ¥ \u0026ndash; HTTP 1.0) å’ŒæŒä¹… (å¤šä¸ªå¯¹è±¡åœ¨ä¸€ä¸ª TCP è¿æ¥ä¸Šä¼ è¾“ \u0026ndash; HTTP 1.1) HTTP  éæµæ°´æ–¹å¼çš„æŒä¹… HTTP: å®¢æˆ·ç«¯åªæœ‰åœ¨æ”¶åˆ°å‰ä¸€ä¸ªå“åº”åæ‰èƒ½äº§ç”Ÿä¸€ä¸ªè¯·æ±‚, æ¯ä¸ªå¼•ç”¨å¯¹è±¡èŠ±è´¹ä¸€ä¸ª RTT æµæ°´æ–¹å¼çš„æŒä¹… HTTP: å®¢æˆ·ç«¯é‡åˆ°ä¸€ä¸ªå¼•ç”¨å¯¹è±¡å°±ç«‹å³äº§ç”Ÿä¸€ä¸ªè¯·æ±‚, æ‰€æœ‰å¼•ç”¨ (å°) å¯¹è±¡åªèŠ±è´¹ä¸€ä¸ª RTT æ˜¯å¯èƒ½çš„   HTTP è¯·æ±‚æŠ¥æ–‡  ä¸¤ç§ç±»å‹: è¯·æ±‚, å“åº” ASCII: è¯·æ±‚è¡Œ (GET, POST, HEAD å‘½ä»¤), é¦–éƒ¨è¡Œ, æ¢è¡Œå›è½¦ç¬¦ (è¡¨ç¤ºæŠ¥æ–‡ç»“æŸ) æäº¤è¡¨å•è¾“å…¥  Post æ–¹å¼ URL æ–¹å¼     HTTP å“åº”æŠ¥æ–‡  çŠ¶æ€è¡Œ (åè®®ç‰ˆæœ¬, çŠ¶æ€ç å’Œç›¸åº”çŠ¶æ€ä¿¡æ¯), é¦–éƒ¨è¡Œ, æ•°æ® (å¦‚è¯·æ±‚çš„ HTML æ–‡ä»¶)   ç”¨æˆ· - æœåŠ¡å™¨çš„çŠ¶æ€: cookies  ç»„æˆéƒ¨åˆ†:  åœ¨ HTTP å“åº”æŠ¥æ–‡ä¸­æœ‰ä¸€ä¸ª cookie çš„é¦–éƒ¨è¡Œ åœ¨ HTTP è¯·æ±‚æŠ¥æ–‡å«æœ‰ä¸€ä¸ª cookie çš„é¦–éƒ¨è¡Œ åœ¨ç”¨æˆ·ç«¯ç³»ç»Ÿä¸­ä¿ç•™æœ‰ä¸€ä¸ª cookie æ–‡ä»¶, ç”±ç”¨æˆ·çš„æµè§ˆå™¨ç®¡ç† åœ¨ Web ç«™ç‚¹æœ‰ä¸€ä¸ªåç«¯æ•°æ®åº“      Web ç¼“å­˜ (ä»£ç†æœåŠ¡å™¨ proxy server) \u0026ndash; ä¸è®¿é—®åŸå§‹æœåŠ¡å™¨, å°±æ»¡è¶³å®¢æˆ·éœ€æ±‚  ç”¨æˆ·è®¾ç½®æµè§ˆå™¨: é€šè¿‡ç¼“å­˜è®¿é—® Web æµè§ˆå™¨å°†æ‰€æœ‰çš„ HTTP è¯·æ±‚å‘ç»™ç¼“å­˜ ç¼“å­˜æ—¢æ˜¯å®¢æˆ·ç«¯åˆæ˜¯æœåŠ¡å™¨ ç”¨å¾ˆå°çš„ç¼“å­˜å°±å¯ä»¥è§£å†³å¤§å¤šæ•°éœ€æ±‚ ( 28 åˆ†å¸ƒ, è®¿é—®çš„è¶‹åŒæ€§) å®‰è£…æœ¬åœ°ç¼“å­˜æ•ˆæœè¿œå¥½äºæ‰©å®½é“¾è·¯ æ¡ä»¶ GET æ–¹æ³•: å¦‚æœç¼“å­˜ä¸­çš„å¯¹è±¡æ‹·è´æ˜¯æœ€æ–°çš„, å°±ä¸è¦å‘é€å¯¹è±¡    2.3 FTP: æ–‡ä»¶ä¼ è¾“åè®®  å‘è¿œç¨‹ä¸»æœºä¸Šä¼ æ–‡ä»¶æˆ–ä»è¿œç¨‹ä¸»æœºæ¥æ”¶æ–‡ä»¶ å®¢æˆ·/æœåŠ¡å™¨æ¨¡å¼  å®¢æˆ·ç«¯: å‘èµ·ä¼ è¾“çš„ä¸€æ–¹ æœåŠ¡å™¨: è¿œç¨‹ä¸»æœº   æ§åˆ¶è¿æ¥ä¸æ•°æ®è¿æ¥åˆ†å¼€  FTPå®¢æˆ·ç«¯ä¸FTPæœåŠ¡å™¨æœåŠ¡å™¨é€šè¿‡ç«¯å£21è”ç³», å¹¶ä½¿ç”¨TCPä¸ºä¼ è¾“åè®® å®¢æˆ·ç«¯é€šè¿‡æ§åˆ¶è¿æ¥è·å¾—èº«ä»½ç¡®è®¤ å®¢æˆ·ç«¯é€šè¿‡æ§åˆ¶è¿æ¥å‘é€å‘½ä»¤æµè§ˆè¿œç¨‹ç›®å½• æ”¶åˆ°ä¸€ä¸ªæ–‡ä»¶ä¼ è¾“å‘½ä»¤æ—¶, æœåŠ¡å™¨æ‰“å¼€ä¸€ä¸ªåˆ°å®¢æˆ·ç«¯çš„æ•°æ®è¿æ¥ ä¸€ä¸ªæ–‡ä»¶ä¼ è¾“å®Œæˆå, æœåŠ¡å™¨å…³é—­è¿æ¥   ç‰¹ç‚¹  æœåŠ¡å™¨æ‰“å¼€ç¬¬äºŒä¸ª TCP æ•°æ®è¿æ¥ç”¨æ¥ä¼ è¾“å¦ä¸€ä¸ªæ–‡ä»¶ æ§åˆ¶è¿æ¥: å¸¦å¤– (\u0026ldquo;out of band\u0026rdquo;) ä¼ é€ FTPæœåŠ¡å™¨ç»´æŠ¤ç”¨æˆ·çš„çŠ¶æ€ä¿¡æ¯: å½“å‰è·¯å¾„, ç”¨æˆ·å¸æˆ·ä¸æ§åˆ¶è¿æ¥å¯¹åº” \u0026ndash; æœ‰çŠ¶æ€    2.4 Email  ä¸‰ä¸ªç»„æˆéƒ¨åˆ†:  ç”¨æˆ·ä»£ç† (\u0026ldquo;é‚®ä»¶é˜…è¯»å™¨\u0026rdquo;, é˜…è¯», ç¼–è¾‘, Foxmail, Outlook) é‚®ä»¶æœåŠ¡å™¨ ç®€å•é‚®ä»¶ä¼ è¾“åè®®: SMTP    SMTP [RFC 2821]  ä½¿ç”¨TCPåœ¨å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´ä¼ é€æŠ¥æ–‡, ç«¯å£å·ä¸º25 ç›´æ¥ä¼ è¾“: ä»å‘é€æ–¹æœåŠ¡å™¨åˆ°æ¥æ”¶æ–¹æœåŠ¡å™¨ ä¼ è¾“çš„3ä¸ªé˜¶æ®µ  æ¡æ‰‹ ä¼ è¾“æŠ¥æ–‡ å…³é—­   å‘½ä»¤/å“åº”äº¤äº’  å‘½ä»¤: ASCIIæ–‡æœ¬ å“åº”: çŠ¶æ€ç å’ŒçŠ¶æ€ä¿¡æ¯   æŠ¥æ–‡å¿…é¡»ä¸º7ä½ASCIIç     2.5 DNS (Domain Name System)  IP åœ°å€æ ‡è¯†ä¸»æœº, è·¯ç”±å™¨. DNS è´Ÿè´£åŸŸå - IP åœ°å€çš„è½¬æ¢. DNSçš„ä¸»è¦æ€æƒ³:  åˆ†å±‚çš„ã€åŸºäºåŸŸçš„å‘½åæœºåˆ¶ è‹¥å¹²åˆ†å¸ƒå¼çš„æ•°æ®åº“å®Œæˆåå­—åˆ°IPåœ°å€çš„è½¬æ¢ è¿è¡Œåœ¨UDPä¹‹ä¸Šç«¯å£å·ä¸º53çš„åº”ç”¨æœåŠ¡ æ ¸å¿ƒçš„ Internet åŠŸèƒ½, ä½†ä»¥åº”ç”¨å±‚åè®®å®ç°  åœ¨ç½‘ç»œè¾¹ç¼˜å¤„ç†å¤æ‚æ€§     DNSä¸»è¦ç›®çš„:  å®ç°ä¸»æœºå-IPåœ°å€çš„è½¬æ¢(name/IP translate) å…¶å®ƒç›®çš„:  ä¸»æœºåˆ«ååˆ°è§„èŒƒåå­—çš„è½¬æ¢:â€€Host aliasing é‚®ä»¶æœåŠ¡å™¨åˆ«ååˆ°é‚®ä»¶æœåŠ¡å™¨çš„æ­£è§„åå­—çš„è½¬æ¢:â€€Mail server aliasing è´Ÿè½½å‡è¡¡:â€€Load Distribution     DNS åŸŸåç»“æ„  ä¸€ä¸ªå±‚é¢å‘½åè®¾å¤‡ä¼šæœ‰å¾ˆå¤šé‡å DNS é‡‡ç”¨å±‚æ¬¡æ ‘çŠ¶ç»“æ„çš„å‘½åæ–¹æ³• Internet æ ¹è¢«åˆ’åˆ†ä¸ºå‡ ç™¾ä¸ªé¡¶çº§åŸŸ  é€šç”¨çš„ (generic): .com, .edu, .gov, .int, .mil, .net, .org, .firm, .hsop, .web, .arts, .rec; å›½å®¶çš„ (contries): .cn, .us, .nl, .jp;   æ¯ä¸ª(å­)åŸŸä¸‹é¢å¯åˆ’åˆ†ä¸ºè‹¥å¹²å­åŸŸ (subdomains) æ ‘å¶æ˜¯ä¸»æœº   åŒºåŸŸ (zone)  åŒºåŸŸçš„åˆ’åˆ†æœ‰åŒºåŸŸç®¡ç†è€…è‡ªå·±å†³å®š å°†DNSåå­—ç©ºé—´åˆ’åˆ†ä¸ºäº’ä¸ç›¸äº¤çš„åŒºåŸŸ, æ¯ä¸ªåŒºåŸŸéƒ½æ˜¯æ ‘çš„ä¸€éƒ¨åˆ† åå­—æœåŠ¡å™¨  æ¯ä¸ªåŒºåŸŸéƒ½æœ‰ä¸€ä¸ªåå­—æœåŠ¡å™¨: ç»´æŠ¤ç€å®ƒæ‰€ç®¡è¾–åŒºåŸŸçš„æƒå¨ä¿¡æ¯ (authoritative record) åå­—æœåŠ¡å™¨å…è®¸è¢«æ”¾ç½®åœ¨åŒºåŸŸä¹‹å¤–, ä»¥ä¿éšœå¯é æ€§     é¡¶çº§åŸŸ (TLD) æœåŠ¡å™¨: è´Ÿè´£é¡¶çº§åŸŸå: å¦‚com, org, net, eduå’Œgov; å’Œæ‰€æœ‰å›½å®¶çº§çš„é¡¶çº§åŸŸå, å¦‚cn, uk, fr, ca, jp) åŒºåŸŸåå­—æœåŠ¡å™¨ç»´æŠ¤èµ„æºè®°å½•  èµ„æºè®°å½• (resource records)  ä½œç”¨:â€€ç»´æŠ¤åŸŸå - IPåœ°å€ (å…¶å®ƒ) çš„æ˜ å°„å…³ç³» ä½ç½®:â€€Name Server çš„åˆ†å¸ƒå¼æ•°æ®åº“ä¸­   RRæ ¼å¼: (domain_name, ttl, type, class, Value)  Domain_name: åŸŸå Ttl - time to live : ç”Ÿå­˜æ—¶é—´ (æƒå¨, ç¼“å†²è®°å½•) Class ç±»åˆ«:â€€å¯¹äºInternet, å€¼ä¸ºIN Value å€¼:â€€å¯ä»¥æ˜¯æ•°å­—â€€åŸŸåæˆ–ASCIIä¸² Type ç±»åˆ«:â€€èµ„æºè®°å½•çš„ç±»å‹      DNSå¤§è‡´å·¥ä½œæµç¨‹  åº”ç”¨è°ƒç”¨è§£æå™¨ (resolver) è§£æå™¨ä½œä¸ºå®¢æˆ·å‘ Name Server å‘å‡ºæŸ¥è¯¢æŠ¥æ–‡ (å°è£…åœ¨UDPæ®µä¸­) Name Serverâ€€å›å“åº”æŠ¥æ–‡(name/ip)    æœ¬åœ°åå­—æœåŠ¡å™¨â€€(Local Name Server)  å¹¶ä¸ä¸¥æ ¼å±äºå±‚æ¬¡ç»“æ„ æ¯ä¸ªISP (å±…æ°‘åŒºçš„ISPã€å…¬å¸ã€å¤§å­¦, éƒ½æœ‰ä¸€ä¸ªæœ¬åœ°DNSæœåŠ¡å™¨)  ä¹Ÿç§°ä¸ºâ€œæœ¬åœ°åå­—æœåŠ¡å™¨â€   å½“ä¸€ä¸ªä¸»æœºå‘èµ·ä¸€ä¸ªDNSæŸ¥è¯¢æ—¶, æŸ¥è¯¢å‘é€åˆ°å…¶æœ¬åœ°DNSæœåŠ¡å™¨  èµ·ç€ä»£ç†çš„ä½œç”¨, å°†æŸ¥è¯¢è½¬å‘åˆ°å±‚æ¬¡ç»“æ„ä¸­     ä¸¤ç§æŸ¥è¯¢:   2.6 P2P åº”ç”¨  æ²¡æœ‰(æå°‘)ä¸€ç›´è¿è¡Œçš„æœåŠ¡å™¨; ä»»æ„ç«¯ç³»ç»Ÿç›´æ¥é€šä¿¡; åˆ©ç”¨ peer çš„æœåŠ¡èƒ½åŠ›; peer èŠ‚ç‚¹é—´æ­‡ä¸Šç½‘, æ¯æ¬¡ IP åœ°å€éƒ½æœ‰å¯èƒ½å˜åŒ–. æœåŠ¡å™¨ä¸Šè½½ N ä»½æ–‡ä»¶, N ä¸ªå®¢æˆ·ç«¯ä¸‹è½½çš„æ—¶é—´ä¸‹é™  C/S æ¨¡å¼ä¸‹, $\\max{\\dfrac{F}{d_{min}}, N\\dfrac{F}{u_s}}$. P2P æ¨¡å¼ä¸‹, $\\max{\\dfrac{F}{d_{min}}, N\\dfrac{F}{u_s+\\sum Nu_c}}$.   ä¸¤å¤§é—®é¢˜: å¦‚ä½•å®šä½æ‰€éœ€èµ„æº; å¦‚ä½•å¤„ç†ç”¨æˆ·çš„åŠ å…¥ä¸ç¦»å¼€. overlay è¦†ç›–ç½‘ (åº”ç”¨å±‚é€»è¾‘ç½‘ç»œ) éç»“æ„åŒ– P2P  é›†ä¸­åŒ–ç›®å½• (æœ‰ä¸€ä¸ªèŠ‚ç‚¹ç»´æŠ¤å…¨å±€ç›®å½•ä¿¡æ¯) (å•ç‚¹æ•…éšœ, æ€§èƒ½ç“¶é¢ˆ, ç‰ˆæƒä¾µçŠ¯) å®Œå…¨åˆ†å¸ƒå¼ (æ²¡æœ‰èŠ‚ç‚¹ç»´æŠ¤å…¨å±€ç›®å½•ä¿¡æ¯) æ´ªæ³› (flooding) å¼æŸ¥è¯¢  æ··åˆä½“ (ç»„é•¿ç»´æŠ¤å±€éƒ¨ç›®å½•ä¿¡æ¯)    DHTC (Distributed Hash Table) (ç»“æ„åŒ–) P2P  Hash DHTæ–¹æ¡ˆ ç¯å½¢DHT ä»¥åŠè¦†ç›–ç½‘ç»œ Peeræ³¢åŠ¨    ","permalink":"https://michelia-zhx.github.io/posts/2022-03-08-computer_network_chep2/","summary":"Chap 2 åº”ç”¨å±‚ 2.1 åº”ç”¨å±‚åŸç† ç½‘ç»œåº”ç”¨çš„ä½“ç³»ç»“æ„: å®¢æˆ·-æœåŠ¡å™¨ (C/S) æ¨¡å¼ (å¯æ‰©å±•æ€§å·®, æ€§èƒ½éšç”¨æˆ·æ•°é‡å¢åŠ ä¼šæœ‰æ–­å´–å¼ä¸‹é™) å¯¹ç­‰ä½“ (P2P) ä½“ç³»ç»“æ„ æ··åˆä½“ è¿›ç¨‹é€šä¿¡:","title":"Notes - Computer Networking -- Chap 2"},{"content":"Active Learning ä¹Ÿç§°ä¸ºæŸ¥è¯¢å­¦ä¹ æˆ–è€…æœ€ä¼˜å®éªŒè®¾è®¡. ä¸»åŠ¨å­¦ä¹ é€šè¿‡è®¾è®¡åˆç†çš„æŸ¥è¯¢å‡½æ•°, ä¸æ–­ä»æœªæ ‡æ³¨çš„æ•°æ®ä¸­æŒ‘é€‰å‡ºæ•°æ®æ ‡æ³¨åæ”¾å…¥è®­ç»ƒé›†. æœ‰æ•ˆçš„ä¸»åŠ¨å­¦ä¹ æ•°æ®é€‰æ‹©ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°é™ä½è®­ç»ƒçš„ä»£ä»·å¹¶åŒæ—¶æé«˜æ¨¡å‹çš„è¯†åˆ«èƒ½åŠ›.\nä¸€. ä¸»åŠ¨å­¦ä¹ çš„åœºæ™¯   Membership Query Sythesis ç”Ÿæˆä¸€ä¸ªè¯¢é—®, å¹¶è¯·æ±‚è¿™ä¸ªæ ·æœ¬çš„æ ‡ç­¾, è¿™ä¸ªæ ·æœ¬å¯èƒ½æ˜¯æœªæ ‡æ³¨æ•°æ®ä¸­çš„ä»»æ„ä¸€ä¸ª, ç”šè‡³æ˜¯ä»å¤´ç”Ÿæˆçš„, åæ­£è¿™äº›æ•°æ®ä¸€èˆ¬ä¸æ˜¯ç®€å•çš„æœä»ä¸€ä¸ªè‡ªç„¶åˆ†å¸ƒçš„éšæœº.\n  Stream-Based (Sequential) Selective Sampling åŸºäºæµçš„æœ‰é€‰æ‹©æ€§åœ°é€‰æ‹©ç›®æ ‡, è¿™ç§é€šå¸¸æ˜¯å‡è®¾ä¼šæœ‰å¤§é‡å»‰ä»·çš„æ— æ ‡ç­¾æ•°æ®. å®ƒå°†é‡‡æ ·ä¸€ä¸ªæ— æ ‡ç­¾çš„æ•°æ®, ç„¶åå†³å®šæ˜¯è¦è¯¢é—®å®ƒçš„æ ‡ç­¾è¿˜æ˜¯å¿½ç•¥å®ƒ (å¦‚æœæ•°æ®æ˜¯å‡åŒ€åˆ†å¸ƒ, é‚£ä¹ˆå’Œä¸Šä¸€ç§æƒ…å†µä¸€æ ·), é€šå¸¸åŸºäºä¸‹é¢çš„ä¸¤ä¸ªåº¦é‡:\n æ›´å¤§çš„ä¿¡æ¯é‡: é€‰æ‹©è¿™äº›å…·æœ‰é«˜ä¿¡æ¯é‡çš„æ•°æ® ä¸ç¡®å®šæ€§åŸåˆ™: é€‰æ‹©è½åœ¨è¿™ç§ä¸ç¡®å®šåŸŸä¹‹ä¸­çš„æ•°æ®    Pool Based Sampling åŸºäºæ± çš„é‡‡æ ·, å‡è®¾èƒ½å¤Ÿä¸€æ¬¡æ€§è·å¾—å¤§é‡æœªæ ‡æ³¨çš„æ•°æ®, å¹¶è¿›è¡ŒåŒæ—¶å¤„ç†. å¯ä»¥å¯¹æ•°æ®æ± ä¸­çš„æ•°æ®è¿›è¡Œä¿¡æ¯é‡æ’åº, ç›´æ¥é‡‡æœ€æœ‰ä¿¡æ¯é‡çš„æ•°æ®è¿›è¡Œåˆ†æ.\n  åŸºäºæµçš„åœºæ™¯ä¸‹, æ•°æ®æ˜¯é¡ºåºæ¥åˆ°çš„, ä¸ä¼šæœ‰å…¨å±€çš„è§†é‡, è€ŒåŸºäºæ± çš„åˆ™æ˜¯æ›´åŠ å¸¸è§çš„åšæ³•å¯ä»¥ä¸€æ¬¡æ€§å¯¹æ‰€æœ‰æ•°æ®çš„ä¿¡æ¯é‡è¿›è¡Œåˆ†æ. ä½†æ˜¯æœ‰çš„æ—¶å€™å› ä¸ºæ•°æ®çš„ç”Ÿæˆæƒ…å†µæˆ–è€…è®¡ç®—å¸¦å®½å†…å­˜ç­‰çš„é™åˆ¶, äººä»¬è¿˜æ˜¯ä¸å¾—å·²è¿˜æ˜¯è¦ä½¿ç”¨åŸºäºæµçš„åœºæ™¯.\näºŒ. Query Strategy Frameworks ç”¨$x^*_A$è¡¨ç¤ºæŸç§é‡‡æ ·ç®—æ³•$A$ä¸‹æœ€æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬.\nUncertainty Sampling  ç”¨ç†µä½œåº¦é‡: $x^*_{ENT} = \\rm{argmax}_x -\\sum_i P(y_i|x;\\theta)\\log P(y_i|x;\\theta)$ ç”¨ç½®ä¿¡åº¦ä½œåº¦é‡: $x^_{LC} = \\rm{argmin}_x P(y^|x; \\theta)$, $y^* = \\rm{argmin}y P{\\theta}(y|x)$, é€‰å–æœ€å¤§ç½®ä¿¡åº¦æœ€å°çš„æ ·æœ¬ ç”¨æœ€å°é—´éš”ä½œåº¦é‡: $x^_{SM} = \\rm{argmax}x P{\\theta}(y^1|x) - P{\\theta}(y^_2|x), y^_1, y^*_2$ ä¸ºå¯èƒ½æ€§æœ€é«˜çš„ä¸¤ä¸ªæ ·æœ¬.  Query-By-Committee (QBC)  é€‰æ‹©ä¸€å®šæ•°é‡çš„æ¨¡å‹æ„æˆå§”å‘˜ä¼š $\\mathcal{C}={\\theta^{(1)},\u0026hellip;,\\theta^{(C)}}$, å¯¹æœªæ ‡æ³¨çš„æ•°æ®è¿›è¡Œå¤„ç†, æŒ‘é€‰å‡ºæ‰€æœ‰æœªæ ‡è®°æ•°æ®ä¸­å„ä¸ªæ¨¡å‹æ„è§æœ€ä¸ä¸€è‡´çš„æ ·æœ¬. ä¸ä¸€è‡´çš„åº¦é‡:  ç”¨æŠ•ç¥¨ç†µä½œåº¦é‡: $$x^*_{VE} = \\rm{argmax}_x-\\sum_i\\dfrac{V(y_i)}{C}\\log\\dfrac{V(y_i)}{C}$$ ç”¨KLæ•£åº¦(è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚)ä½œåº¦é‡: $$x^*{KL} = \\argmax_x\\dfrac{1}{C}\\sum{c=1}^CD(P_{\\theta^{(c)}}|P_{\\mathcal{C}})$$, å…¶ä¸­ $$D(P_{\\theta^{(c)}}|P_{\\mathcal{C}}) = \\sum_iP(y_i|x;\\theta^{(c)})\\log\\dfrac{P(y_i|x;\\theta^{(c)})}{P(y_i|x;\\mathcal{C})}$$    Expected Model Change  å»é‡‡çš„æ ·æœ¬åº”å½“å…·å¤‡æ¡ä»¶: å½“å®ƒè¢«èµ‹äºˆæ ‡è®°, åº”å½“æœ€å¤§ç¨‹åº¦ä¼˜åŒ–æ¨¡å‹ â€”â€” ç”¨è®­ç»ƒæ¢¯åº¦ä½œä¸ºè¿™ç§ä¼˜åŒ–çš„è¡¡é‡. $$x^*_{EGL} = \\rm{argmax}_x\\sum_iP(y_i|x;\\theta)|\\nabla \\mathcal{l}(\\mathcal{L}\\cup\\langle x, y_i\\rangle;\\theta)|$$  Variance Reduction and Fisher Information Ratio   æœ€å°åŒ–å­¦ä¹ å™¨çš„æœªæ¥è¯¯å·® (åç½®-æ–¹å·®åˆ†è§£): $$E_T[(o-y)^2|x] = E[(y-E[y|x])^2] + (E_{\\mathcal{L}}[o] - E[y|x])^2 + E_{\\mathcal{L}}[(o-E_{\\mathcal{L}}[o])^2]$$\n  å³è¾¹ä¸‰é¡¹åˆ†åˆ«ä¸ºå™ªéŸ³ã€åç½®çš„å¹³æ–¹ã€æ–¹å·®. åªé€‚ç”¨äºå›å½’ä»»åŠ¡.\n  ç¦»æ•£åˆ†ç±»å™¨ä½¿ç”¨Fisher Information: $$\\mathcal{I}(\\theta) = -\\int_xP(x)\\int_yP(y|x;\\theta)\\dfrac{\\partial^2}{\\partial\\theta^2}\\log P(y|x;\\theta)$$\n  æœ€ä½³æ£€ç´¢æ ·æœ¬åº”æœ€å°åŒ–Fisher Information Ratio: $$x^*_{FIR} = \\argmin_xtr(\\mathcal{I}x(\\theta)^{-1}\\mathcal{I}{\\mathcal{U}}(\\theta))$$\n  $\\mathcal{I}x(\\theta)$ä¸ä»…è¯´æ˜æ¨¡å‹å¯¹æ ·æœ¬$x$çš„ä¸ç¡®å®šæ€§æœ‰å¤šå¤§, è€Œä¸”è¯´æ˜äº†æ˜¯å“ªä¸ªå‚æ•°é€ æˆäº†è¿™ç§ä¸ç¡®å®šæ€§. $\\mathcal{I}{\\mathcal{U}}(\\theta)$è¡¨æ˜äº†åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šçš„ä¸ç¡®å®šæ€§.\n  Estimated Error Reduction Density-Weight Methods ä¸‰. ä¸»åŠ¨å­¦ä¹ åˆ†æ ç†è®ºåˆ†æ å››. Problem Setting Varients (å˜ä½“) ç»“æ„åŒ–è¾“å‡ºçš„ä¸»åŠ¨å­¦ä¹   åºåˆ—åŒ–æ¨¡å‹ (CRF æˆ– HMM) äº§ç”Ÿçš„è¾“å‡º. æ ‘çŠ¶è¾“å‡º.  æ‰¹å¤„ç†æ¨¡å¼ä¸‹çš„ä¸»åŠ¨å­¦ä¹   é€‚åˆå¹¶è¡Œå¤„ç†. æ¯æ¬¡é€‰å–æœ€ä¼˜çš„Nä¸ªæ ·æœ¬ä¸ä¸€å®šèƒ½è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœ, å› ä¸ºæ²¡æœ‰è€ƒè™‘è¿™äº›æ ·æœ¬é—´ä¿¡æ¯çš„é‡åˆåº¦.  ä¸»åŠ¨å­¦ä¹ çš„ä»£ä»· å¤šç§è®¿é—®ç±»å‹ ","permalink":"https://michelia-zhx.github.io/posts/2021-08-31-active_learning/","summary":"Active Learning ä¹Ÿç§°ä¸ºæŸ¥è¯¢å­¦ä¹ æˆ–è€…æœ€ä¼˜å®éªŒè®¾è®¡. ä¸»åŠ¨å­¦ä¹ é€šè¿‡è®¾è®¡åˆç†çš„æŸ¥è¯¢å‡½æ•°, ä¸æ–­ä»æœªæ ‡æ³¨çš„æ•°æ®ä¸­æŒ‘é€‰å‡ºæ•°æ®æ ‡æ³¨åæ”¾å…¥è®­ç»ƒé›†. æœ‰æ•ˆçš„ä¸»åŠ¨å­¦ä¹ æ•°æ®é€‰æ‹©ç­–","title":"Paper Notes - Active Learning"},{"content":" Learning from Multiple Teacher Networks http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf\n  loss:  teachersçš„softmaxè¾“å‡ºå–å¹³å‡å’Œstudentçš„äº¤å‰ç†µ ä¸­é—´å±‚è¡¨ç¤ºçš„ç›¸å¯¹ç›¸å¼‚åº¦(ä»…é€‚ç”¨äºMTKD), ä¸‰å…ƒç»„$(q_i,q_i^+,q_i^-)$, å…¶ä¸­$q$æ˜¯ä¸­é—´å±‚çš„è¾“å‡º, ååºå…³ç³»$q_i^+ \u0026gt; q_i^-$ç”±ä¸¤è€…å’Œ$q_i$çš„è·ç¦»$d$å†³å®š, å‚æ•°$w_s$å†³å®šé€‰å–å“ªäº›å±‚. åœ¨ä¸åŒteacherä¸­, ä¸­é—´å±‚è¾“å‡ºçš„ååºå…³ç³»å¯èƒ½ä¸åŒ, å› æ­¤ç”¨æŠ•ç¥¨æ³•å†³å®šä¸­é—´å±‚è¾“å‡ºåº”å½“çš„ååºå…³ç³», è®¾è®¡å’Œstudentå¯¹åº”å±‚è¾“å‡ºçš„loss, ä»¥æ­¤é¼“åŠ±studentæ‹¥æœ‰å’Œteacherä¸­é—´å±‚ç±»ä¼¼çš„ç›¸å¯¹ç›¸ä¼¼(ç›¸å¼‚)å…³ç³». studentå’Œgroudtruthçš„äº¤å‰ç†µ   å®éªŒè®¾ç½®: åŸºäºCIFAR-10, CIFAR-100, MNIST, SVHNçš„å®éªŒ  CIFAR-10, æ¯”è¾ƒstudentä¸åŒå±‚æ•°å’Œå‚æ•°é‡(11/250K, 11/862K, 13/1.6M, 19/2.5M)æ—¶çš„è¡¨ç°(compression rate, acceleration rate and classification accuracy)(å’ŒFitnetsæ¯”è¾ƒ) CIFAR-10, studentå‡ä¸º11å±‚, æ¯”è¾ƒå½“studentçš„å‚æ•°ä¸º250Kå’Œ862Kæ—¶, teacheræ•°é‡ä¸º1, 3, 5æ—¶, Teacher, RDL, FitNets, KDå’Œä»–ä»¬çš„å‡†ç¡®ç‡ CIFAR-10, CIFAR-100, æ¯”è¾ƒä¸åŒæ–¹æ³•(Teacher(5å±‚), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networkså’Œæ­¤æ–¹æ³•(19å±‚))åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ MNIST, æ¯”è¾ƒä¸åŒæ–¹æ³•(Teacher(4å±‚), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networkså’Œæ­¤æ–¹æ³•(7å±‚))çš„å‡†ç¡®ç‡ SVHN, æ¯”è¾ƒä¸åŒæ–¹æ³•(Teacher(5å±‚), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networkså’Œæ­¤æ–¹æ³•(19å±‚))çš„å‡†ç¡®ç‡   @inproceedings{you2017learning, author={You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng}, booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, title={Learning from multiple teacher networks}, pages={1285\u0026ndash;1294}, year={2017} }   Semi-Supervised Knowledge Transfer for Deep Learning from Private Training Data. ICLR 2017 https://openreview.net/pdf?id=HkwoSDPgg\n  å­¦ä¹ ç®—æ³•åº”å½“ä¿æŠ¤ç”¨æˆ·ç§äººæ•°æ®, ä½†æ¨¡å‹ä¼šè®°ä½æ•°æ®, ä¹Ÿä¼šå—åˆ°æ”»å‡»(black/white box attack). å°†æ ·æœ¬åˆ†æˆnä»½, åˆ†åˆ«è®­ç»ƒteacher model, å°†è¿™äº›teachersèšåˆ:  å¦‚æœå¤§å¤šæ•°teacheræœ‰ç›¸åŒçš„è¾“å‡º, åˆ™è¾“å‡ºä¸ä¾èµ–äºåˆ†åˆ«è®­ç»ƒteacherçš„ä¸ç›¸äº¤é›† å¦‚æœæœ‰æŸä¸¤ç±»ç¥¨æ•°ç›¸è¿‘, åˆ™åˆ†æ­§å¯èƒ½ä¼šæ³„éœ²ç§äººä¿¡æ¯(æˆ‘ä¸ç†è§£) åœ¨æŠ•ç¥¨ä¸­å¼•å…¥éšæœºå™ªå£°   studentæ˜¯åŠç›‘ç£, ä¸€éƒ¨åˆ†æ˜¯åˆ©ç”¨private dataé€šè¿‡teacherå¾—åˆ°çš„label, ä¹‹åä½¿ç”¨æ— æ ‡è®°çš„public data. åˆ©ç”¨GANè®­ç»ƒstudent, discriminatorå¢åŠ 1ç±»(m+ç”±ç”Ÿæˆå™¨ç”Ÿæˆ), è®­ç»ƒååªä½¿ç”¨discriminator. å®éªŒè®¾ç½®:    Dataset Teacher Student Student Public Data testing Data     MNIST 2 conv + 1 relu GANS(6 fc layers) test[:1000] test[1000:]   SVHN 2 conv + 2 relu GANS(7 conv + 2 NIN) test[:1000] test[1000:]   UCI Adult RF(100 trees) RF(100 trees) test[:500] test[500:]   UCI Diabetes RF(100 trees) RF(100 trees) test[:500] test[500:]     @article{Papernot2017SemisupervisedKT, title={Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data}, author={Nicolas Papernot and Mart{'i}n Abadi and {'U}lfar Erlingsson and Ian J. Goodfellow and Kunal Talwar}, journal={arXiv preprint}, pages = {}, year={2016} }   Knowledge Adaptation: Teaching to Adapt. ICLR, 2017 https://openreview.net/pdf?id=rJRhzzKxl\n  teachersçš„domainå’Œstudent $\\mathcal{D}{T_i}, \\mathcal{D}S$ä¸å®Œå…¨ä¸€è‡´, å› æ­¤studentå¯¹teacherçš„ä¿¡ä»»åº¦å–å†³äºä¸¤è€…è¡¨ç¤ºç©ºé—´çš„ç›¸ä¼¼åº¦ $\\mathcal{L} = \\mathcal{H}(\\sum{i=1}sim(\\mathcal{D}{T_i}, \\mathcal{D}S)\\cdot D{T_i}, D_S)$ å®šä¹‰MCD, MCDè¶Šå¤§è¡¨ç¤ºè¶Šè¿œç¦»åˆ†ç±»è¾¹ç•Œ, è¾“å…¥teacherå¾—åˆ°çš„ç»“æœç½®ä¿¡åº¦è¶Šé«˜. å› æ­¤é€‰å–MCDæœ€å¤§(å³ç½®ä¿¡åº¦æœ€é«˜)çš„nä¸ªæ ·æœ¬, åœ¨teacherä¸­å¾—åˆ°çš„ç»“æœä½œä¸ºä¼ªæ ‡è®°ä»¥è®­ç»ƒstudentå¯ä»¥ä½¿æ— ç›‘ç£å­¦ä¹ çš„æ€§èƒ½å¾—åˆ°æå‡. å®éªŒè®¾ç½®: åŸºäºAmazon product reviews sentiment analysis dataset. åŒ…å«Book, DVD, Electronics, Kitchenå››ç±».  æ¯”è¾ƒteacher, ç”±ä»¥ç›¸åŒç±»åˆ«æ ·æœ¬è®­ç»ƒçš„teacherè®­ç»ƒå‡ºæ¥çš„student, ç”±ä»¥æ‰€æœ‰æ ·æœ¬è®­ç»ƒçš„teacherè®­ç»ƒå‡ºæ¥çš„student, ç»“åˆä»¥ä¸Šä¸¤ç§teacherè®­ç»ƒå‡ºæ¥çš„student, ä»¥åŠè®¸å¤šå…¶ä»–æ¨¡å‹(SCL, SFA, SCL-com, SFA-com, SST, IDDIWP, DWHC, DAM, CP-MDA, SDAMS-SVM, SDAMS-Log)åœ¨å››ç§ç±»åˆ«ä¸Šçš„æ€§èƒ½. æ¯”è¾ƒåˆ†åˆ«ä»¥å…¶ä¸­ä¸‰ç±»ä¸ºdomainçš„teacherè®­ç»ƒä»¥ç¬¬å››ç±»ä¸ºdomainçš„student(B$\\rightarrow$D,E$\\rightarrow$D,K$\\rightarrow$Dä¾æ¬¡è½®æ¢)åœ¨ä¸åŒæ–¹æ³•ä¸‹çš„æ€§èƒ½   @article{ruder2017knowledge, title={Knowledge adaptation: Teaching to adapt}, author={Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G}, journal={arXiv preprint arXiv:1702.02052}, pages = {}, year={2017} }   Deep Model Compression: Distilling Knowledge from Noisy Teachers. Sau, Bharat Bhusan et al. arXiv:1610.09650 https://arxiv.org/pdf/1610.09650.pdf\n  åœ¨teacherçš„è¾“å‡º(studentçš„ç›®æ ‡)ä¸ŠåŠ æ‰°åŠ¨, ç­‰ä»·äºåŸºäºå™ªå£°çš„æ­£åˆ™åŒ–. å¯ç”¨äºæ¨¡å‹å‹ç¼©. å®éªŒè®¾ç½®: åŸºäºMNIST, SVHN, CIFAR-10.  MNIST: teacher \u0026ndash; a modified network of LeNet([C5(S1P0)@20-MP2(S2)]- [C5(S1P0)@50-MP2(S2)]- FC500- FC10); student \u0026ndash; FC800-FC800-FC10 SVHN: Network-in-Network([C5(S1P2)@192]- [C1(S1P0)@160]- [C1(S1P0)@96-MP3(S2)]- D0.5- [C5(S1P2)@192]- [C1(S1P0)@192]- [C1(S1P0)@192- AP3(S2)]- D0.5- [C3(S1P1)@192]- [C1(S1P0)@192]- [C1(S1P0)@10]- AP8(S1)); student: LeNet([C5(S1P2)@32-MP3(S2)]- [C5(S1P2)@64-MP3(S2)]- FC1024-FC10) CIFAR-10: teacher: same as SVHN; student: a modified version of the LeNet([C5(S1P2)@64-MP2(S2)]- [C5(S1P2)@128- MP2(S2)]-FC1024-FC10).   @article{sau2016deep, title={Deep model compression: Distilling knowledge from noisy teachers}, author={Sau, Bharat Bhusan and Balasubramanian, Vineeth N}, journal={CoRR}, pages = {}, year={2016} }   Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Tarvainen, Antti and Valpola, Harri. NeurIPS 2017 https://papers.nips.cc/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf\n  ç®—æ³•:  æ„å»ºä¸€ä¸ªæ™®é€šçš„ç›‘ç£æ¨¡å‹; copyä¸€ä»½ç›‘ç£å­¦ä¹ æ¨¡å‹, åŸæ¨¡å‹å«åšstudent, æ–°çš„å«teacher; æ¯æ­¥ä½¿ç”¨åŒæ ·çš„minibatchè¾“å…¥åˆ°studentä¸teacheræ¨¡å‹ä¸­, ä½†åœ¨è¾“å…¥æ•°æ®å‰åˆ†åˆ«åŠ å…¥éšæœºå¢å¼ºæˆ–è€…å™ªéŸ³; åŠ å…¥studentä¸teacherè¾“å‡ºçš„ä¸€è‡´æ€§æŸå¤±å‡½æ•°; ä¼˜åŒ–å™¨åªæ›´æ–°studentçš„æƒé‡; æ¯æ­¥ä¹‹å, é‡‡ç”¨studentæƒé‡çš„EMAæ›´æ–°teacheræƒé‡;   æ ¸å¿ƒæ€æƒ³æ˜¯: æ¨¡å‹æ—¢å……å½“å­¦ç”Ÿ, åˆå……å½“è€å¸ˆ. ä½œä¸ºè€å¸ˆ, ç”¨æ¥äº§ç”Ÿå­¦ç”Ÿå­¦ä¹ æ—¶çš„ç›®æ ‡; ä½œä¸ºå­¦ç”Ÿ, åˆ™åˆ©ç”¨æ•™å¸ˆæ¨¡å‹äº§ç”Ÿçš„ç›®æ ‡æ¥è¿›è¡Œå­¦ä¹ . è€Œæ•™å¸ˆæ¨¡å‹çš„å‚æ•°æ˜¯ç”±å†å²ä¸Š(å‰å‡ ä¸ªstep)å‡ ä¸ªå­¦ç”Ÿæ¨¡å‹çš„å‚æ•°ç»è¿‡åŠ æƒå¹³å‡å¾—åˆ°. å¯ä»¥çœ‹æˆæ˜¯ĞŸ-modelä¸­çš„ä¸¤æ¬¡è®¡ç®—ä¸­æ¨¡å‹æ¢æˆäº†ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹, ä¸€ä¸ªå«teacher, ä¸€ä¸ªå«student; å¦å¤–, ä¹Ÿå¯ä»¥çœ‹æˆä½œTemporal ensemblingçš„æ”¹è¿›ç‰ˆ, åœ¨Temporal ensemblingä¸­, é‡‡ç”¨çš„æ˜¯æ¯epochçš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼æ¥èšåˆå†å²æ•°å†…å®¹, è€ŒMean teacheråˆ™æ˜¯åœ¨æ¯è®­ç»ƒæ­¥è¿›è¡Œå¯¹Studentçš„æƒé‡è¿›æŒ‡æ•°ç§»åŠ¨å¹³å‡. å®éªŒè®¾ç½®: åŸºäºSVHNå’ŒCIFAR-10  All the methods in the comparison use a similar 13-layer ConvNet architecture.   @inproceedings{10.5555/3294771.3294885, title = {Mean Teachers Are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results}, author = {Tarvainen, Antti and Valpola, Harri}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {1195â€“1204}, year = {2017} }   Born-Again Neural Networks. Furlanello, Tommaso et al. ICML 2018 https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf\n  åŸºäºæ–°æ¨¡å‹çš„è¾“å…¥å’ŒåŸæ¨¡å‹çš„è¾“å…¥é—´çš„äº¤å‰ç†µ, ä½¿ç”¨KDé¡¹ä¿®æ”¹æ›¿ä»£å’Œæ­£åˆ™åŒ–åŸæ¥çš„loss Selves Born-Again Networksé›†æˆçš„å­¦ä¹ é¡ºåº: $\\mathcal{L}(f(x, \\arg\\min_{\\theta_{k-1}}\\mathcal{L}(f(x, \\theta_{k-1}))),f(x,\\theta_k))$, å°†ä¸Šä¸€ä¸ªstudentå­¦åˆ°çš„çŸ¥è¯†ä½œä¸ºç›‘ç£ä¿¡æ¯, æ•™å¯¼ä¸‹ä¸€ä¸ªå­¦ç”Ÿ. å®éªŒè®¾ç½®:  CIFAR-10: Wide-ResNet with different depth and width (28-1, 28-2, 28-5, 28-10) and DenseNet of different depth and growth factor (112-33, 90-60, 80-80, 80-120) CIFAR-100: ä¸ä¸ŠåŒ.   @inproceedings{Furlanello2018BornAN, title={Born Again Neural Networks}, author={Tommaso Furlanello and ZaKnowledge Adaptation: Teaching to Adaptchary Chase Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar}, booktitle={ICML}, year={2018} }   Deep Mutual Learning. Zhang, Ying et al. CVPR 2018 https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf\n  ä¸¤ä¸ª(å¤šä¸ª?)studentsç›¸äº’å­¦ä¹ , å¯¹äºæ¯ä¸ªstudent, æŸå¤±ä¸ºå’Œgroundtruthçš„äº¤å‰ç†µä»¥åŠç›¸å¯¹äºå¦ä¸€ä¸ªstudentçš„softmaxè¾“å‡ºçš„KLæ•£åº¦: $\\mathcal{L}{\\theta_1} = \\mathcal{L}{C_1} + D_{KL}(p_2|p_1), \\theta_1\\leftarrow\\theta_1+\\gamma_t\\dfrac{\\partial\\mathcal{\\theta_1}}{\\partial\\theta_1}$; $\\mathcal{L}{\\theta_2} = \\mathcal{L}{C_2} + D_{KL}(p_1|p_2), \\theta_2\\leftarrow\\theta_2+\\gamma_t\\dfrac{\\partial\\mathcal{\\theta_2}}{\\partial\\theta_2}$ ä¼˜ç‚¹:  éšç€å­¦ç”Ÿç½‘ç»œçš„å¢åŠ å…¶æ•ˆç‡ä¹Ÿå¾—åˆ°æé«˜ å®ƒå¯ä»¥åº”ç”¨åœ¨å„ç§å„æ ·çš„ç½‘ç»œä¸­, åŒ…æ‹¬å¤§å°ä¸åŒçš„ç½‘ç»œ å³ä½¿æ˜¯éå¸¸å¤§çš„ç½‘ç»œé‡‡ç”¨ç›¸äº’å­¦ä¹ ç­–ç•¥, å…¶æ€§èƒ½ä¹Ÿèƒ½å¤Ÿå¾—åˆ°æå‡   å®éªŒè®¾ç½®:  æ•°æ®é›†: ImageNet, CIFAR-10, CIFAR-100, Market-1501 Networks:    ResNet-32 MobileNet InceptionV1 WRN-28-10     0.5M 3.3M 7.8M 36.5M       @inproceedings{8578552, title = {Deep Mutual Learning}, author = {Y. Zhang and T. Xiang and T. M. Hospedales and H. Lu}, booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages = {4320-4328}, year = {2018} }   Data Distillation: Towards Omni-Supervised Learning. Radosavovic, Ilija et al. CVPR 2018 https://openaccess.thecvf.com/content_cvpr_2018/papers/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.pdf\n  Model Distillation vs. Data Distillation: å‰è€…ensembleåŒä¸€æ ·æœ¬åœ¨ä¸åŒæ¨¡å‹çš„è¾“å‡º, åè€…ensembleåŒä¸€æ ·æœ¬ç»ä¸åŒè½¬æ¢ååœ¨åŒä¸€æ¨¡å‹çš„è¾“å‡º. æ–¹æ³•:  ç”¨æ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®è®­ç»ƒæ¨¡å‹A ç”¨æ¨¡å‹Aå»è®­ç»ƒæ•°æ®å¢å¹¿ (æœ¬æ–‡ä¸­ä¸º scaling and horizontal flipping) çš„æœªæ ‡æ³¨æ•°æ® å°†æœªæ ‡æ³¨æ•°æ®çš„é¢„æµ‹ç»“æœé€šè¿‡ ensembling å¤šä¸ªé¢„æµ‹ç»“æœ, è½¬åŒ–ä¸º labels åœ¨æ‰‹åŠ¨æ ‡æ³¨å’Œè‡ªåŠ¨æ ‡æ³¨çš„æ•°æ®é›†é‡æ–°è®­ç»ƒæ¨¡å‹   å®éªŒ: åœ¨COCO Keypoint Detection, Object Detection éªŒè¯. teacherå’Œstudentéƒ½æ˜¯Mask R-CNN keypoint detection variant @inproceedings{inproceedings, title = {Data Distillation: Towards Omni-Supervised Learning}, author = {Radosavovic, Ilija and Dollar, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming}, year = {2018}, doi = {10.1109/CVPR.2018.00433} pages = {4119-4128} }   Multilingual Neural Machine Translation with Knowledge Distillation. ICLR 2019 https://openreview.net/pdf?id=S1gUsoR9YX\n  æ–¹æ³•å¾ˆç®€å•, å°±æ˜¯å…ˆé’ˆå¯¹æ¯å¯¹è¯­è¨€è®­ç»ƒå•ç‹¬çš„ç¿»è¯‘æ¨¡å‹ä½œä¸ºteacher, å†ç”¨multi-teacher KDè®­ç»ƒstudent, losså°±æ˜¯studentå’Œlabelçš„äº¤å‰ç†µä»¥åŠå’Œteacherçš„softmaxè¾“å‡ºçš„äº¤å‰ç†µ. å®éªŒè®¾ç½®: æ•°æ®é›†: IWSLT, WMT, Ted Talk; studentå’Œteacherå‡ä½¿ç”¨Transformer    Task model hidden size $d_{model}$ feed-forward hidden size $d_{ff}$ number of layer     IWSLT and Ted talk tasks 256 1024 2   WMT task 512 2048 6     @article{Tan2019MultilingualNM, title={Multilingual Neural Machine Translation with Knowledge Distillation}, author={Xu Tan and Yi Ren and Di He and Tao Qin and Zhou Zhao and Tie-Yan Liu}, journal={ICLR}, year={2019}, volume={abs/1902.10461} }   Unifying Heterogeneous Classifiers with Distillation. Vongkulbhisal et al. CVPR 2019 https://openaccess.thecvf.com/content_CVPR_2019/papers/Vongkulbhisal_Unifying_Heterogeneous_Classifiers_With_Distillation_CVPR_2019_paper.pdf\n  Nä¸ªä¸åŒçš„æ¨¡å‹$\\mathcal{C} = {C_i}_{i=1}^N$å…·æœ‰ä¸åŒçš„ç»“æ„å’Œç›®æ ‡ç±»åˆ«, $C_i$è¢«è®­ç»ƒä»¥åˆ†åˆ«é¢„æµ‹$p_i(Y=l_j)$, å¹¶æ•´åˆå‡ºæ ·æœ¬åœ¨æ‰€æœ‰ç±»ä¸­çš„æ¦‚ç‡$q(Y=i_j)$. æœ€ååˆ©ç”¨$q$è®­ç»ƒstudent. ä½œè€…æå‡ºäº†åŸºäºäº¤å‰ç†µæœ€å°åŒ–å’ŒçŸ©é˜µåˆ†è§£çš„æ–¹æ³•ï¼Œä»æœªæ ‡è®°çš„æ ·æœ¬ä¸­ä¼°è®¡æ‰€æœ‰ç±»åˆ«çš„soft-labels. å®éªŒè®¾ç½®:  æ•°æ®é›†: ImageNet, LSUN, Places365 $C_i$ä»AlexNet, VGG16, ResNet18, ResNet34ä¸­éšæœºé€‰æ‹©   @article{Vongkulbhisal2019UnifyingHC, title={Unifying Heterogeneous Classifiers With Distillation}, author={Jayakorn Vongkulbhisal and Phongtharin Vinayavekhin and Marco Visentini Scarzanella}, journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2019}, pages={3170-3179} }   Distilled Person Re-Identification: Towards a More Scalable System. Wu, Ancong et al. CVPR 2019 https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf\n  è§£å†³ä¸‰ä¸ªé—®é¢˜: é™ä½æ ‡ç­¾æˆæœ¬(å‡å°‘æ ‡ç­¾çš„éœ€æ±‚é‡); é™ä½è·¨æ•°æ®åº“æˆæœ¬(åˆ©ç”¨ä¸€äº›å…ˆéªŒçŸ¥è¯†); é™ä½æµ‹è¯•æˆæœ¬(ä½¿ç”¨è½»é‡çº§ç½‘ç»œ) å‡è®¾taregt domainåŒ…å«10ä¸ªç±»çš„å›¾ç‰‡, å…ˆç”¨å¤šä¸ªä¸ªsource domainåˆ†åˆ«è®­ç»ƒå¤šä¸ªteacher model, source domainä¹‹åå¹¶ä¸ä¼šè¢«ç”¨åˆ°(åˆ©ç”¨ä¸€äº›å…ˆéªŒçŸ¥è¯†\u0026ndash;é™ä½è·¨æ•°æ®åº“æˆæœ¬); target domainå¯ä»¥åªåŒ…å«10ä¸ªlabelled sample(10ç±»å‡æœ‰), å…¶ä½™å‡ä¸ºunlabeled sample, å¯¹äºNä¸ªunlabelled input, å®šä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ$A$, å…¶ä¸­ç¬¬iè¡Œç¬¬jåˆ—è¡¨ç¤ºç¬¬iä¸ªå›¾åƒå’Œç¬¬jä¸ªå›¾åƒåœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸‹è¾“å‡ºçš„ç›¸ä¼¼åº¦. ä¸ºäº†å°†çŸ¥è¯†ä»teacherè¿ç§»åˆ°student, éœ€è¦æœ€å°åŒ–teacherçš„ç›¸ä¼¼åº¦çŸ©é˜µ$A_T$å’Œstudentçš„ç›¸ä¼¼åº¦çŸ©é˜µ$A_S$çš„è·ç¦»(è¿™å¥è¯æ˜¯å­¦ä¹ single teacher). åˆ†åˆ«åˆ©ç”¨teacherè®¡ç®—target domainä¸­æ¯ä¸€ä¸ªxçš„ç‰¹å¾å‘é‡, å¹¶åˆ†åˆ«è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ$A$, ä½¿ç”¨$L_{ver}$æ›´æ–°æ¯ä¸€ä¸ªè€å¸ˆæ¨¡å‹çš„æƒé‡$a$(å¯ä»¥ç†è§£ä¸ºï¼Œæƒé‡è¶Šå¤§ï¼Œè¯¥è€å¸ˆæ¨¡å‹å¯¹åº”çš„sourceå’Œtargetè¶Šç›¸ä¼¼) è®¡ç®—å‡ºæ¯ä¸€ä¸ªè€å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹å¾—åˆ°çš„ç›¸ä¼¼çŸ©é˜µçš„å·®å¼‚ï¼Œå¹¶ä½¿ç”¨ä¸Šè¿°çš„æƒé‡åŠ æƒï¼Œä»è€Œå¾—åˆ°$L_{ta}$. ä½¿ç”¨$L_{ta}$å¯¹å­¦ç”Ÿæ¨¡å‹è¿›è¡Œæ›´æ–°, å¾ªç¯è®­ç»ƒ. å®éªŒè®¾ç½®: æ•°æ®é›† \u0026ndash; Market-1501, DukeMTMC. åˆ†åˆ«ä½¿ç”¨MSMT17, CUHK03, ViPER, DukeMTMC, Market-1501è®­ç»ƒ5ä¸ªteacher model$T_1, T_2, T_3, T_4, T_5$; teacher \u0026ndash; an advanced Re-ID model PCB, student \u0026ndash; a lightweight mod- el MobileNetV2. @InProceedings{Wu_2019_CVPR, author = {Wu, Ancong and Zheng, Wei-Shi and Guo, Xiaowei and Lai, Jian-Huang}, title = {Distilled Person Re-Identification: Towards a More Scalable System}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2019} }   Diversity with Cooperation: Ensemble Methods for Few-Shot Classification. Dvornik, Nikita et al. ICCV 2019 https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf\n  meta learning \u0026ndash; learning to learn æ¨¡å‹é—´çš„å…³ç³»æœ‰ä¸‰ç§ - åˆä½œ(é¢„æµ‹çš„ç»“æœæ— è®ºæ˜¯å±äºæ­£ç¡®ç»“æœçš„æ¦‚ç‡è¿˜æ˜¯å±äºé”™è¯¯ç»“æœçš„æ¦‚ç‡éƒ½æ˜¯æ¯”è¾ƒä¸€è‡´çš„), ç‹¬ç«‹(ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹çš„ç»“æœä¹‹é—´ä¸å­˜åœ¨æ˜æ˜¾çš„å…³ç³»), å¤šæ ·æ€§(é™¤äº†æ­£ç¡®ç»“æœï¼Œé¢„æµ‹ä¸ºå…¶ä»–ç»“æœçš„æ¦‚ç‡å·®å¼‚æ˜æ˜¾). æ–‡ç« é€šè¿‡å‡ºäº†äº¤å‰ç†µæŸå¤±å¤–è®¾è®¡ä¸åŒçš„æŸå¤±å‡½æ•°$\\psi(y_i,f_{\\theta_j}(x_i),f_{\\theta_l}(x_i))$è¯±å¯¼æ¨¡å‹çš„å…³ç³»å‘ä¸åŒæ–¹å‘å‘å±•, ä¾‹å¦‚åŸºäºcosæˆ–KLæ•£åº¦. å®éªŒè®¾ç½®:  æ•°æ®é›†: mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds (CUB) 2002011. ensemble of ResNet18 and WideResNet28   @INPROCEEDINGS{9010380, title={Diversity With Cooperation: Ensemble Methods for Few-Shot Classification},author={Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia}, booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, pages={3722-3730},\nyear={2019} }   Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System. Yang, Ze et al. WSDM 2020 https://arxiv.org/pdf/1910.08381.pdf\n  ä¸€ç§ç”¨äºç½‘ç»œé—®ç­”ç³»ç»Ÿçš„ä¸¤é˜¶æ®µå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦(ç®€ç§° TMKD)æ–¹æ³•, é¦–å…ˆç”¨ä¸€ä¸ªé€šç”¨çš„é—®ç­”æç‚¼ä»»åŠ¡å¯¹studentè¿›è¡Œé¢„è®­ç»ƒ(è²Œä¼¼ä¹Ÿæ˜¯ä½¿ç”¨Multi-teacher), å¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡(å¦‚ Web Q\u0026amp;A ä»»åŠ¡, MNLI, SNLI, æ¥è‡ª GLUE çš„ RTE ä»»åŠ¡)ä¸Šä½¿ç”¨Multi-Teacher KDè¿›ä¸€æ­¥å¾®è°ƒè¿™ä¸ªé¢„è®­ç»ƒçš„student. â€œearly calibrationâ€ effectç¼“è§£äº†å•ä¸ªteacheré€ æˆçš„è¿‡æ‹Ÿåˆåå·®. å®éªŒè®¾ç½®:  æ•°æ®é›† - DeepQA, CommQA-Unlabeled, CommQA-Labeled, MNLI, SNLI, QNLI, RTE. Baseline: teacher - BERT-3, BERT_{large}, BERT_{large}Ensemble; student(Traditional Distillation Model) - Bi-LSTM(1-o-1, 1_{avg}-o-1, m-o-m), BERT3(1-o-1, 1_{avg}-o-1, m-o-m), student(TMKD) - Bi-LSTM(TMKD), TMKD_{base}, TMKD_{large}(åä¸¤è€…éƒ½æ˜¯BERT-3 models).   @inproceedings{inproceedings, author = {Ze, Yang and Shou, Linjun and Gong, Ming and Lin, Wutao and Jiang, Daxin}, title = {Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System}, publisher = {Association for Computing Machinery}, doi = {10.1145/3336191.3371792}ï¼Œ pages = {690-698}, year = {2020} }   FEED: Feature-level Ensemble for Knowledge Distillation. Park, SeongUk and Kwak, Nojun. AAAI 2020 https://openreview.net/pdf?id=BJxYEsAqY7\n  å®éªŒæ–¹æ³•å°±æ˜¯è®©studentç›´æ¥å­¦teacherçš„feature. å®éªŒè®¾ç½®: æ•°æ®é›†: CIFAR-100; é€‰å–æ¨¡å‹: student \u0026ndash; ResNet-56, ResNet-110, WRN28-10, ResNext29-16x64d; æ²¡è¯´teacheræ˜¯è°. @article{Park2019FEEDFE, title={FEED: Feature-level Ensemble for Knowledge Distillation}, author={Seonguk Park and Nojun Kwak}, journal={ECAI}, year={2019}, volume={abs/1909.10754} }   Stochasticity and Skip Connection Improve Knowledge Transfer. Lee, Kwangjin et al. ICLR 2020 https://openreview.net/pdf?id=HklA93NYwS\n  åˆ©ç”¨å•ä¸ªæ•™å¸ˆç½‘ç»œç”Ÿæˆå¤šä¸ªæ•™å¸ˆç½‘ç»œ(åŠ å…¥stochastic blockså’Œskip connections)å¹¶è®­ç»ƒå­¦ç”Ÿç½‘ç»œ, åˆ†å—å¹¶å«æœ‰skip connectionsçš„ç½‘ç»œå¯ä»¥çœ‹æˆæ ‘çŠ¶ç½‘ç»œ, ä»inputåˆ°outputæœ‰å¤šæ¡è·¯å¾„. å®éªŒè®¾ç½®: æ•°æ®é›† \u0026ndash; CIFAR-100 å’Œ tiny imagenet, å¹¶å°†è¿™ç§æ–¹æ³•åº”ç”¨åˆ°KD, AT(attention tranfer), ML. å®éªŒä¸­æ¶‰åŠåˆ°çš„teacheræœ‰ResNet 32, ResNet 110, WRN 28-10, MobileNet, WRN 40-4; æ¶‰åŠåˆ°çš„studentæœ‰VGG 13, ResNet 20, ResNet 32, WRN 40-4. @INPROCEEDINGS{9287227, author={Nguyen, Luong Trung and Lee, Kwangjin and Shim, Byonghyo}, title={Stochasticity and Skip Connection Improve Knowledge Transfer}, booktitle={2020 28th European Signal Processing Conference (EUSIPCO)}, pages={1537-1541}, year={2021} }   Hydra: Preserving Ensemble Diversity for Model Distillation. Tran, Linh et al. arXiv:2001.04694 http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-026.pdf\n  æ™®é€šmulti-teacher KDå¯¹teacherçš„é¢„æµ‹å€¼å–å¹³å‡, è¿™æ ·ä¼šä¸§å¤±å¤šteacherç»“æœåŒ…å«çš„ä¸ç¡®å®šæ€§ä¿¡æ¯(?), æœ¬æ–‡å°†studentæ‹†åˆ†æˆbodyå’Œå¤šä¸ªhead, æ¯ä¸ªheadå¯¹åº”ä¸€ä¸ªteacher, ä»¥ä¿ç•™å¤šteacherè¾“å‡ºçš„å¤šæ ·æ€§ å‡è®¾æœ‰Mä¸ªteacher, é¦–å…ˆè®­ç»ƒä¸€ä¸ªheadç›´è‡³å…¶æ”¶æ•›è‡³teacherçš„å¹³å‡, å†æ·»åŠ å…¶ä»–M-1ä¸ªhead, Mä¸ªheadä¸€èµ·è®­ç»ƒ, å®éªŒè¯æ˜å¦‚æœæ²¡æœ‰ç¬¬ä¸€ä¸ªheadä¼šå¾ˆéš¾æ”¶æ•›. ä½œè€…å®šä¹‰äº†ä¸€ä¸ªæ¨¡å‹ä¸ç¡®å®šæ€§, ç”±æ•°æ®ä¸ç¡®å®šæ€§å’Œæ€»ä¸ç¡®å®šæ€§ç»„æˆ(æˆ‘ä¸ç†è§£ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªé¡ºåº). å®éªŒè®¾ç½®:  æ•°æ®é›†: a spiral toy dataset(ç”¨äºå¯è§†åŒ–å¹¶è§£é‡Šæ¨¡å‹ä¸ç¡®å®šæ€§), MNIST(æµ‹è¯•æ—¶ç”¨äº†å®ƒçš„æµ‹è¯•é›†å’ŒFashion-MNIST), CIFAR-10(æµ‹è¯•æ—¶ç”¨äº†å®ƒçš„æµ‹è¯•é›†, cyclic translated test set, 80 different corrupted test sets å’Œ SVHN). æ¨¡å‹: toy dataset - ä¸¤å±‚MLP, æ¯å±‚100ä¸ªç»“ç‚¹; MNIST - MLP; CIFAR-10 - ResNet-20 V1. åœ¨å›å½’é—®é¢˜ä¸­, æ‰€æœ‰æ•°æ®é›†å‡ä½¿ç”¨MLP.   @article{DBLP:journals/corr/abs-2001-04694, author = {Linh Tran, Bastiaan S. Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V. Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Sebastian Nowozin, Rodolphe Jenatton}, title = {Hydra: Preserving Ensemble Diversity for Model Distillation}, journal = {CoRR}, year = {2020} }   Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition. Gao, Yan et al. arXiv:2005.09310 https://arxiv.org/pdf/2005.09310v1.pdf\n  @article{DBLP:journals/corr/abs-2005-09310, author = {Yan Gao, Titouan Parcollet, Nicholas D. Lane}, title = {Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition}, journal = {CoRR}, year = {2020} }   Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection. Chen, Cong et al. IEEE 2020 [code]\n  Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation. MICCAI 2020\n  Knowledge Distillation for Multi-task Learning. Li, WeiHong \u0026amp; Bilen, Hakan. arXiv:2007.06889 [project]\n  Adaptive Multi-Teacher Multi-level Knowledge Distillation. Liu, Yuang et al. Neurocomputing 2020 [code] https://arxiv.org/pdf/2103.04062.pdf\n  loss: $\\mathcal{L} = \\mathcal{L}{KD}+\\alpha\\mathcal{L}{angle}+\\beta\\mathcal{L}_{HT}$  $\\mathcal{L}{KD}$: å¯¹äºæ¯ä¸ªæ ·æœ¬, studentéœ€è¦èµ‹äºˆteachersçš„è¾“å‡ºä¸åŒçš„æƒé‡, studentä¸­fcä¹‹å‰çš„è¡¨ç¤ºç»è¿‡maxpoolingåå’Œæ¯ä¸ªteacherfcå‰çš„è¡¨ç¤ºåˆ†åˆ«åšç‚¹ç§¯ä½œä¸ºæƒé‡, teacherçš„åŠ æƒå’Œä½œä¸ºweighted target, å°†weighted targetä¸studentçš„soft-targeté—´çš„KLæ•£åº¦å’Œstudentè¾“å‡ºä¸groungtruthçš„äº¤å‰ç†µä½œä¸º$\\mathcal{L}{KD}$. $\\mathcal{L}{angle}$: å¯¹äºæ ·æœ¬ç»„æˆçš„ä¸‰å…ƒç»„, è®¡ç®—å®ƒä»¬çš„teacherå’Œstudentè¡¨ç¤ºçš„ç©ºé—´ç›¸å¯¹ä½ç½®, è®¡ç®—äºŒè€…çš„Huber lossä½œä¸º$\\mathcal{L}{angle}$. $\\mathcal{L}_{HT}$: è®¡ç®—teacherå’Œstudentä¸­é—´å±‚è¡¨ç¤ºçš„å·®çš„äºŒèŒƒå¼, studentçš„ä¸­é—´å±‚éœ€è¦ç»è¿‡ä¸€ä¸ªå•å±‚FitNetä½¿å…¶è§„æ¨¡ç­‰äºteacherçš„ä¸­é—´å±‚è¡¨ç¤º.   å®éªŒè®¾ç½®: æ•°æ®é›†æœ‰CIFAR-10, CIFAR-100å’ŒTiny-ImageNet.  CIFAR-10, CIFAR-100: teacherä½¿ç”¨ResNet110, VGG-19, DenseNet121, studentä¸ºResNet20; æ¯”è¾ƒä¸åŒbaseline (OKD, FitNet, RKD, AvgMKD, DML) åœ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°; æ¯”è¾ƒä¸åŒbaseline(OKD, AvgMKD, DML)åœ¨teacheræ•°é‡ä¸º2,3,5æ—¶çš„è¡¨ç°. Tiny-ImageNet: teacher(ResNet110, ResNet56, ResNet32), student - ResNet20.   @article{LIU2020106, author={Yuang Liu and W. Zhang and Jijie Wang}, title = {Adaptive multi-teacher multi-level knowledge distillation}, journal = {Neurocomputing}, pages = {106-113}, year = {2020} }  ","permalink":"https://michelia-zhx.github.io/posts/2022-02-23-multi_teacher_knowledge_distillation-1/","summary":"Learning from Multiple Teacher Networks http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf loss: teachersçš„softmaxè¾“å‡ºå–å¹³å‡å’Œstudentçš„äº¤å‰ç†µ ä¸­é—´å±‚è¡¨ç¤ºçš„ç›¸å¯¹ç›¸å¼‚åº¦(ä»…é€‚ç”¨äºMTKD), ä¸‰å…ƒç»„$(q_i","title":"Paper Notes - Multi-Teacher Knowledge Distillation - 1"},{"content":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks https://arxiv.org/pdf/2004.05937.pdf\n Learning from Multiple Teacher Networks, KDD 2017 Efficient knowledge distillation from an ensemble of teachers. Interspeech 2017: å¯¹teacherçš„logitså–åŠ æƒå¹³å‡, åŠ æƒå¹³å‡å’Œstudentçš„logitsè®¡ç®—äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°; å¦å¤–æ²¡æœ‰è¯´æƒæ˜¯æ€ä¹ˆåˆ†é…çš„ (æå‰è®¾ç½®å¥½çš„). A Two-Teacher Framework for Knowledge Distillation. ISNN 2019 Feature-Level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks. ECAI 2020 Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation: å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ä¸­å¦‚ä½•æ•´åˆå¤šæ•™å¸ˆçš„çŸ¥è¯†çš„é—®é¢˜ä¾ç„¶æ²¡æœ‰å¾—åˆ°å¾ˆå¥½çš„è§£å†³. ä¸åŒçš„æ•™å¸ˆå…·æœ‰ä¸åŒçš„é‡è¦æ€§, ä¸”æœ‰äº›æ•™å¸ˆä¼šå¯¹å­¦ç”Ÿçš„æ³›åŒ–æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“. æœ¬æ–‡æå‡ºäº†åŸºäºå¤šä»»åŠ¡å­¦ä¹ çš„è‡ªé€‚åº”æ–¹æ³• (æ²¡çœ‹æ‡‚). Ensemble Knowledge Distillation for Learning Improved and Efficient Networks. ECAI 2020 Knowledge Distillation based Ensemble Learning for Neural Machine Translation. ICLR 2021: æœºå™¨ç¿»è¯‘æ–¹å‘çš„æ–‡ç« , ä¸»è¦æå‡ºäº†æ–°çš„æŸå¤±å‡½æ•°. A Simple Ensemble Learning Knowledge Distillation. MLIS 2020: åªæœ‰ä¸€ä¸ªæŸå¤±å‡½æ•° ($\\mathcal{L}{CL}+\\mathcal{L}{KD}$) (???) Stochasticity and Skip Connection Improve Knowledge Transfer. ICLR 2020 Amalgamating Knowledge towards Comprehensive Classification. AAAI 2019 Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification. ECCV 2020: ä¸»è¦ç”¨äºè§£å†³é•¿å°¾é—®é¢˜, æ¯ä¸ªteacherå¯¹åº”å‡ ç±», ç”¨æ ·æœ¬æ•°ç›¸è¿‘çš„å‡ ç±»æ•°æ®å»è®­ç»ƒçš„æ•ˆæœä¼šä¼˜äºä»é•¿å°¾åˆ†å¸ƒçš„æ•°æ®ä¸­å­¦ä¹ . Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning. IJCAI 2019  Semi-Supervised Knowledge Amalgamation for Sequence Classification. AAAI 2021   Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation. CVPR 2019  Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning. ECCV 2020   Highlight Every Step: Knowledge Distillation via Collaborative Teaching Hydra: Preserving Ensemble Diversity for Model Distillation  Diversity Matters When Learning From Ensembles. NIPS 2021   Knowledge flow: Improve upon your teachers. ICLR 2019  Learning from Multiple Teacher Networks. You, Shan et al. KDD 2017   http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf è¿™æ˜¯ç¬¬ä¸€ç¯‡æå‡ºMulti-teacher KDçš„æ–‡ç« \n  é—®é¢˜: å¦‚ä½•è¿›è¡Œå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦? æ–¹æ³•: ä¸»è¦æ”¹è¿›äº†loss function, ç”±ä¸‰éƒ¨åˆ†ç»„æˆ:  teachersçš„softmaxè¾“å‡ºå–å¹³å‡å’Œstudentçš„äº¤å‰ç†µ ä¸­é—´å±‚è¡¨ç¤ºçš„ç›¸å¯¹ç›¸å¼‚åº¦ (ä»…é€‚ç”¨äºMTKD), ä¸‰å…ƒç»„ $(q_i,q_i^+,q_i^-)$, å…¶ä¸­$q_i$æ˜¯æ ·æœ¬$i$åœ¨ä¸­é—´å±‚çš„è¡¨ç¤º, ååºå…³ç³»$q_i^+ \u0026gt; q_i^-$ç”±ä¸¤è€…å’Œ$q_i$çš„è·ç¦»$d$å†³å®š, å‚æ•°$w_s$å†³å®šé€‰å–å“ªå±‚. åœ¨ä¸åŒteacherä¸­, è¾“å…¥çš„ä¸‰ä¸ªæ ·æœ¬çš„ä¸­é—´å±‚è¡¨ç¤ºçš„ååºå…³ç³»å¯èƒ½ä¸åŒ, å› æ­¤ç”¨æŠ•ç¥¨æ³•å†³å®šæ­£ç¡®çš„ååºå…³ç³». è®¾è®¡å’Œstudentå¯¹åº”å±‚è¾“å‡ºçš„loss, ä»¥æ­¤é¼“åŠ±studentçš„ä¸­é—´å±‚çš„è¡¨ç¤ºç©ºé—´æ‹¥æœ‰å’Œteacherè¿‘ä¼¼çš„ç»“æ„. studentå’Œgroudtruthçš„äº¤å‰ç†µ.   å®éªŒè®¾ç½®: åŸºäºCIFAR-10, CIFAR-100, MNIST, SVHNçš„å®éªŒ  CIFAR-10, æ¯”è¾ƒstudentä¸åŒå±‚æ•°å’Œå‚æ•°é‡ (11/250K, 11/862K, 13/1.6M, 19/2.5M) æ—¶çš„è¡¨ç° (compression rate, acceleration rate and classification accuracy) (å’ŒFitnetsæ¯”è¾ƒ) CIFAR-10, studentå‡ä¸º11å±‚, æ¯”è¾ƒå½“studentçš„å‚æ•°ä¸º250Kå’Œ862Kæ—¶, teacheræ•°é‡ä¸º1, 3, 5æ—¶, Teacher, RDL, FitNets, KDå’Œä»–ä»¬çš„å‡†ç¡®ç‡ CIFAR-10, CIFAR-100, æ¯”è¾ƒä¸åŒæ–¹æ³• (Teacher (5å±‚), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networkså’Œæ­¤æ–¹æ³• (19å±‚)) åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ MNIST, æ¯”è¾ƒä¸åŒæ–¹æ³• (Teacher (4å±‚), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networkså’Œæ­¤æ–¹æ³• (7å±‚)) çš„å‡†ç¡®ç‡ SVHN, æ¯”è¾ƒä¸åŒæ–¹æ³• (Teacher (5å±‚), FitNets, KD, Maxout Networks, Network in Network, Deeply-Supervised Networkså’Œæ­¤æ–¹æ³• (19å±‚)) çš„å‡†ç¡®ç‡   @inproceedings{you2017learning, author={You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng}, booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, title={Learning from multiple teacher networks}, pages={1285\u0026ndash;1294}, year={2017} }  A Two-Teacher Framework for Knowledge Distillation. ISNN 2019   https://link.springer.com/chapter/10.1007%2F978-3-030-22796-8_7 (æ‰¾ä¸åˆ°pdf, åªæœ‰ç½‘é¡µç‰ˆ)\n  é—®é¢˜: single-teacher KDä¸è¡Œ. æ–¹æ³•: è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªä»¥ä¸åŒç­–ç•¥è®­ç»ƒçš„æ•™å¸ˆç½‘ç»œç»„æˆ, ä¸€ä¸ªè¢«ä¸¥æ ¼åœ°è®­ç»ƒä»¥æŒ‡å¯¼å­¦ç”Ÿç½‘ç»œå­¦ä¹ å¤æ‚çš„ç‰¹å¾ (lossä¸ºæ¯ä¸€å±‚è¡¨ç¤ºçš„å·®), å¦ä¸€ä¸ªæŒ‡å¯¼å­¦ç”Ÿç½‘ç»œå­¦ä¹ åŸºäºå­¦åˆ°çš„ç‰¹å¾è¿›è¡Œçš„å†³ç­– (lossä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿlogitsçš„äº¤å‰ç†µ). å…¶ä¸­ç”¨åˆ°äº†adversarial learningçš„æ–¹æ³•, ç”¨ä¸€ä¸ªdiscriminatoråˆ†è¾¨æ•™å¸ˆå’Œå­¦ç”Ÿçš„è¡¨ç¤º. å®éªŒè®¾ç½®: å®éªŒåšå¾—åˆå°‘åˆæ‹‰, ä¹Ÿæ²¡è·Ÿsingle-teacheræ¯”, å°±ä¸å†™äº†. @InProceedings{10.1007/978-3-030-22796-8_7, title = {A Two-Teacher Framework for Knowledge Distillation}, author = {Chen, Xingjian, Su Jianbo, Zhang Jun\u0026quot;, booktitle = {Advances in Neural Networks \u0026ndash; ISNN 2019}, pages = {58\u0026ndash;66}, year = {2019} }  Feature-Level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks. ECAI 2020   https://ecai2020.eu/papers/405_paper.pdf (è¿™ç¯‡å’Œä¸Šæ¬¡çš„FEEDæ˜¯åŒä¸€ç¯‡)\n  é—®é¢˜: å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦åœ¨ä½¿ç”¨åŸºäºfeature-mapçš„è’¸é¦ä»»åŠ¡ä¸­ä¸æ–¹ä¾¿. æ–¹æ³•: åŠ å…¥ä¸€äº›éçº¿æ€§è½¬æ¢å±‚. loss functionç”±ä¸¤éƒ¨åˆ†ç»„æˆ: studentå’Œgroudtruthçš„äº¤å‰ç†µ; studentçš„featureç»è¿‡nç§éçº¿æ€§è½¬æ¢å¹¶å½’ä¸€åŒ–, åˆ†åˆ«å’Œteacherçš„feature (å½’ä¸€åŒ–å) åšå·®å¹¶æ±‚ä¸€èŒƒå¼. å®éªŒè®¾ç½®: æ•°æ®é›†: CIFAR-100; é€‰å–æ¨¡å‹: student \u0026ndash; ResNet-56, ResNet-110, WRN28-10, ResNext29-16x64d; æ²¡è¯´teacheræ˜¯è°. @article{Park2019FEEDFE, title={FEED: Feature-level Ensemble for Knowledge Distillation}, author={Seonguk Park and Nojun Kwak}, journal={ECAI}, year={2020} }  Ensemble Knowledge Distillation for Learning Improved and Efficient Networks. ECAI 2020   https://arxiv.org/pdf/1909.08097v1.pdf\n  é—®é¢˜: ç”±CNNç»„æˆçš„é›†æˆæ¨¡å‹åœ¨æ¨¡å‹æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºæ˜¾ç€æ”¹è¿›ï¼Œä½†ä»£ä»·æ˜¯è®¡ç®—é‡å¤§å’Œå†…å­˜éœ€æ±‚å¤§. (æœºç¿»çš„, æ‘˜è¦æå‡ºæ¥çš„é—®é¢˜è²Œä¼¼å’Œä»–åé¢åšçš„äº‹æ²¡å•¥å…³ç³», ä¸”è¿™ç¯‡å’Œä¸Šæ¬¡çœ‹çš„å€’æ•°ç¬¬äºŒç¯‡ç»“æ„ä¸€æ ·) æ–¹æ³•: å­¦ç”Ÿç”±å¤šä¸ªbranchç»„æˆ, æ¯ä¸ªbranchå’Œteacherä¸€ä¸€å¯¹åº”, lossç”±ä¸‰éƒ¨åˆ†ç»„æˆ: teacher (å¤šteacherè¾“å‡ºç›¸åŠ ) å’Œgroundtruthçš„äº¤å‰ç†µ, student (å¤šbranchçš„è¾“å‡ºç›¸åŠ ) å’Œgroungtruthçš„äº¤å‰ç†µ, teacherå’Œstudentå¯¹åº”branchè¡¨ç¤ºçš„KLæ•£åº¦å’ŒMSE. ç»“æ„:  å®éªŒè®¾ç½®: teacher \u0026ndash; ResNet14, ResNet20, ResNet26, ResNet32, ResNet44, ResNet56, and ResNet110; student \u0026ndash; a CNN with dense connections, a medium capacity CNN with 6 dense layers (DenseNet6), a large capacity CNN with 12 dense layers (DenseNet12); æ•°æ®é›†: EKD, CIFAR-10, CIFAR-100. @article{asif2019ensemble, title={Ensemble knowledge distillation for learning improved and efficient networks}, author={Asif, Umar and Tang, Jianbin and Harrer, Stefan}, journal={ECAI}, year={2020} }  Stochasticity and Skip Connection Improve Knowledge Transfer. Lee, Kwangjin et al. ICLR 2020   https://openreview.net/pdf?id=HklA93NYwS\n  é—®é¢˜: éƒ¨ç½²å¤šä¸ªæ•™å¸ˆç½‘ç»œæœ‰åˆ©äºå­¦ç”Ÿç½‘ç»œçš„å­¦ä¹ , ä½†åœ¨ä¸€å®šç¨‹åº¦ä¸Šé€ æˆèµ„æºæµªè´¹. æ–¹æ³•: åˆ©ç”¨å•ä¸ªæ•™å¸ˆç½‘ç»œç”Ÿæˆå¤šä¸ªæ•™å¸ˆç½‘ç»œ (åŠ å…¥add stochastic blockså’Œskip connections) å¹¶è®­ç»ƒå­¦ç”Ÿç½‘ç»œ, åœ¨æ²¡æœ‰é¢å¤–èµ„æºçš„æƒ…å†µä¸‹ä¸ºå­¦ç”Ÿç½‘ç»œæä¾›è¶³å¤Ÿçš„çŸ¥è¯†. å®éªŒè®¾ç½®: æ•°æ®é›† \u0026ndash; CIFAR-100 å’Œ tiny imagenet, å¹¶å°†è¿™ç§æ–¹æ³•åº”ç”¨åˆ°KD, AT(attention tranfer), ML. å®éªŒä¸­æ¶‰åŠåˆ°çš„teacheræœ‰ResNet 32, ResNet 110, WRN 28-10, MobileNet, WRN 40-4; æ¶‰åŠåˆ°çš„studentæœ‰VGG 13, ResNet 20, ResNet 32, WRN 40-4. @INPROCEEDINGS{9287227, author={Nguyen, Luong Trung and Lee, Kwangjin and Shim, Byonghyo}, title={Stochasticity and Skip Connection Improve Knowledge Transfer}, booktitle={2020 28th European Signal Processing Conference (EUSIPCO)}, pages={1537-1541}, year={2021} }  Amalgamating Knowledge towards Comprehensive Classification. AAAI 2019   https://arxiv.org/pdf/1811.02796v1.pdf\n  é—®é¢˜: é‡ç”¨å·²ç»è¿‡è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æ˜¾è‘—é™ä½é™ä½ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹çš„æˆæœ¬, å› ä¸ºç”¨äºè®­ç»ƒåŸå§‹ç½‘ç»œçš„æ³¨é‡Šé€šå¸¸ä¸å‘å…¬ä¼—å…¬å¼€. æ–¹æ³•: ä½¿ç”¨multi-teacher KDçš„æ–¹æ³•å¯¹å¤šä¸ªæ¨¡å‹è¿›è¡Œåˆå¹¶, å¾—åˆ°è½»é‡çº§çš„studentæ¨¡å‹. æ–¹æ³•åˆ†ä¸ºä¸¤æ­¥: The feature amalgamation step \u0026ndash; å°†teacherçš„æ¯ä¸€ä¸ªä¸­é—´å±‚è¡¨ç¤ºéƒ½åˆå¹¶, å¾—åˆ°studentå¯¹åº”çš„ä¸­é—´å±‚è¡¨ç¤º. ä¸€ç§ç®€å•çš„æ–¹å¼æ˜¯ç›´æ¥concat, ä½†ä¼šå¯¼è‡´studentå˜æˆteacherçš„4å€å¤§å°. å› æ­¤åœ¨concatå¤šä¸ªteacherçš„ä¸­é—´å±‚è¡¨ç¤ºåç»è¿‡ä¸€ä¸ªauto-encoder, å‹ç¼©student\u0026rsquo;s featureçš„åŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯; The parameter learning step \u0026ndash; æ ¹æ®studentç›¸é‚»ä¸¤å±‚è¡¨ç¤ºå­¦ä¹ ä¸­é—´å±‚å‚æ•°. æŸå¤±å‡½æ•°ç”±å‡ éƒ¨åˆ†ç»„æˆ: feature amalgamation \u0026ndash; teacherçš„ä¸­é—´å±‚è¡¨ç¤ºconcatä¹‹åç»è¿‡$1\\times 1$å·ç§¯å¾—åˆ°å‹ç¼©çš„è¡¨ç¤º, å†ç»è¿‡$1\\times 1$å·ç§¯è¯•å›¾å¤åŸ, å’Œconcatçš„è¡¨ç¤ºä¹‹å·® (çš„æ¨¡) ä½œä¸ºloss; parameter learning: studentå‰ä¸€å±‚è¡¨ç¤º $F_a^{l-1}$ ç»è¿‡ä¸­é—´å±‚åçš„è¡¨ç¤º $\\hat{F}_a^l = conv(pool(activation(F_a^{l-1})))$ å’Œç”±teacherç”Ÿæˆçš„ä¸‹ä¸€å±‚ $F_a^l$ è¡¨ç¤ºçš„å·®. ç»“æ„:  å®éªŒè®¾ç½®: æ•°æ®é›† \u0026ndash; CUB-200- 2011, Stanford Dogs, FGVC-Aircraft, Cars; teacher \u0026ndash; AlexNet (åœ¨ImageNetä¸Šfine-tune) @inproceedings{shen2019amalgamating, title={Amalgamating knowledge towards comprehensive classification}, author={Shen, Chengchao and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, pages={3068\u0026ndash;3075}, year={2019} }  Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning. IJCAI 2019   https://arxiv.org/pdf/1906.10546.pdf\n  é—®é¢˜: æ•™å¸ˆç½‘ç»œç»“æ„ä¸ä¸€, å„è‡ªæ“…é•¿è§£å†³ä¸åŒçš„ä»»åŠ¡. æ–¹æ³•: studentå­¦ä¹ teacherç»è¿‡è½¬åŒ–çš„è¡¨ç¤º (Common Feature Learning), åŒæ—¶å­¦ä¹ teacherè¾“å‡ºçš„soft target. ç»“æ„:  å®éªŒè®¾ç½®: networks: alexnet, vgg-16, resnet-18, resnet-34, resnet-50. datasets: (classification) Stanford Dog, Stanford Car, CUB200-2011, FGVC-Aircraft, Catech 101; (face) CASIA, MS-Celeb-1M, CFP-FP, LFW, AgeDB-30. @inproceedings{10.5555/3367243.3367468, author = {Luo Sihui, Wang Xinchao, Fang Gongfan, Hu Yao, Tao Dapeng, Song Mingli}, title = {Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning}, booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence}, pages = {3087â€“3093}, year = {2019} }  Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation. CVPR 2019   https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.pdf (è¿™ç¯‡æ˜¯å¤šæ•™å¸ˆ, ä½†ä¸»è¦é’ˆå¯¹çš„æ˜¯ä»å¤šä»»åŠ¡çš„æ•™å¸ˆé›†åˆä¸­è¿›è¡Œé€‰æ‹©æ€§å­¦ä¹ , å…¶ä¸­åªéœ€äº†è§£ç¬¬ä¸‰æ¡, é’ˆå¯¹çš„æ˜¯é—®é¢˜ä¸­çš„â€œå­¦ç”Ÿé€‰æ‹©æŸä¸ªè€å¸ˆç‰¹å¾ï¼Ÿâ€)\n  é—®é¢˜: å¦‚ä½•åˆ©ç”¨å¤šä¸ªé’ˆå¯¹ä¸åŒä»»åŠ¡, åœ¨ä¸åŒæ•°æ®é›†ä¸Šä¼˜åŒ–çš„æ•™å¸ˆæ¨¡å‹, æ¥è®­ç»ƒå¯ä»¥å®šåˆ¶çš„å¯ä»¥å¤„ç†é€‰æ‹©æ€§ä»»åŠ¡çš„å­¦ç”Ÿ? æ–¹æ³•: å‡è®¾æ²¡æœ‰å¯ç”¨çš„äººå·¥æ³¨é‡Šï¼Œå¹¶ä¸”æ¯ä¸ªè€å¸ˆå¯èƒ½æ˜¯å•ä»»åŠ¡æˆ–å¤šä»»åŠ¡. é¦–å…ˆä»å…±äº«ç›¸åŒå­ä»»åŠ¡çš„å¼‚æ„æ•™å¸ˆ(Source Net)ä¸­æå–ç‰¹å®šçš„çŸ¥è¯†(Component Net), åˆå¹¶æå–çš„çŸ¥è¯†ä»¥æ„å»ºå­¦ç”Ÿç½‘ç»œ(Target Net). ä¸ºäº†ä¿ƒè¿›è®­ç»ƒï¼Œä½œè€…é‡‡ç”¨äº†é€‰æ‹©æ€§å­¦ä¹ æ–¹æ¡ˆï¼Œå¯¹äºæ¯ä¸ªæœªæ ‡è®°çš„æ ·æœ¬ï¼Œå­¦ç”Ÿä»…ä»å…·æœ‰æœ€å°é¢„æµ‹æ­§ä¹‰çš„æ•™å¸ˆé‚£é‡Œè‡ªé€‚åº”åœ°å­¦ä¹ . Selective Learning: $I(p^t(x_i)) = -\\sum_ip^t(x_i)\\log(p^t(x_i))$, é’ˆå¯¹æ¯ä¸ªæ ·æœ¬å­¦ç”Ÿåªå­¦ç½®ä¿¡åº¦æœ€å¤§çš„æ•™å¸ˆ. å®éªŒè®¾ç½®: æ•°æ®é›†: CelebFaces Attributes Dataset (CelebA), Stanford Dogs, FGVC-Aircraft, CUB-200-2011, Cars. Source net: resnet-18; component net and target net adopt resnet-18-like network architectures. @inproceedings{shen2019customizing, title={Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation}, author={Shen, Chengchao and Xue, Mengqi and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={3504\u0026ndash;3513}, year={2019} }  Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning. ECCV 2020   https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510630.pdf (ä½œè€…å’Œ\u0026quot;7\u0026quot;æ˜¯åŒä¸€ä¸ª. è¿™ç¯‡æ–‡ç« ä¸»è¦é’ˆå¯¹å¤šä»»åŠ¡çŸ¥è¯†è’¸é¦, studentçš„å¤šä¸ªheadé’ˆå¯¹ä¸åŒä»»åŠ¡, ä½†å…¶ä¸­çš„Competi-Collaborationç­–ç•¥å¯ä»¥å­¦ä¹ )\n  é—®é¢˜: å’Œ\u0026quot;7\u0026quot;ä¸€æ ·. æ–¹æ³•: è®­ç»ƒåˆ†ä¸ºä¸¤æ­¥, ç¬¬ä¸€æ­¥æ•´åˆteacherçš„å¤šæ¨¡æ€ä¿¡æ¯, è®­ç»ƒstudentçš„å…±äº«å‚æ•°éƒ¨åˆ†; ç¬¬äºŒæ­¥ç”¨åŸºäºæ¢¯åº¦çš„ç«äº‰-å¹³è¡¡ç­–ç•¥, è®­ç»ƒstudentçš„multi-headéƒ¨åˆ†, æ¯ä¸ªéƒ¨åˆ†é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡. ä»¤$\\Omega, \\Theta_i, \\Theta_j$è¡¨ç¤ºstudentçš„å…±äº«å‚æ•°éƒ¨åˆ† (encoder) å’Œä¸¤ä¸ªé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„headå‚æ•°, Competi-Collaborationç­–ç•¥è¿­ä»£å¦‚ä¸‹:  Competition step: å›ºå®šstudentçš„å…±äº«å‚æ•°éƒ¨åˆ†, æœ€å°åŒ–multi-headçš„è¾“å‡ºå’Œmulti-teacherè¾“å‡ºlogitsçš„å·®å¼‚. Collaborative step: å›ºå®šmulti-headéƒ¨åˆ†å‚æ•°, æœ€å°åŒ–$head_i, head_j$è¾“å‡ºçš„logitså’Œå¯¹åº”teacherçš„å·®, ä»¥åŠstudentçš„encoderè¾“å‡ºå’Œteacherçš„encoderè¾“å‡ºçš„å·®. æœ€åæ›´æ–°åˆ†é…ç»™$head_i, head_j$çš„æƒé‡ (è®¡ç®—lossæ—¶ç”¨åˆ°), å¹¶å½’ä¸€åŒ–æ›´æ–°åçš„æƒé‡å‘é‡.   ç»“æ„:  å®éªŒè®¾ç½®: ä½¿ç”¨çš„æ•°æ®é›†æ˜¯Taskonomy dataset. teacherå‡ä½¿ç”¨taskonomy models, studentçš„encoderä½¿ç”¨ResNet-50åŠ ä¸€ä¸ªå·ç§¯å±‚, decoderçš„ç»“æ„æ ¹æ®å…·ä½“ä»»åŠ¡å„ä¸ç›¸åŒ. @InProceedings{10.1007/978-3-030-58539-6_38, author = {Luo Sihui, Pan Wenwen, Wang Xinchao, Wang Dazhou, Tang Haihong, Song Mingli}, title = {Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning}, booktitle = {Computer Vision \u0026ndash; ECCV 2020}, pages = {631\u0026ndash;646}, year = {2020} }  Highlight Every Step: Knowledge Distillation via Collaborative Teaching   https://arxiv.org/pdf/1907.09643v1.pdf (è¿™ç¯‡é‡Œç”¨åˆ°çš„attention lossè¿˜è¡Œ)\n  é—®é¢˜: ç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•æ—¶å¸¸å¿½ç•¥è®­ç»ƒè¿‡ç¨‹ä¸­ä¸è®­ç»ƒç»“æœæœ‰å…³çš„æœ‰ä»·å€¼çš„ä¿¡æ¯. æ–¹æ³•: æœ¬æ–‡ä¸€å…±ä½¿ç”¨ä¸¤ä¸ªteacher, ä¸€ä¸ªæ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„ (scratch teacher), æŒ‡å¯¼studentèµ°èƒ½åˆ°è¾¾æœ€ç»ˆlogitsçš„æœ€ä½³è·¯å¾„ (lossåŒ…å«å’Œgroundtruthçš„äº¤å‰ç†µ, å’Œstudent soft targetçš„L2 loss); å¦ä¸€ä¸ªæ˜¯å·²ç»é¢„è®­ç»ƒå¥½çš„ (expert teacher), å¼•å¯¼å­¦ç”Ÿä¸“æ³¨äºå¯¹ä»»åŠ¡æ›´æœ‰ç”¨çš„å…³é”®åŒºåŸŸ (ä¼šè®¡ç®—å’Œstudentä¸­é—´å±‚è¡¨ç¤ºçš„attention loss). ç»“æ„:  å®éªŒè®¾ç½®: æ•°æ®é›†é‡‡ç”¨CIFAR-10, CIFAR-100, SVHNå’ŒTiny-ImageNet. teacherså’Œstudentå‡ä½¿ç”¨Wide-ResNet, å…¶ä¸­teachersä¸ºWRN-40-1, studenté‡‡ç”¨WRN-16-1. @ARTICLE{9151346, author={Zhao Haoran, Sun Xin, Dong Junyu, Chen Changrui, Dong Zihe}, title={Highlight Every Step: Knowledge Distillation via Collaborative Teaching}, journal={IEEE Transactions on Cybernetics}, pages={1-12}, year={2020} }  Knowledge flow: Improve upon your teachers. ICLR 2019   https://openreview.net/pdf?id=BJeOioA9Y7\n  é—®é¢˜: å¦‚ä»Šå‡ ä¹å¯¹äºæ‰€æœ‰ç»™å®šçš„ä»»åŠ¡éƒ½å¯ä»¥ä½¿ç”¨ç°æœ‰çš„æ·±åº¦å­¦ä¹ ç½‘ç»œ, å¹¶ä¸”äººä»¬è¶Šæ¥è¶Šä¸æ¸…æ¥šåœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶åº”ä»å“ªä¸ªç½‘ç»œå¼€å§‹, æˆ–è€…é€‰æ‹©å“ªä¸ªç½‘ç»œè¿›è¡Œå¾®è°ƒ. æœ¬æ–‡å°†\u0026quot;çŸ¥è¯†\u0026quot;ä»å¤šä¸ªæ·±åº¦ç½‘ç»œ (ç§°ä¸ºæ•™å¸ˆ) ç§»åŠ¨åˆ°ä¸€ä¸ªæ–°çš„æ·±åº¦ç½‘ç»œæ¨¡å‹ (ç§°ä¸ºå­¦ç”Ÿ). æ•™å¸ˆå’Œå­¦ç”Ÿçš„ç»“æ„å¯ä»¥ä»»æ„ä¸åŒ, ä»–ä»¬ä¹Ÿå¯ä»¥åœ¨å…·æœ‰ä¸åŒè¾“å‡ºç©ºé—´çš„å®Œå…¨ä¸åŒçš„ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒ. æ–¹æ³•: æ•™å¸ˆçš„ä¸­é—´å±‚è¡¨ç¤º, ç»è¿‡å¯è®­ç»ƒçš„çŸ©é˜µ$Q$, åŠ æƒç»„æˆå­¦ç”Ÿçš„ä¸­é—´å±‚è¡¨ç¤º, æƒé‡$w$ä¹Ÿå¯å­¦ä¹ . å®éªŒè®¾ç½®: ä¸»è¦æ˜¯å¼ºåŒ–å­¦ä¹ . @inproceedings{liu2018knowledge, title={Knowledge Flow: Improve Upon Your Teachers}, author={Iou-Jen Liu and Jian Peng and Alexander Schwing}, booktitle={International Conference on Learning Representations}, year={2019} }  Semi-Supervised Knowledge Amalgamation for Sequence Classification. AAAI 2021   https://www.aaai.org/AAAI21Papers/AAAI-1292.ThadajarassiriJ.pdf\n  é—®é¢˜: æ¯ä¸ªæ•™å¸ˆåœ¨ä¸åŒçš„è®­ç»ƒé›†ä¸Šè®­ç»ƒ, å¯¼è‡´ä»–ä»¬å¯¹äºæœªçŸ¥çš„ç§ç±»æ ·æœ¬çš„è¾“å‡ºæ˜¯ä¸å¯é¢„æµ‹çš„, å¹¶ä¸”å’Œå…¶ä»–æ•™å¸ˆçš„è¾“å‡ºæ— å…³. å› æ­¤åœ¨èåˆå¤šæ•™å¸ˆçš„çŸ¥è¯†æ—¶, ä¸€äº›æ•™å¸ˆæœ‰å¯èƒ½ä¼šç»™å‡ºå¾ˆé«˜ç½®ä¿¡åº¦çš„é”™è¯¯åˆ†ç±». æ–¹æ³•: åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†, ä¸€ä¸ªæ˜¯ Teacher Trust Learner (TTL), åœ¨æœ‰æ ‡æ³¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå¯¹äºç»™å®šè¾“å…¥æ ·æœ¬, æ¯ä¸ªæ•™å¸ˆçš„å¯ä¿¡åº¦æœ‰å¤šé«˜ $P(y_j\\in\\mathcal{Y}_k|X)$; å¦ä¸€ä¸ªæ˜¯ Knowledge Amalgamator, ç”¨äºå°†å¤šä¸ªæ•™å¸ˆç»™å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæ•´åˆæˆåœ¨æœ€ç»ˆç±»åˆ«é›†åˆä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ. å®éªŒè®¾ç½®: Datasets: SyntheticControl (SYN), MelbournePedestrian (PED), Human Activity Recognition Using Smartphones (HAR), ElectricDevices (ELEC). Baselines: Original Teachers, SupLSTM, SelfTrain. @article{Sun2021CollaborativeTL, title={Collaborative Teacher-Student Learning via Multiple Knowledge Transfer}, author={Liyuan Sun, Jianping Gou, Lan Du, Dacheng Tao}, journal={ArXiv}, year={2021} }  Hydra: Preserving Ensemble Diversity for Model Distillation   https://arxiv.org/pdf/2001.04694.pdf\n  æ™®é€šmulti-teacher KDå¯¹teacherçš„é¢„æµ‹å€¼å–å¹³å‡, è¿™æ ·ä¼šä¸§å¤±å¤šteacherç»“æœåŒ…å«çš„ä¸ç¡®å®šæ€§ä¿¡æ¯, æœ¬æ–‡å°†studentæ‹†åˆ†æˆbodyå’Œå¤šä¸ªhead, æ¯ä¸ªheadå¯¹åº”ä¸€ä¸ªteacher, ä»¥ä¿ç•™å¤šteacherè¾“å‡ºçš„å¤šæ ·æ€§ å‡è®¾æœ‰Mä¸ªteacher, é¦–å…ˆè®­ç»ƒä¸€ä¸ªheadç›´è‡³å…¶æ”¶æ•›è‡³teacherçš„å¹³å‡, å†æ·»åŠ å…¶ä»–M-1ä¸ªhead, Mä¸ªheadä¸€èµ·è®­ç»ƒ, å®éªŒè¯æ˜å¦‚æœæ²¡æœ‰ç¬¬ä¸€ä¸ªheadä¼šå¾ˆéš¾æ”¶æ•›. ä½œè€…å®šä¹‰äº†ä¸€ä¸ªæ¨¡å‹ä¸ç¡®å®šæ€§, ç”±æ•°æ®ä¸ç¡®å®šæ€§å’Œæ€»ä¸ç¡®å®šæ€§ç»„æˆ(æˆ‘ä¸ç†è§£ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªé¡ºåº). ç»“æ„:  å®éªŒè®¾ç½®:  æ•°æ®é›†: a spiral toy dataset(ç”¨äºå¯è§†åŒ–å¹¶è§£é‡Šæ¨¡å‹ä¸ç¡®å®šæ€§), MNIST(æµ‹è¯•æ—¶ç”¨äº†å®ƒçš„æµ‹è¯•é›†å’ŒFashion-MNIST), CIFAR-10(æµ‹è¯•æ—¶ç”¨äº†å®ƒçš„æµ‹è¯•é›†, cyclic translated test set, 80 different corrupted test sets å’Œ SVHN). æ¨¡å‹: toy dataset - ä¸¤å±‚MLP, æ¯å±‚100ä¸ªç»“ç‚¹; MNIST - MLP; CIFAR-10 - ResNet-20 V1. åœ¨å›å½’é—®é¢˜ä¸­, æ‰€æœ‰æ•°æ®é›†å‡ä½¿ç”¨MLP.   @article{DBLP:journals/corr/abs-2001-04694, author = {Linh Tran, Bastiaan S. Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V. Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Sebastian Nowozin, Rodolphe Jenatton}, title = {Hydra: Preserving Ensemble Diversity for Model Distillation}, journal = {CoRR}, year = {2020} }  Diversity Matters When Learning From Ensembles. NIPS 2021   https://papers.nips.cc/paper/2021/file/466473650870501e3600d9a1b4ee5d44-Paper.pdf\n  é—®é¢˜: ä½œè€…çš„å‡è®¾æ˜¯ï¼Œä¸€ä¸ªè’¸é¦æ¨¡å‹åº”è¯¥å°½å¯èƒ½å¤šåœ°å¸æ”¶é›†æˆæ¨¡å‹å†…éƒ¨çš„åŠŸèƒ½å¤šæ ·æ€§, è€Œå…¸å‹çš„è’¸é¦æ–¹æ³•ä¸èƒ½æœ‰æ•ˆåœ°ä¼ é€’è¿™ç§å¤šæ ·æ€§, å°¤å…¶æ˜¯å®ç°æ¥è¿‘é›¶è®­ç»ƒè¯¯å·®çš„å¤æ‚æ¨¡å‹. æ–¹æ³•: é¦–å…ˆè¯æ˜äº†ä¸Šè¿°çŒœæƒ³, éšåä½œè€…æå‡ºäº†ä¸€ç§è’¸é¦æ‰°åŠ¨ç­–ç•¥, é€šè¿‡å¯»æ‰¾ä½¿å¾—é›†æˆæˆå‘˜è¾“å‡ºä¸ä¸€è‡´çš„è¾“å…¥æ¥æ­ç¤ºå¤šæ ·æ€§. ä½œè€…å‘ç°ç”¨è¿™ç§æ‰°åŠ¨æ ·æœ¬è’¸é¦å‡ºçš„æ¨¡å‹ç¡®å®è¡¨ç°å‡ºæ›´å¥½çš„å¤šæ ·æ€§.  å¯¹äºå¤šä¸ªæ•™å¸ˆæ¨¡å‹, ä½œè€…ç”¨ODS (è¾“å‡ºå¤šæ ·æ€§é‡‡æ ·) å¯¹è¾“å…¥æ ·æœ¬æ·»åŠ æ‰°åŠ¨, æœ€å¤§ç¨‹åº¦åœ°æé«˜é›†æˆçš„è¾“å‡ºåœ¨æ‰€ç”Ÿæˆæ ·æœ¬ä¹‹é—´çš„å¤šæ ·æ€§. æ•™å¸ˆåˆ†åˆ«å¯¹æ·»åŠ æ‰°åŠ¨åçš„æ ·æœ¬ç”Ÿæˆé¢„æµ‹, å­¦ç”Ÿæ¨¡å‹å¯ä»¥æœ€å¤§ç¨‹åº¦åœ°å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„å¤šæ ·æ€§. è®¾è®¡çš„æŸå¤±å‡½æ•°ä¹Ÿå¾ˆç®€å•, CE + KD   ç»“æ„:  å®éªŒè®¾ç½®: æ•°æ®é›†: CIFAR-10, CIFAR-100, TinyImageNet. åœ¨CIFAR-10ä¸Šç”¨çš„æ˜¯ResNet-32, åœ¨CIFAR-100, TinyImageNetä¸Šç”¨çš„æ˜¯WideResNet-28x10. @inproceedings{nam2021diversity, title={Diversity Matters When Learning From Ensembles}, author={Nam, Giung and Yoon, Jongmin and Lee, Yoonho and Lee, Juho}, booktitle={Thirty-Fifth Conference on Neural Information Processing Systems}, year={2021} }  Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective  https://arxiv.org/abs/2102.00650v1\n  $L_{ce}=-y_k\\log \\hat{y}{k,1}^{s}, L{kd}=-\\tau^2\\sum_k\\hat{y}{k,\\tau}^t\\log \\hat{y}{k,\\tau}^s$å¯¹ä¸¤ç§æœŸæœ›é”™è¯¯ç‡$\\text{error}{ce},\\text{error}{kd}$è¿›è¡Œåç½®-æ–¹å·®åˆ†è§£. $L_{kd}=L_{kd}-L_{ce}+L_{ce}$, æ ¹æ®ä¸Šä¸€æ¡, $L_{kd}-L_{ce}$å¯¼è‡´æ–¹å·®å‡å°, $L_{ce}$å¯¼è‡´åç½®å‡å°. If a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. å®šä¹‰äº† regularization sample: å®šä¹‰ä¸¤ä¸ªé‡$a=\\dfrac{\\partial L_{ce}}{\\partial z_i}, b=\\dfrac{\\partial(L_{kd}-L_{ce})}{\\partial z_i}$, å¯¹äºä¸€ä¸ªæ ·æœ¬, å½“$b \u0026gt; a$æ—¶, æ–¹å·®ä¸»å¯¼äº†ä¼˜åŒ–çš„æ–¹å‘, å› æ­¤å°†è¿™ä¸ªæ ·æœ¬å®šä¹‰ä¸ºregularization sample. å®éªŒè¡¨æ˜, regularization sampleçš„æ•°é‡å’Œè®­ç»ƒçš„æ•ˆæœæœ‰ä¸€å®šçš„å…³ç³» (åŒºåˆ«åœ¨äºæœ‰æ²¡æœ‰ label smoothingçš„æ•ˆæœç›¸å·®å¾ˆå¤§, åŒæ—¶ regularization sample çš„æ•°é‡ä¹Ÿç›¸å·®å¾ˆå¤§; ä½†æ˜¯$\\tau$å–ä¸åŒå€¼æ—¶è®­ç»ƒæ•ˆæœå’Œ regularization sample çš„æ•°é‡æ²¡æœ‰æ˜æ˜¾çš„å…³ç³»)  ","permalink":"https://michelia-zhx.github.io/posts/2022-02-24-multi_teacher_knowledge_distillation-2/","summary":"Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks https://arxiv.org/pdf/2004.05937.pdf Learning from Multiple Teacher Networks, KDD 2017 Efficient knowledge distillation from an ensemble of teachers. Interspeech 2017: å¯¹teacherçš„logitså–åŠ æƒå¹³å‡, åŠ æƒå¹³å‡å’Œstudentçš„logit","title":"Paper Notes - Multi-Teacher Knowledge Distillation - 2"},{"content":"Definition  Image Classification: è¾“å…¥å›¾ç‰‡, è¾“å‡ºå›¾ä¸­ç›®æ ‡ç‰©ä½“çš„ç±»åˆ«. Object Localization: è¾“å…¥å›¾ç‰‡, è¾“å‡ºå›¾ä¸­ç‰©ä½“çš„ bounding box. Object Detection: è¾“å…¥å›¾ç‰‡, è¾“å‡ºå›¾ä¸­ç‰©ä½“çš„ bounding box å’Œç±»åˆ«.  R-CNN Model Family é‡‡ç”¨region proposal methods, é¦–å…ˆç”Ÿæˆæ½œåœ¨çš„bounding boxes, ç„¶åé‡‡ç”¨åˆ†ç±»å™¨ è¯†åˆ«è¿™äº›bounding boxesåŒºåŸŸ. æœ€åé€šè¿‡post-processingæ¥å»é™¤é‡å¤bounding boxesæ¥è¿›è¡Œä¼˜åŒ–.\nè¿™ç±»æ–¹æ³•æµç¨‹å¤æ‚, å­˜åœ¨é€Ÿåº¦æ…¢å’Œè®­ç»ƒå›°éš¾çš„é—®é¢˜.\nR-CNN R-CNN ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆ:\n Region Proposal: Generate and extract category independent region proposals. Feature Extractor: Extract feature from each candidate region. Classifier: Classify features as one of the known class.  The feature extractor used by the model was the AlexNet deep CNN that won the ILSVRC-2012 image classification competition. The output of the CNN was a 4,096 element vector that describes the contents of the image that is fed to a linear SVM for classification, specifically one SVM is trained for each known class. é—®é¢˜æ˜¯è¿è¡Œå¾ˆæ…¢, testé˜¶æ®µCNNè¦ä»çº¦2000ä¸ªproposed regionä¸Šæå–ç‰¹å¾.\nFast R-CNN æå‡ºäº†R-CNNçš„ä¸‰ä¸ªé™åˆ¶:\n Training is a multi-stage pipeline. Training is expensive in space and time. Object detection is slow.  Fast R-CNNåªè®­ç»ƒä¸€ä¸ªæ¨¡å‹å»å­¦ä¹ ç‰©ä½“ä½ç½®å’Œåˆ†ç±», è€Œä¸æ˜¯ä¸€ä¸ªpipeline, è¾“å…¥æ˜¯ä¸€ä¸ªregion proposalçš„é›†åˆ, ç»è¿‡deep CNN, è¿›è¡Œç‰¹å¾æå–. CNNçš„ç»“å°¾æ˜¯ Region of Interest Pooling Layer (ROI Pooling), é’ˆå¯¹è¾“å…¥çš„candidate è¿›è¡Œç‰¹å¾æå–. CNNçš„è¾“å‡ºé€å…¥ä¸€ä¸ªFC, å¾—åˆ°ä¸¤ä¸ªè¾“å‡º, ä¸€ä¸ªç”¨äºsoftmax layeré¢„æµ‹ç±»åˆ«, å¦ä¸€ä¸ªç”¨äºregressionç”¨äºç”Ÿæˆbounding box.\nè¿™ä¸ªè¿‡ç¨‹é’ˆå¯¹æ¯ä¸€ä¸ªregion of interestè¿›è¡Œå¾ªç¯.\nFaster R-CNN ç”±ä¸¤éƒ¨åˆ†ç»„æˆ:\n Region Proposal Network: Convolutional neural network for proposing regions and the type of object to consider in the region. Fast R-CNN: Convolutional neural network for extracting features from the proposed regions and outputting the bounding box and class labels.  RPNç½‘ç»œæ¥å—CNNçš„è¾“å‡º, feature mapé€å…¥ä¸€ä¸ªå°å‹ç½‘ç»œå¾—åˆ°è®¸å¤šregion proposals, æ¯ä¸ªå¯¹åº”ä¸€ä¸ªåˆ†ç±». Region proposalsæ˜¯bounding boxes, æˆ–è€…è¯´anchor boxes, åç»­ä¼˜åŒ–. Class prediction is binary, indicating the presence of an object, or not, so-called \u0026ldquo;objectness\u0026rdquo; of the proposed region.\nYOLO Model Family YOLO è®­ç»ƒå•ç‹¬ä¸€ä¸ªç¥ç»ç½‘ç»œ, ç«¯åˆ°ç«¯, æ¥å—ä¸€ä¸ªå›¾ç‰‡ç›´æ¥é¢„æµ‹bounding box. å‡†ç¡®ç‡ä¸é«˜ä½†æ˜¯é€Ÿåº¦å¿«.\nYOLOé¦–å…ˆå°†å›¾åƒåˆ†ä¸º $$S\\times S$$ çš„æ ¼å­ (grid cell). å¦‚æœä¸€ä¸ªç›®æ ‡çš„ä¸­å¿ƒè½å…¥æ ¼å­, è¯¥æ ¼å­å°±è´Ÿè´£æ£€æµ‹è¯¥ç›®æ ‡. (å³ä½¿ä¸€ä¸ªå¯¹è±¡è·¨è¶Šå¤šä¸ªç½‘æ ¼, å®ƒä¹Ÿåªä¼šè¢«åˆ†é…åˆ°å…¶ä¸­ç‚¹æ‰€åœ¨çš„å•ä¸ªç½‘æ ¼. å¯ä»¥é€šè¿‡å¢åŠ æ›´å¤šç½‘æ ¼æ¥å‡å°‘å¤šä¸ªå¯¹è±¡å‡ºç°åœ¨åŒä¸€ç½‘æ ¼å•å…ƒä¸­çš„å‡ ç‡.)\næ¯ä¸€ä¸ªæ ¼å­ (grid cell) é¢„æµ‹bounding boxes(B)å’Œè¯¥boxesçš„ç½®ä¿¡å€¼(confidence score). ç½®ä¿¡å€¼ä»£è¡¨boxåŒ…å«ä¸€ä¸ªç›®æ ‡çš„ç½®ä¿¡åº¦. ç„¶å, æˆ‘ä»¬å®šä¹‰ç½®ä¿¡å€¼ä¸º $$Pr(Object)*IOU^{truth}_{pred}$$ . å¦‚æœæ²¡æœ‰ç›®æ ‡, ç½®ä¿¡å€¼ä¸ºé›¶. å¦å¤–, æˆ‘ä»¬å¸Œæœ›é¢„æµ‹çš„ç½®ä¿¡å€¼å’Œground truthçš„intersection over union (IOU)ç›¸åŒ.\næ¯ä¸€ä¸ªbounding boxåŒ…å«5ä¸ªå€¼: $$x, y, w, h$$ å’Œconfidence. $$(x, y)$$ ä»£è¡¨ä¸æ ¼å­ç›¸å…³çš„boxçš„ä¸­å¿ƒ. $$(w, h)$$ ä¸ºä¸å…¨å›¾ä¿¡æ¯ç›¸å…³çš„boxçš„å®½å’Œé«˜. confidenceä»£è¡¨é¢„æµ‹boxesçš„IOUå’Œgound truth. (IOU = äº¤å‰é¢ç§¯/è”åˆçš„é¢ç§¯)\næ¯ä¸ªæ ¼å­(grid cell)é¢„æµ‹æ¡ä»¶æ¦‚ç‡å€¼C $$Pr(Class_i|Object)$$ , æ¦‚ç‡å€¼Cä»£è¡¨äº†æ ¼å­åŒ…å«ä¸€ä¸ªç›®æ ‡çš„æ¦‚ç‡, æ¯ä¸€æ ¼å­åªé¢„æµ‹ä¸€ç±»æ¦‚ç‡. åœ¨æµ‹è¯•æ—¶, æ¯ä¸ªboxé€šè¿‡ç±»åˆ«æ¦‚ç‡å’Œboxç½®ä¿¡åº¦ç›¸ä¹˜æ¥å¾—åˆ°ç‰¹å®šç±»åˆ«ç½®ä¿¡åˆ†æ•°: $$Pr(Class_i|Object)*Pr(Object)*IOU^{truth}{pred} = Pr(Class_i)*IOU^{truth}{pred}$$\nè¿™ä¸ªåˆ†æ•°ä»£è¡¨è¯¥ç±»åˆ«å‡ºç°åœ¨boxä¸­çš„æ¦‚ç‡å’Œboxå’Œç›®æ ‡çš„åˆé€‚åº¦. ä¾‹å­è®²è§£\nå½“ä¸€ä¸ªç›®æ ‡ä¸æ­¢ä¸€æ¬¡è¢«è¯†åˆ«, éæå¤§å€¼æŠ‘åˆ¶å¯ä»¥æ˜¾ç€æé«˜YOLOçš„æ•ˆæœ.\nYOLOç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•æœ‰å¦‚ä¸‹æœ‰ä¼˜ç‚¹ï¼š\n éå¸¸å¿«. YOLOé¢„æµ‹æµç¨‹ç®€å•, é€Ÿåº¦å¾ˆå¿«. æˆ‘ä»¬çš„åŸºç¡€ç‰ˆåœ¨Titan X GPUä¸Šå¯ä»¥è¾¾åˆ°45å¸§/s; å¿«é€Ÿç‰ˆå¯ä»¥è¾¾åˆ°150å¸§/s. å› æ­¤, YOLOå¯ä»¥å®ç°å®æ—¶æ£€æµ‹. YOLOé‡‡ç”¨å…¨å›¾ä¿¡æ¯æ¥è¿›è¡Œé¢„æµ‹. ä¸æ»‘åŠ¨çª—å£æ–¹æ³•å’Œregion proposal-basedæ–¹æ³•ä¸åŒ, YOLOåœ¨è®­ç»ƒå’Œé¢„æµ‹è¿‡ç¨‹ä¸­å¯ä»¥åˆ©ç”¨å…¨å›¾ä¿¡æ¯. Fast R-CNNæ£€æµ‹æ–¹æ³•ä¼šé”™è¯¯çš„å°†èƒŒæ™¯ä¸­çš„æ–‘å—æ£€æµ‹ä¸ºç›®æ ‡, åŸå› åœ¨äºFast R-CNNåœ¨æ£€æµ‹ä¸­æ— æ³•çœ‹åˆ°å…¨å±€å›¾åƒ. ç›¸å¯¹äºFast R-CNN, YOLOèƒŒæ™¯é¢„æµ‹é”™è¯¯ç‡ä½ä¸€åŠ. YOLOå¯ä»¥å­¦ä¹ åˆ°ç›®æ ‡çš„æ¦‚æ‹¬ä¿¡æ¯ (generalizable representation), å…·æœ‰ä¸€å®šæ™®é€‚æ€§. æˆ‘ä»¬é‡‡ç”¨è‡ªç„¶å›¾ç‰‡è®­ç»ƒYOLO, ç„¶åé‡‡ç”¨è‰ºæœ¯å›¾åƒæ¥é¢„æµ‹. YOLOæ¯”å…¶å®ƒç›®æ ‡æ£€æµ‹æ–¹æ³• (DPMå’ŒR-CNN) å‡†ç¡®ç‡é«˜å¾ˆå¤š.  YOLOv2 (YOLO9000) and YOLOv3 YOLOv2 model makes use of anchor boxes, pre-defined bounding boxes with useful shapes and sizes that are tailored during training.\nThe choice of bounding boxes for the image is pre-processed using a k-means analysis on the training dataset.\né‡è¦çš„æ˜¯, æ›´æ”¹äº†è¾¹ç•Œæ¡†çš„é¢„æµ‹è¡¨ç¤ºå½¢å¼, ä»¥å…è®¸è¾ƒå°çš„æ›´æ”¹å¯¹é¢„æµ‹äº§ç”Ÿè¾ƒå°çš„å½±å“, ä»è€Œäº§ç”Ÿæ›´ç¨³å®šçš„æ¨¡å‹. ä¸æ˜¯ç›´æ¥é¢„æµ‹ä½ç½®å’Œå¤§å°, è€Œæ˜¯é¢„æµ‹åç§»é‡, ä»¥ç›¸å¯¹äºç½‘æ ¼å•å…ƒç§»åŠ¨å’Œé‡å¡‘é¢„å®šä¹‰çš„é”šæ¡†, å¹¶é€šè¿‡é€»è¾‘å‡½æ•°å¯¹å…¶è¿›è¡Œé˜»å°¼.\n[YOLOF: You Only Look One-level Feature](CVPR2021: https://arxiv.org/pdf/2103.09460.pdf) Problem: Address optimization problem by utilizing only one-level feature for detection. Two key components, Dilated Encoder and Uniform Matching are proposed and bring considerable improvements.\nPerformance: YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being $$2. 5\\times$$ faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with $$7\\times$$ less training epochs.\nSSD: Single Shot MultiBox Detector  é—®é¢˜: RCNNç³»åˆ—ä¸ºtwo-stageæ–¹æ³•, å…ˆé¢„å…ˆå›å½’ä¸€æ¬¡è¾¹æ¡†, å†è¿›è¡Œéª¨å¹²ç½‘ç»œè®­ç»ƒ, æ‰€ä»¥ç²¾åº¦æ›´é«˜, ä½†é€Ÿåº¦æ–¹é¢æœ‰å¾…æå‡. YOLOä¸ºone-stageæ–¹æ³•, åªåšä¸€æ¬¡è¾¹æ¡†å›å½’å’Œæ‰“åˆ†, é€Ÿåº¦å¿«ä½†å¯¹å°ç›®æ ‡æ•ˆæœå·®, å¯¹å°ºå¯¸æ•æ„Ÿ. ä½¿ç”¨one-stageæ€æƒ³, èå…¥Faster R-CNNä¸­çš„anchoræ€æƒ³, åšäº†ç‰¹å¾åˆ†å±‚æå–å¹¶ä»¥æ­¤è®¡ç®—è¾¹æ¡†å›å½’å’Œåˆ†ç±»æ“ä½œ, å› æ­¤å¯ä»¥é€‚åº”å¤šå°ºåº¦ç›®æ ‡çš„è®­ç»ƒå’Œæ£€æµ‹ä»»åŠ¡. åœ¨æ¯ä¸ªstageä¸­æ ¹æ®feature mapçš„å¤§å°æŒ‰ç…§å›ºå®šçš„ratioå’Œscaleç”Ÿæˆdefault boxes. ä¾‹å¦‚conv9çš„è¾“å‡ºfeature mapä¸º5*5, æ¯ä¸ªç‚¹é»˜è®¤ç”Ÿæˆ6ä¸ªbox, å› æ­¤ä¸€å¼ feature mapä¸Šæœ‰5*5*6=150ä¸ªdefault boxes, è€Œåæ¯ä¸ªdefault boxå°†ç”Ÿæˆ(c+1+4)ç»´çš„ç‰¹å¾å‘é‡, å…¶ä¸­cæ˜¯ç±»åˆ«æ•°, 1ä»£è¡¨èƒŒæ™¯, 4æ˜¯boxçš„åç§»å’Œç¼©æ”¾å°ºåº¦. SSDçš„backboneæ˜¯VGG16, å°†æœ€åçš„fc6å’Œfc7è½¬åŒ–æˆconv6å’Œconv7, å†åœ¨ä¹‹ååŠ ä¸Šä¸åŒå°ºåº¦çš„conv8, 9, 10, 11å››ä¸ªå·ç§¯ç½‘ç»œå±‚. è”åˆæŸå¤±å‡½æ•° $L(x,c,l,g) = \\dfrac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))$, å…¶ä¸­$L_{conf}$ä»£è¡¨åˆ†ç±»è¯¯å·®, ä½¿ç”¨softmax; $L_{loc}$ä»£è¡¨å›å½’è¯¯å·®, é‡‡ç”¨smooth L1 loss.  $L_{loc}(x,j,g) = \\sum_{i\\in Pos}^N\\sum_{m\\in{cx,cy,w,h}}x_{ij}^ksmooth_{L1}(l_i^m - \\hat{g}_j^m)$ $L_{conf}(x,c) = -\\sum_{x\\in Pos}x_{ij}^p\\log(\\hat{c}{i}^p) - \\sum{i\\in Neg}\\log(\\hat{c}i^0), where\\ \\hat{c}{i}^p = \\dfrac{\\exp(c_i^p)}{\\sum_p\\exp(c_i^p)}$   è®­ç»ƒç­–ç•¥:  åŒ¹é…ç­–ç•¥: ç¬¬ä¸€æ­¥æ˜¯æ ¹æ®æœ€å¤§çš„overlapå°†ground truthå’Œdefault boxè¿›è¡ŒåŒ¹é…, ç¬¬äºŒæ­¥æ˜¯å°†default boxesä¸overlapå¤§äºæŸä¸ªé˜ˆå€¼çš„fround truthè¿›è¡ŒåŒ¹é…. Default Boxesç”Ÿæˆå™¨: $S_k = S_{min}+\\dfrac{S_{max}-S_{min}}{m-1}(k-1), k\\in[1,m], ratio: a_r\\in{1,2,\\dfrac{1}{2}, 3, \\dfrac{1}{3}}$ Hard Negative Mining: æ ¹æ®confidence losså¯¹æ‰€æœ‰boxè¿›è¡Œæ’åº, é€‰å–ç½®ä¿¡åº¦è¯¯å·®è¾ƒå¤§çš„top-kä½œä¸ºè´Ÿæ ·æœ¬, ä½¿å¾—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹æ§åˆ¶åœ¨1:3ä¹‹å†….    PS: æ­£è´Ÿæ ·æœ¬æ€ä¹ˆç”¨å•Šå•Šå•Šå•Š\n","permalink":"https://michelia-zhx.github.io/posts/2021-09-12-object_detection/","summary":"Definition Image Classification: è¾“å…¥å›¾ç‰‡, è¾“å‡ºå›¾ä¸­ç›®æ ‡ç‰©ä½“çš„ç±»åˆ«. Object Localization: è¾“å…¥å›¾ç‰‡, è¾“å‡ºå›¾ä¸­ç‰©ä½“çš„ bounding box. Object Detection: è¾“å…¥å›¾ç‰‡, è¾“å‡ºå›¾ä¸­ç‰©ä½“çš„ bounding box å’Œç±»åˆ«. R-CNN Model Family é‡‡ç”¨region proposal methods, é¦–","title":"Paper Notes - Object Detection"},{"content":" Why self-supervised learning: ä¸»è¦çš„é—®é¢˜åœ¨äºè·å–æ•°æ®åŠå…¶æ ‡æ³¨éƒ¨åˆ†. Definition:  Self-supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one: \u0026ldquo;Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?\u0026rdquo; Replace the human annotation block by creatively exploiting some property of data to set up a pseudo-supervised task. å­¦åˆ°è¿™äº›ç‰¹å¾å, åˆ©ç”¨è¿ç§»å­¦ä¹ å°†ç‰¹å¾åœ¨ä¸‹æ¸¸çš„ç›‘ç£ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒ, ä»è€Œåˆ©ç”¨æ›´å°‘çš„æ•°æ®è¿›è¡Œè®­ç»ƒ.    Self-Supervised Learning from Image Pattern 1: Reconstruction  Image Colorization: ç”Ÿæˆå½©è‰²å’Œé»‘ç™½çš„å›¾ç‰‡å¯¹, é»‘ç™½å›¾ç‰‡ç»è¿‡encoder-decoderç”Ÿæˆé¢„æµ‹çš„å›¾ç‰‡, å’ŒçœŸæ˜¯å›¾ç‰‡è®¡ç®—L2 loss. å¦‚æ­¤å¯ä»¥å­¦åˆ°å›¾ç‰‡ä¸­çš„ç‰©ä½“åŠå…¶å…³è”éƒ¨åˆ†ä»¥æ­£ç¡®ä¸Šè‰². Image Superresolution: åŸºäºGANçš„SRGAN, ä½åˆ†è¾¨ç‡çš„å›¾ç‰‡ç»è¿‡å·ç§¯ç½‘ç»œç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾ç‰‡, è®¡ç®—å’ŒåŸå›¾çš„MSE. åŒæ—¶å°†ç”Ÿæˆçš„å›¾ç‰‡å’ŒçœŸå®å›¾ç‰‡é€å…¥äºŒåˆ†ç±»å™¨, åˆ¤æ–­å“ªä¸ªæ˜¯çœŸå®å›¾ç‰‡, ä»¥æ­¤ä½¿ç”Ÿæˆå™¨å­¦åˆ°å¦‚ä½•ç”Ÿæˆå…·æœ‰ç»†èŠ‚çš„é«˜åˆ†è¾¨ç‡å›¾ç‰‡. Image Impainting: å°†å›¾ç‰‡æ‰£æ‰ä¸€å—, ç”±ç”Ÿæˆå™¨ç”Ÿæˆå®Œæ•´çš„å›¾ç‰‡, åç»­æ­¥éª¤åŒä¸Š. Cross-Channel Prediction: ä¸€å¼ å›¾åˆ†æˆç°åº¦å›¾å’Œé¢œè‰²ä¿¡é“, ç”¨ç°åº¦å›¾é¢„æµ‹é¢œè‰²ä¿¡é“, ç”¨é¢œè‰²ä¿¡é“é¢„æµ‹ç°åº¦å›¾, å°†é¢„æµ‹çš„ä¸¤ä¸ªchannelå¤åˆæˆé¢„æµ‹çš„å›¾ç‰‡, å’ŒåŸå›¾å¯¹æ¯”è®¡ç®—lossä»¥æ”¹è¿›æ¨¡å‹.  Pattern 2: Common Sense Tasks  Image Jigsaw Puzzle: å›¾åƒåˆ†æˆ9ä¸ªpatch, æŒ‰ç…§æ±‰æ˜è·ç¦»æœ€å¤§çš„64ç§æ‰“ä¹±æ–¹å¼æ‰“ä¹±, è¾“å…¥æ˜¯9ä¸ªpatch, éœ€è¦é¢„æµ‹æ˜¯æŒ‰ç…§å“ªä¸€ç§æ–¹å¼ (1-64) æ‰“ä¹±çš„. è¿™ä¸€ä»»åŠ¡è¦æ±‚æ¨¡å‹å­¦ä¹ ç‰©ä½“é‡Œçš„å„éƒ¨åˆ†æ˜¯å¦‚ä½•ç»„åˆçš„, ä»¥åŠç‰©ä½“çš„å½¢çŠ¶. Context Prediction: å›¾ä¸­å–9ä¸ªç›¸é‚»çš„patché€‰å–ä¸­å¿ƒçš„patchå’Œå‘¨å›´éšæ„ä¸€ä¸ªpatch, è¾“å…¥å’Œå‰ä¸€æ¡ç±»ä¼¼çš„ç½‘ç»œ, é¢„æµ‹ä¸¤è€…ä½ç½®å…³ç³». Geometric Transformation Recognition: å°†æ—‹è½¬åçš„å›¾ç‰‡è¾“å…¥, é¢„æµ‹æ—‹è½¬äº†å¤šå°‘åº¦ (0, 90, 180, 270). è¿™ä¸ªä»»åŠ¡è¦æ±‚æ¨¡å‹å­¦ä¹ ç‰©ä½“çš„åœ°ç‚¹, ç±»å‹å’Œé€ å‹.  Pattern 3: Automatic Label Generation  Image Clustering: åœ¨æ— æ ‡æ³¨æ•°æ®é›†ä¸Šå¯¹å›¾ç‰‡è¿›è¡Œèšç±». Deep Clustering, å›¾ç‰‡é¦–å…ˆè¢«èšç±», ç„¶åè¿™äº›ç±»æ‰è¢«ä½œä¸ºclasses. Synthetic Imagery: ç”¨game engineç”Ÿæˆäººé€ å›¾ç‰‡, å’ŒçœŸå®çš„å›¾ç‰‡é€å…¥convnet, å¹¶åˆ¤æ–­ç”Ÿæˆçš„ç‰¹å¾æ˜¯å±äºäººé€ çš„orçœŸå®å­˜åœ¨çš„åœºæ™¯å›¾ç‰‡.  Self-Supervised Learning from Video  Frame Order Verification: (ä¸)æ‰“ä¹±å¸§çš„é¡ºåº, é€å…¥convnet, é¢„æµ‹è¾“å…¥é¡ºåºæ˜¯å¦æ˜¯æ­£ç¡®çš„.  SimCLR  Contrastive Learning: It attempts to teach machines to distinguish between similar and dissimilar images. éœ€è¦æ„é€ ç›¸ä¼¼çš„å›¾ç‰‡å¯¹å’Œä¸ç›¸ä¼¼çš„å›¾ç‰‡å¯¹. ä¸€å¼ å›¾ç‰‡è¢«ç»è¿‡éšæœºå˜æ¢åå’ŒåŸå›¾æ„æˆç›¸ä¼¼å›¾ç‰‡å¯¹ $(x_i,x_j)$, ä¸¤å¼ å›¾ç‰‡åˆ†åˆ«é€å…¥encoderä»¥è·å–è¡¨ç¤º $(h_i,h_j)$, åœ¨ç»è¿‡éçº¿æ€§å…¨è¿æ¥å±‚ä»¥è·å¾—æ–°çš„è¡¨ç¤º $(z_i,z_j)$, æœ€ååˆ¤æ–­ $(z_i,z_j)$ çš„ç›¸ä¼¼æ€§.  æ­¥éª¤  Self-supervised Formulation  Batch Size = N Transformation function T = random(crop + flip + color jitter + grayscale).   Getting Representation  é’ˆå¯¹ä¸€ä¸ªbatché‡Œçš„æ‰€æœ‰å›¾ç‰‡åšéšæœºå˜æ¢, å…±è·å¾—2Nå¼ å›¾ç‰‡. å¢å¼ºåçš„å›¾ç‰‡å¯¹ç»è¿‡å…±äº«å‚æ•°çš„encoderç”Ÿæˆè¡¨ç¤º$(h_i, h_j)$. ä½œè€…ä½¿ç”¨ResNet-50, è¾“å‡ºçš„ç‰¹å¾ä¸º2048ç»´å‘é‡.   Projection Head  Projection Head g(Â·) = Dense + Relu + Dense $(h_i, h_j)$ç»è¿‡ g ç”Ÿæˆæ–°çš„è¡¨ç¤º$(z_i, z_j)$, åœ¨ä»–ä»¬ä¹‹é—´è®¡ç®—ç›¸ä¼¼åº¦.   Tuning Model, é’ˆå¯¹Nå¼ å›¾ç‰‡ç”Ÿæˆçš„å¢å¼ºåçš„2Nä¸ªè¡¨ç¤º, è¿›è¡Œå¦‚ä¸‹æ“ä½œ  è®¡ç®—cosineç›¸ä¼¼åº¦, åŒä¸€å¼ å›¾å¢å¼ºåç›¸ä¼¼æ€§é«˜, å…¶ä½™å¾ˆä½ å°†2Nå¼ å›¾ç‰‡ç»„æˆå¾ˆå¤šå¯¹, ç”¨softmaxè®¡ç®—ä¸¤å¼ å›¾ç‰‡ç›¸ä¼¼çš„æ¦‚ç‡, é™¤äº†åŒä¸€å¼ å›¾ç”Ÿæˆçš„ä¸¤ä¸ªè¡¨ç¤ºå¾ˆç›¸ä¼¼, å…¶ä½™çš„å›¾ç‰‡å¯¹å…¨éƒ¨æ˜¯è´Ÿä¾‹. ä¸”è¿™ä¸ªæ–¹æ³•ä¸éœ€è¦memory bank, queueç­‰ç‰¹åˆ«çš„ç»“æ„. è®¡ç®—æŸå¤±: $l(i,j) = -\\log\\dfrac{\\exp(s_{i,j})}{\\sum_{k=1}^{2N}l_{[k!=i]}\\exp(s_{i,k})}$ ä¸€ä¸ªbatchçš„æŸå¤±: $L = \\dfrac{1}{2N}\\sum_{k=1}^M[l(2k-1, 2k) + l(2k, 2k-1)]$ ä¼˜åŒ–è¿™ä¸ªæŸå¤±å‡½æ•°å¯ä»¥æå‡å›¾ç‰‡è¡¨ç¤ºèƒ½åŠ›, æ˜¯æ¨¡å‹å­¦ä¼šåŒºåˆ†å›¾ç‰‡æ˜¯å¦ç›¸ä¼¼ (å¦‚æœä¸¤å¼ å›¾ç‰‡å±äºåŒä¸€ç±»æ˜¯å¦éœ€è¦æ›´é«˜çš„ç›¸ä¼¼åº¦? ä½†æ˜¯æ²¡æœ‰æ ‡æ³¨, æ— æ³•åˆ¤æ–­ä¸¤å¼ ç…§ç‰‡æ˜¯å¦å±äºåŒä¸€ç±».)    Downstream Tasks PIRL: Pretext-Invariant Representation Learning  å­˜åœ¨çš„é—®é¢˜: éšæœºå˜æ¢æœ‰å¯èƒ½å¯¼è‡´ä¸åŒçš„å›¾ç”Ÿæˆç›¸ä¼¼çš„æ–°å›¾. è§£å†³æ–¹æ³•: ä½¿åŒä¸€å¼ å›¾å˜æ¢åçš„æ–°å›¾å°½å¯èƒ½ç›¸ä¼¼, ä¸åŒå›¾å˜æ¢åçš„æ–°å›¾å°½å¯èƒ½ä¸ç›¸ä¼¼.  PIRL Framework  åŸå›¾ç»è¿‡ä¸€ä¸ªè½¬æ¢åç”Ÿæˆæ–°å›¾, ä¸¤å¼ å›¾ç»è¿‡å…±äº«å‚æ•°çš„convnet $\\theta$ åå¾—åˆ°ä¸¤ä¸ªè¡¨ç¤º$V_I, V_{I^T}$, $V_I$ç»è¿‡projection head $f$åå¾—åˆ°æ–°çš„è¡¨ç¤º $f(V_I)$, $V_{I^T}$ç»è¿‡projection head $g$åå¾—åˆ°æ–°çš„è¡¨ç¤º $g(V_{I^T})$. ä¼˜åŒ–æŸå¤±å‡½æ•°ä½¿å¾—ä¸¤ä¸ªè¡¨ç¤ºå°½é‡ç›¸ä¼¼, è€Œ$f(V_I)$å’Œmemory banké‡Œå…¶ä»–å›¾ç”Ÿæˆçš„è¡¨ç¤ºå°½é‡ä¸ç›¸ä¼¼. ç”¨memory bankå­˜å‚¨åˆ«çš„å›¾åƒç”Ÿæˆçš„è¡¨ç¤º, ä»¥é¿å…æ›´å¤§çš„batch size æŸå¤±å‡½æ•°: $h(f(V_I), g(V_{I^T})) = \\dfrac{\\exp(\\frac{s(f(V_I), g(V_{I^T}))}{\\tau})}{\\exp(\\frac{s(f(V_I), g(V_{I^T})}{\\tau}) + \\sum_{I'\\in D_N}\\exp(\\frac{g(V_{I^T}), s(f(V_{I'})}{\\tau})}$ $L_{NCE}(I, I^t) = -\\log[h(m_I, g(V_{I^t}))] - \\sum_{I'\\in D_N}\\log[1-h(g(V_{I^t}), m_{I'})]$  Self-Labelling Combining clustering and representation learning together to learn both features and labels simultaneously.\n ç”Ÿæˆæ ‡ç­¾ç„¶åç”¨æ ‡ç­¾è®­ç»ƒä¸€ä¸ªæ¨¡å‹ ç”±è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–°çš„æ ‡ç­¾ é‡å¤ä»¥ä¸Šæ“ä½œ ä»¥ä¸Šå½¢æˆé¸¡è›‹é—®é¢˜, æœ€åˆç”¨éšæœºåˆå§‹åŒ–çš„Alexnet, åœ¨Imageetä¸Šè¯„ä¼°.  Self-Labelling Pipeline  åˆ©ç”¨ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„æ¨¡å‹ä¸ºå¢å¼ºåçš„æ— æ ‡ç­¾æ•°æ®ç”Ÿæˆæ ‡ç­¾ ä½¿ç”¨Sinkhorn-Knoppç®—æ³•å°†æ— æ ‡ç­¾å›¾ç‰‡èšç±», å¾—åˆ°æ–°çš„æ ‡ç­¾ åˆ©ç”¨æ–°ç”Ÿæˆçš„æ ‡ç­¾è®­ç»ƒæ¨¡å‹ä¸Š, åˆ©ç”¨äº¤å‰ç†µæŸå¤±ä¼˜åŒ– é‡å¤ä»¥ä¸Šæ­¥éª¤  Sinkhron-Knopp Algorithm: æºäºæœ€ä¼˜è¿è¾“é—®é¢˜ (ä»“åº“, å•†åº—, è¿è¾“è·ç¦») - åŸé—®é¢˜: å°†Nä¸ªæ ·æœ¬åˆ†åˆ°Kä¸ªç±»ä¸­ - é™åˆ¶æ¡ä»¶: Nä¸ªæ ·æœ¬è¢«å‡åˆ†åˆ°Kä¸ªç±»ä¸­ - ä»£ä»·çŸ©é˜µ: å°† ä½¿ç”¨è¿™ç§åˆ†ç±»æ–¹å¼è®­ç»ƒå‡ºçš„æ¨¡å‹çš„æ¨¡å‹è¡¨ç° ä½œä¸ºè¿™ç§åˆ’åˆ†æ–¹å¼çš„ä»£ä»·. å¦‚æœä»£ä»·å¾ˆé«˜, è¯´æ˜éœ€è¦è°ƒæ•´åˆ’åˆ†æ–¹å¼ä»¥æ¥è¿‘ç†æƒ³æ•ˆæœ.\nFixMatch for Semi-Supervised Learning  Intuition: æ•°æ®é›†ä¸­åªæœ‰å°‘éƒ¨åˆ†æ ‡æ³¨æ•°æ®å’Œå¤§éƒ¨åˆ†æœªæ ‡æ³¨æ•°æ®, å…ˆç”¨æ ‡æ³¨æ•°æ®æœ‰ç›‘ç£è®­ç»ƒä¸€ä¸ªæ¨¡å‹. å¯¹äºæœªæ ‡æ³¨æ•°æ®, å°†æ‰€æœ‰å›¾ç‰‡åšä¸¤ç§å˜æ¢, åˆ†åˆ«é€å…¥ä¹‹å‰è®­ç»ƒçš„æ¨¡å‹, ç”±äºæ˜¯åŒä¸€å¼ å›¾ç‰‡ç”Ÿæˆçš„è¾“å…¥, å› æ­¤æ¨¡å‹çš„è¾“å‡ºåº”è¯¥ç›¸åŒ. æ¨¡å‹ç»“æ„:  å°†å·²æ ‡è®°æ•°æ®ç”¨äºè®­ç»ƒ, äº¤å‰ç†µæŸå¤±ä½œä¸ºç›‘ç£å­¦ä¹ çš„æŸå¤±å‡½æ•°: $l_s = \\dfrac{1}{B}\\sum_{b=1}^BH(p_b, p_m(y|\\alpha(x_b)))$ ä¼ªæ ‡è®°: å¯¹äºä¸€å¼ æœªæ ‡è®°å›¾ç‰‡, å…ˆå¯¹å®ƒè¿›è¡Œweak augmentation, è¾“å…¥æ¨¡å‹å¾—åˆ°ä¼ªæ ‡ç­¾, å†å’Œstrongly augmented imageçš„è¾“å‡ºå¯¹æ¯”. é’ˆå¯¹æœªæ ‡è®°æ•°æ®, åŠç›‘ç£éƒ¨åˆ†æŸå¤±å‡½æ•°: $l_u = \\dfrac{1}{\\mu B}\\sum_{b=1}^{\\mu B}1(max(q_b\\geq \\tau)H(\\hat{q}_b, p_m(y|A(u_b))))$, $loss = l_s + \\mu l_u$, å…¶ä¸­$\\tau$ä»£è¡¨ä¼ªæ ‡è®°ç”Ÿæˆçš„é˜ˆå€¼, å½“weakly augmented imageçš„softmaxè¶…è¿‡é˜ˆå€¼åˆ™ç”Ÿæˆä¼ªæ ‡è®°, å¹¶å’Œstrongly augmented imageçš„ç»“æœè®¡ç®—è¾ƒå·®ç†µ. åœ¨æœ€ç»ˆlossçš„è®¡ç®—ä¸­, éšç€è®­ç»ƒçš„è¿›åº¦$\\mu$é€æ¸å¢å¤§, å…·ä½“å¯ä»¥è§£é‡Šä¸ºå‰æœŸå¯¹ç”¨å°‘é‡æ ·æœ¬è®­ç»ƒå‡ºçš„æ¨¡å‹ä¸ä¿¡ä»», åæœŸä¼šé€æ¸åŠ å…¥å¯¹æœªæ ‡è®°æ•°æ®çš„è€ƒé‡.    DeepCluster  ä¸»è¦æ€æƒ³: å›¾ç‰‡ç»è¿‡augmentationä¹‹åé€å…¥convnetm å–åˆ†ç±»ä¹‹å‰çš„ç‰¹å¾, ç»è¿‡pcaé™ç»´, ç”¨kmeansèšç±»å¾—åˆ°å„ä¸ªç±»åˆ«. å°†kmeansçš„ç»“æœä½œä¸ºä¼ªæ ‡è®°, å’Œconvnetè¾“å‡ºçš„è¡¨ç¤ºç»è¿‡åˆ†ç±»å±‚å¾—åˆ°çš„ç»“æœè®¡ç®—äº¤å‰ç†µ.  ","permalink":"https://michelia-zhx.github.io/posts/2021-09-05-self_supervised_learning/","summary":"Why self-supervised learning: ä¸»è¦çš„é—®é¢˜åœ¨äºè·å–æ•°æ®åŠå…¶æ ‡æ³¨éƒ¨åˆ†. Definition: Self-supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one: \u0026ldquo;Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?\u0026rdquo; Replace","title":"Paper Notes - Self-Supervised Learning"},{"content":"Mining Typhoon Knowledge with Neural Networks \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999  éœ€è§£å†³çš„é—®é¢˜: ç¥ç»ç½‘ç»œçš„ä¸¤ä¸ªç¼ºç‚¹ \u0026ndash; æ•°æ®é‡å¤§, è®­ç»ƒæ—¶é—´é•¿; ç¥ç»ç½‘ç»œå¯¹çŸ¥è¯†çš„å­¦ä¹ æœä¸èƒ½ç›´æ¥ç”¨äºå†³ç­–. Fast neural model - FTART (Firld Theory based Adaptive Resonance Theory): éšå±‚çš„æ¿€æ´»å‡½æ•°æ˜¯Sigmoidå‡½æ•°, è¾“å…¥å±‚å’Œç¬¬äºŒå±‚ä¹‹é—´ä½¿ç”¨Gaussianæƒé‡, å¹¶æ›´æ–°. ç¬¬äºŒå±‚ç”¨äºåˆ†ç±»è¾“å…¥,ç¬¬ä¸‰å±‚ç”¨äºåˆ†ç±»è¾“å‡º, åœ¨è¿™ä¸¤å±‚ä¹‹é—´å»ºç«‹å…³ç³»æ¥è¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ . Rule extraction algorithm - SPT (Statistic based Producing and Testing):  ç”¨å¤§é‡å®ä¾‹æ¥è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œ ç»“åˆè¾“å…¥å’Œç¥ç»ç½‘ç»œçš„è¾“å‡ºæ¥æ„é€ ä¸€ä¸ªè™šæ‹Ÿç¤ºä¾‹é›†, å¦‚æœå­˜åœ¨å¤šä¸ªè¾“å…¥åˆ†é‡çš„ç»„åˆ, ä¸”æŠ•å½±åˆ°å®ƒæ—¶ç­‰ä»·çš„ç¤ºä¾‹å±äºæŸä¸ªç±»çš„æ¦‚ç‡ä¸º$\\lambda$, åˆ™é€šè¿‡å°†ç»„åˆä½œä¸ºå‰å› , ç±»åˆ«ä½œä¸ºåç»§æ¥æ„é€ è§„åˆ™. å¦‚æœæ²¡æœ‰è¿™æ ·çš„ç»„åˆ, åˆ™é€‰æ‹©å…·æœ‰æœ€ä½³èšç±»æ•ˆæœçš„è¿ç»­è¾“å…¥ç»„ä»¶å¹¶ç¦»æ•£åŒ–. (?) å¦‚ä½•å»å™ª: (?).    FANRE: A Fast Adaptive Neural Regression Estimator \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999  Adaptive Resonance Theory: åƒæ˜¯ä¸€ä¸ªKNNç½‘ç»œ, å¯ä»¥å®ç°ç¥ç»å…ƒçš„åŠ¨æ€æ‰©å……. å¯¹äºæ–°æ¥çš„æ ·æœ¬, å’Œä¹‹å‰çš„èšç±»ä¸­å¿ƒè¿›è¡Œæ¯”è¾ƒ, å¦‚æœç¬¦åˆé˜ˆå€¼è®¾å®š(å½¢æˆå…±æŒ¯), åˆ™å•ç‹¬è®­ç»ƒä¸ä¹‹åŒ¹é…çš„èšç±»ä¸­å¿ƒå¯¹åº”çš„ç¥ç»å…ƒçš„ç›¸å…³é“¾æ¥æƒé‡, å…¶ä»–ç¥ç»å…ƒä¿æŒä¸å˜; å¦‚æœæ‰€æœ‰ç¥ç»å…ƒéƒ½ä¸åŒ¹é…, é‚£å°±åˆ›é€ ä¸€ä¸ªæ–°çš„ç¥ç»å…ƒåˆ†é…ç»™è¿™ä¸ªæ•°æ®å½¢æˆæ–°çš„ä¸€ç±». FANREçš„ç»“æ„: è¾“å…¥å±‚, è¾“å‡ºå±‚, ä¸­é—´ä¸¤å±‚éšå±‚. æœ€åˆéšå±‚ä¸ºç©º, éšè¾“å…¥åŠ¨æ€æ·»åŠ éšå±‚ç»“ç‚¹. å¢é‡å­¦ä¹ , æ¯ä¸ªæ ·ä¾‹åªè¿‡ä¸€é.  ä¸¤ä¸ªå‚æ•°$\\theta_{ij}, \\alpha_{ij}$, ä»£è¡¨ä»ç¬¬ä¸€å±‚çš„$unit_i$åˆ°ç¬¬äºŒå±‚çš„$unit_j$çš„é«˜æ–¯æƒé‡çš„å“åº”ä¸­å¿ƒå’Œå“åº”ç‰¹å¾å®½åº¦. $Err_{max}$: æœ€å¤§å®¹é”™å€¼, $Vig_1$: first-degree vigilance, $Vig_2$: second-degree vigilance. ä¸”æœ‰$Err_{max} \u0026lt; Vig_1 \u0026lt; Vig_2$   FANREçš„å­¦ä¹ æµç¨‹: è¾“å…¥æ–°æ ·ä¾‹ $\\Rightarrow$ ç¬¬äºŒå±‚ç«äº‰ $\\Rightarrow$ ç¬¬ä¸‰å±‚ç«äº‰ $\\Rightarrow$ è®¡ç®—ç½‘ç»œè¾“å‡º:(*)  $Err \u0026lt; Err_{max}$ [å·²æœ‰çš„attracting basinèƒ½è¦†ç›–å½“å‰æ ·ä¾‹, ä¸éœ€è¦è°ƒæ•´] $\\Rightarrow$ ä¸‹ä¸€ä¸ªæ ·ä¾‹(**) $Err \\geq Err_{max}$:  $Err_{max} \\leq Err \u0026lt; Vig_1$ [è™½ç„¶æ€»ä½“çš„è¿‘ä¼¼è¡¨ç°ä¸å°½å¦‚äººæ„, ä½†å†…éƒ¨å¯¹è¾“å…¥è¾“å‡ºæ¨¡å¼çš„è¿‘ä¼¼è¿˜æ˜¯å¯ç”¨çš„] $\\Rightarrow$ è°ƒæ•´$\\theta_{ij}'$å’Œ$\\alpha_{ij}$, å›åˆ°(*) $Vig_1 \\leq Err \u0026lt; Vig_2$ [ç”±ç»“ç‚¹$u$è¡¨ç¤ºçš„å¯¹è¾“å‡ºçš„è¿‘ä¼¼å¯ç”¨, ä½†ç”±ç¬¬äºŒå±‚ç»“ç‚¹è¡¨ç¤ºçš„å¯¹è¾“å…¥çš„è¿‘ä¼¼ä¸åˆé€‚] $\\Rightarrow$ ç¬¬äºŒå±‚æ·»åŠ ä¸€ä¸ªç»“ç‚¹, å›åˆ°(**) $Err \\geq Vig_2$ [ç”±ç»“ç‚¹$u$è¡¨ç¤ºçš„å¯¹è¾“å‡ºçš„è¿‘ä¼¼, å’Œç”±ç¬¬äºŒå±‚ç»“ç‚¹è¡¨ç¤ºçš„å¯¹è¾“å…¥çš„è¿‘ä¼¼å‡ä¸åˆé€‚] $\\Rightarrow$ ç¬¬äºŒå±‚å’Œç¬¬ä¸‰å±‚å„æ·»åŠ ä¸€ä¸ªç»“ç‚¹, å›åˆ°(**)      Ensemble of GA based Selective Neural Network Ensembles \u0026ndash; Jian-Xin Wu, Zhi-Hua Zhou, Zhao-Qian Chen - 2002 $\\textbf{GASEN}$\n $N$ä¸ªåŸºå­¦ä¹ å™¨$f_i:\\mathbf{R}^m\\rightarrow\\mathbf{R}^n$åŠ æƒ: $\\overline{f}(x) = \\sum_{i=1}^Nw_if_i(x)$ è®¾å¯¹äºè¾“å…¥$x$, æœŸæœ›è¾“å‡ºä¸º$d(x)$, åˆ™åŸºå­¦ä¹ å™¨å’Œensembleçš„è¯¯å·®ä¸º $$E_i(x) = (f_i(x)-d(x))^2, E(x) = (\\overline{f}(x)-d(x))^2$$ è®¾$x$æœä»åˆ†å¸ƒ$p(x)$, åˆ™åŸºå­¦ä¹ å™¨å’Œensembleåœ¨åˆ†å¸ƒä¸Šçš„æ³›åŒ–è¯¯å·®ä¸º $$E_i = \\int p(x)E_i(x)\\rm{d}x, E = \\int p(x)E(x)\\rm{d}x$$ å¹³å‡è¯¯å·®ä¸º$\\overline{E}(x) = \\sum_{i=1}^Nw_iE_i(x)$, å¹³å‡æ³›åŒ–è¯¯å·®ä¸º$\\overline{E} = \\int p(x)\\overline{E}(x)\\rm{d}x$ Ambiguity of the i-th learner on input $x$: $$A_i(x) = (f_i(x)-\\overline{f}(x))^2, A_i = \\int p(x)A_i(x)\\rm{d}x$$ $$\\overline{A}(x)=\\sum_{i=1}^Nw_iA_i(x), \\overline{A} = \\int p(x)\\overline{A}(x)\\rm{d}x$$ å®šä¹‰ensembleçš„æ³›åŒ–æ€§èƒ½: $E = \\overline{E} - \\overline{A}$ ä¸¤ä¸ªåŸºå­¦ä¹ å™¨çš„ç›¸å…³æ€§: $$C_{ij} = \\int p(x)(f_i(x)-d(x))(f_j(x)-d(x))\\rm{d}x$$ $$E = \\sum_{i=1}^N\\sum_{j=1}^Nw_iw_jC_{ij}$$ å½“å¯¹äºç¬¬$k$ä¸ªåŸºå­¦ä¹ å™¨, æ»¡è¶³ $$(2N-1)\\sum_{i=1,i\\neq k}^N\\sum_{j=1,j\\neq k}^NC_{ij} \u0026lt; 2(N-1)^2\\sum_{i=1,i\\neq k}^NC_{ik} + (N-1)^2E_k$$  $\\textbf{e-GASEN}$ å…ˆç”¨GASENç®—æ³•è®­å‡ ä¸ªensembles, å†ç”¨ç®€å•çš„é›†æˆç®—æ³•æŠŠè¿™å‡ ä¸ªensemblesé›†æˆèµ·æ¥.\nHybrid Decision Tree \u0026ndash; Zhi-Hua Zhou, Zhao-Qian Chen, 2002  å¤„ç†æœ‰åºå±æ€§ \u0026ndash; å®šé‡åˆ†æ; æ— åºå±æ€§ \u0026ndash; å®šæ€§åˆ†æ. ç»“åˆsymbolic leanring (æ— åºå±æ€§) å’Œ neural learning (æœ‰åºå±æ€§). æ ‘çš„æ‰©å±•: å°†å±æ€§é›†åˆ†ä¸ºæ— åºå±æ€§$\\mathcal{L}_0$å’Œæœ‰åºå±æ€§$\\mathcal{L}_1$, å…ˆå°†HDTæŒ‰ç…§æ— åºå±æ€§æ‰©å±•, å½“æ ‘çš„åˆ†æ”¯å› ä¸ºç»“ç‚¹é‡Œçš„æ ·æœ¬å±äºåŒä¸€ç±»åˆ«è€Œæ— æ³•æ‰©å±•æ—¶, ç»ˆæ­¢æ‰©å±•; å½“å› ä¸ºç»“ç‚¹ä¸­æ— åºå±æ€§å‡è¢«ä½¿ç”¨è¿‡è€Œæ— æ³•æ‰©å±•æ—¶, å°†è¿™ä¸ªå¶ç»“ç‚¹æ ‡è®°ä¸ºneural node. Neural Processing: è½å…¥neural nodeçš„æ ·æœ¬è¢«è¿ç»­å±æ€§é›†é‡æ–°è¡¨ç¤º, å¹¶è¢«å½’ä¸€åŒ–. ç„¶åä½¿ç”¨FANNCåšåˆ†ç±» (å¢é‡å­¦ä¹ ) å‡ ç§å¢é‡å­¦ä¹ :  E-IL (Example-Incremental Learning) â€”â€” æ–°çš„æ ·æœ¬åˆ°æ¥æ—¶, ä¿è¯å­¦åˆ°æ–°çŸ¥è¯†çš„åŒæ—¶ä¸è¦ç‰ºç‰²è¿‡å¤šçš„æ—§çŸ¥è¯†. éå¢é‡å­¦ä¹ æ–¹æ³•ä¼šæœ‰ç¾éš¾æ€§é—å¿˜çš„ç¼ºé™·. C-IL (Class-Incremental Learning) â€”â€” å½“æ–°æ ·ä¾‹å±äºæ–°ç±»æ—¶, å­¦åˆ°æ–°çš„çŸ¥è¯†ä¸”ä¸ç”¨ç‰ºç‰²å¤ªå¤šæ—§çš„çŸ¥è¯† (ä¾‹å¦‚é‡æ–°å­¦ä¹ æ•´ä¸ªç³»ç»Ÿ) A-IL (Attribute-Incremental Learning) â€”â€” å½“æ–°æ ·ä¾‹å¸¦æœ‰æ–°å±æ€§æ—¶, å­¦åˆ°æ–°çš„çŸ¥è¯†ä¸”ä¸ç”¨ç‰ºç‰²å¤ªå¤šæ—§çš„çŸ¥è¯† (ä¾‹å¦‚é‡æ–°å­¦ä¹ æ•´ä¸ªç³»ç»Ÿ)    Face recognition with one training image per person \u0026ndash; Jian-Xin Wu, Zhi-Hua Zhou, 2002  äººè„¸è¯†åˆ«ç®—æ³•ä¸»è¦æœ‰ä¸¤ç§ç±»åˆ«: geometric feature-based and template-based techniques. PCAå±äºåè€…, ä½†æ²¡æœ‰è€ƒè™‘æ ‡ç­¾ä¿¡æ¯; è€Œè€ƒè™‘æ ‡ç­¾ä¿¡æ¯çš„æ¯ä¸ªç±»åˆ«è‡³å°‘éœ€è¦ä¸¤å¼ å›¾ç‰‡ (LDA). $(PC)^2A$ â€”â€” projection-combined principal component analysis: é’ˆå¯¹æ¯å¼ å›¾ç‰‡å¯¹å…¶è¿›è¡Œå˜æ¢  $x\\in [1,N_1], y\\in [1,N_2], P(x,y)\\in [0,1]$, $P(x,y)$æ˜¯ç°åº¦å›¾ $V_P(x) = \\sum_{y=1}^{N_2}P(x,y), H_P(y) = \\sum_{x=1}^{N_1}P(x,y)$ $\\overline{P} = \\dfrac{\\sum_{x=1}^{N_1}\\sum_{y=1}^{N_2}P(x,y)}{N_1N_2}, M_P(x,y) = \\dfrac{V_P(x)H_P(y)}{N_1N_2\\overline{P}}$ $P_{\\alpha}(x,y) = \\dfrac{P(x,y)+\\alpha M_P(x,y)}{1+\\alpha}$ $P_{\\alpha}'(x,y) = \\dfrac{P_{\\alpha}(x,y) - \\min(P_{\\alpha}(x,y))}{\\max(P_{\\alpha}(x,y)) - \\min(P_{\\alpha}(x,y))}$   æœ€ååœ¨projection-combined version of image $P_{\\alpha}(x,y)$ä¸Šä½¿ç”¨$PCA$.  Learning a Rare Event Detection Cascade by Direct Feature Selection \u0026ndash; Jianxin Wu, James M.Rehg, Matthew D.Mullin, 2003  äººè„¸æ£€æµ‹æ˜¯ç¨€æœ‰äº‹ä»¶æ£€æµ‹çš„å…¸å‹ä¾‹å­, ç»™ä¸€äº›äººè„¸å¤§å°çš„å›¾ç‰‡, å…¶ä¸­å¾ˆå°‘çš„éƒ¨åˆ†ä¼šåŒ…å«äººè„¸, target patterns occur with much lower frequency than non-targets. æœç´¢-åˆ†ç±»: æœç´¢å›¾ç‰‡ä¸­å¯èƒ½çš„åŒºåŸŸ, å†åˆ¤åˆ«æ˜¯å¦åŒ…å«è„¸. Viola-Jones framework åŒ…å«ä¸‰ä¸ªå…ƒç´ : å±‚å å¼ç»“æ„, ä¸€äº›é•¿æ–¹å½¢ç‰¹å¾, åŸºäºAdaBoostçš„ç®—æ³• - åœ¨æ¯ä¸ªåˆ†ç±»å™¨ä¸­æ„é€ é•¿æ–¹å½¢ç‰¹å¾çš„ensemble. æ¯ä¸ªåˆ†ç±»å™¨æ‹’ç»ä¸€éƒ¨åˆ†ä¸åŒ…å«äººè„¸çš„åŒºåŸŸ, å¹¶ä½¿åŒ…å«äººè„¸çš„é€šè¿‡. åœ¨æ¯ä¸ªç»“ç‚¹, ç»™å®šä¸€ä¸ªè®­ç»ƒé›†${x_i,y_i}$, è®­ç»ƒç›®æ ‡æ˜¯ä»æ€»å…±$F$ä¸ªç‰¹å¾ä¸­é€‰å‡ºä¸€äº›å¼±åˆ†ç±»å™¨${h_t}$, é›†æˆçš„åˆ†ç±»å™¨$H_i$éœ€è¦æœ‰å¾ˆé«˜çš„æ£€æµ‹ç‡$d_i$å’Œä¸­ç­‰çš„å‡æ­£ç‡$f_i$, åˆ™æ•´ä¸ªå±‚å å¼æ¨¡å‹çš„æ£€æµ‹ç‡$d = \\prod_{i=1}^nd_i$å’Œå‡æ­£ç‡$f = \\prod_{i=1}^nf_i$, å¯ä»¥ä¿è¯æœ‰è¾ƒé«˜çš„æ£€æµ‹ç‡å’Œå¾ˆä½çš„å‡æ­£ç‡. ç¬¬$t$è½®boostingå, ensembleè¡¨ç¤ºä¸º $$H(x) = \\left{\\begin{aligned} 1\\quad \u0026amp; \\sum_{t=1}^T\\alpha_th_t(x) \\geq \\theta\\ 0\\quad \u0026amp; otherwise \\end{aligned}\\right.$$ è®­ç»ƒä¸€ä¸ªç»“ç‚¹çš„è¿‡ç¨‹:  è®­ç»ƒæ‰€æœ‰çš„å¼±åˆ†ç±»å™¨, (*)åˆ¤æ–­æ˜¯å¦$d \u0026gt; D?$  yes $\\Rightarrow$ æ·»åŠ è¿™ä¸ªç‰¹å¾ä»¥æœ€å°åŒ–ensembleçš„å‡æ­£ç‡ no $\\Rightarrow$ æ·»åŠ è¿™ä¸ªç‰¹å¾ä»¥æœ€å¤§åŒ–ensembleçš„æ£€æµ‹ç‡ (ä»¥ä¸Šçš„æœ€å¤§åŒ–å’Œæœ€å°åŒ–å‡é€šè¿‡ç©·ä¸¾æ³•å®Œæˆ, é€‰æ‹©åŠ å…¥ensembleåèƒ½ç»™ensembleå¸¦æ¥æœ€å¤§æå‡çš„classifier)   å¦‚æœ $f \\geq F or d \\leq D$ $\\Rightarrow$ è¿”å›(*)   å’ŒViola-Jonesç›¸æ¯”, æœ¬ç®—æ³•åœ¨æ¯ä¸ªç»“ç‚¹æ¯ä¸ªå¼±å­¦ä¹ å™¨ä¹‹è®­ç»ƒä¸€æ¬¡, è€ŒViola-Jonesç®—æ³•ä¸­æ¯ä¸ªå¼±å­¦ä¹ å™¨æ¯é’ˆå¯¹ä¸€ä¸ªç‰¹å¾å°±è¦è®­ç»ƒä¸€æ¬¡.(?)  A Scalable Approach to Activity Recognition based on Object Use \u0026ndash; Jianxin-Wu, Adebola Osuntogun, Tanzeem Choudhury, ICCV 2006  ä½¿ç”¨åŠ¨æ€è´å¶æ–¯ç½‘ç»œ (Dynamic Bayesian Network), ä»è§†é¢‘ä¸­ç¨€ç–ä¸”æœ‰å™ªå£°çš„RFIDä¼ æ„Ÿå™¨æ•°æ®å’Œä¸€äº›æ´»åŠ¨çš„å¸¸è¯†å­¦ä¹ è®­ç»ƒæ¨¡å‹. Object-use Based Activity Recognition:  $A^t, O^t, R^t, V^t$åˆ†åˆ«ä»£è¡¨æ´»åŠ¨, ä½¿ç”¨çš„ç‰©ä½“, RFIDå’Œè§†é¢‘å¸§. DBNå…·æœ‰çš„å‚æ•°: å…ˆéªŒ$P(A^1)$, è§‚æµ‹æ¨¡å‹$P(O^1|A^1), P(O^{t+1}|O^{t}, A^{t+1})$, çŠ¶æ€è½¬ç§»æ¨¡å‹$P(A^{t+1}|A^{t})$, è¾“å‡ºæ¨¡å‹$P(V^t|O^t), P(R^t|O^t)$ å¦‚ä½•ç¡®å®šæ­£åœ¨ä½¿ç”¨çš„ç‰©ä½“ â€”â€” é™¤äº†RFIDä¼ æ„Ÿå™¨æ•°æ®, å€ŸåŠ©è§†é¢‘, å°†åƒç´ ç»„æˆ$8\\times 8$çš„superpixels, å¯¹æ¯”å½“å‰å¸§$t$å’Œ$t-3$$t+3$ä¸¤å¸§çš„superpixels, è®¡ç®—å·®è·, è‹¥å·®è·å‡è¶…è¿‡é˜ˆå€¼åˆ™å°†å…¶ä¸­çš„ç‰©ä½“è®¤å®šä¸ºæ­£åœ¨ä½¿ç”¨çš„ç‰©ä½“. å°†segmented areaä¸­æå–SIFTç‰¹å¾, å°†è§†é¢‘å¸§çœ‹æˆçš„é›†åˆ$V^t = (v^t_1,v^t_2,\u0026hellip;,v^t_{n^t})$, å…¶ä¸­ä»»ä¸¤ä¸ªSIFTç‰¹å¾ç›¸äº’ç‹¬ç«‹, ä¸”$P(V^t|O^t) = \\prod_{i=1}^{n^t}P(v^t_i|O^t) = \\prod_{i=1}^{n^t}\\mathbf{h}_{O^t}(v_i^t)$   Learning object models w/o human labeling  åœ¨EMç®—æ³•ä¸­ä½¿ç”¨RFID readingså’Œcommon knowledgeå»å­¦ä¹ object models. Eæ­¥: ä¼°è®¡ç»™å®š$R^t, V^t, \\mathbf{h}_{O^t}(v_i^t)$æ—¶$O^t$çš„è¾¹é™…æ¦‚ç‡ ç”¨standard junction treeç®—æ³•ä¼°è®¡æ¯ä¸€ä¸ªæ—¶åˆ»$O^t$çš„è¾¹é™…æ¦‚ç‡ ç»™å®š$O^t$çš„è¾¹é™…æ¦‚ç‡, $V^t$å’Œ$A^t$ç‹¬ç«‹ Mæ­¥: è®¡æ•°, æ›´æ–°$\\mathbf{h}_{O^t}(v_i^t)$   Specify parameters from domain knowledge  ä¸æƒ³ç»†çœ‹äº†    ","permalink":"https://michelia-zhx.github.io/posts/2021-07-15-pros_paper_notes/","summary":"Mining Typhoon Knowledge with Neural Networks \u0026ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999 éœ€è§£å†³çš„é—®é¢˜: ç¥ç»ç½‘ç»œçš„ä¸¤ä¸ªç¼ºç‚¹ \u0026ndash; æ•°æ®é‡å¤§, è®­ç»ƒæ—¶é—´é•¿; ç¥ç»ç½‘ç»œå¯¹çŸ¥è¯†çš„å­¦ä¹ æœä¸èƒ½ç›´æ¥ç”¨äºå†³ç­–. Fast neural model - FTART (Firld Theory","title":"Paper Notes - Week 1"},{"content":"Multi-Instance Multi-Label Learning with Application to Scene Classification \u0026ndash; Zhi-Hua Zhou, Min-Ling Zhang, NIPS 2006  Multi-instance: ä¸€ä¸ªexampleåŒ…å«å¤šä¸ªinstance, exampleåªå¯¹åº”1ä¸ªlabel; Multi-label: ä¸€ä¸ªexampleå¯¹åº”å¤šä¸ªlabel. ä»¥Multi-instanceæˆ–Multi-labelä¸ºæ¡¥æ¢, å°†multi-instance multi-label learning ($f_{MIML}: 2^{\\mathcal{X}}\\rightarrow 2^{\\mathcal{Y}}$) è½¬åŒ–ä¸ºä¼ ç»Ÿæœºå™¨å­¦ä¹ ä»»åŠ¡.  æ–¹æ³•ä¸€ (é€šè¿‡Multi-instance): å…ˆè½¬åŒ–ä¸º$f_{MIL}:2^{\\mathcal{X}}\\times \\mathcal{Y}\\rightarrow {-1. +1}$, å†è½¬åŒ–ä¸º$f_{SISL}: \\mathcal{X}\\times\\mathcal{Y}\\rightarrow{-1, +1}$, $f_{MIL}(X_i, y) = sign[\\sum_{i=1}^{n_i}f_{SISL}(x_{j}^{(i)}, y)]$ æ–¹æ³•äºŒ (é€šè¿‡Multi-label): å…ˆè½¬åŒ–ä¸º$f_{MLL}:\\mathcal{Z}\\rightarrow 2^{\\mathcal{Y}}$, å¯¹äºä»»æ„$z_i\\in \\mathcal{Z}, f_{MLL}(z_i) = f_{MIML}(X_i)$ if $z_i\\in\\phi(X_i), \\phi: 2^{\\mathcal{X}}\\rightarrow\\mathcal{Z}$, å†è½¬åŒ–ä¸º$f_{SISL}:\\mathcal{Z}\\times\\mathcal{Y}\\rightarrow{-1, +1}$, $f_{MLL}(z_i) = {y|\\arg_{y\\in\\mathcal{Y}}[f_{SISL}(z_i,y)=+1}$   MINIBOOST (åœ¨æ¯è½®boostingä¸­è¯•å›¾å°†$\\mathcal{F}(B)$æ‰©å±•ä¸º$\\mathcal{F}(B)+cf(B)$)  å°†æ¯ä¸ªMIMLæ ·æœ¬ $(X_u, Y_u)$ è½¬åŒ–ä¸º $|\\mathcal{Y}|$ ä¸ªmulti-instance bags ${[(X_u,y_1), \\Psi(X_u,y_1)],\u0026hellip;,[(X_u,y_{|\\mathcal{Y}|}), \\Psi(X_u,y_{|\\mathcal{Y}|})]}$ åˆå§‹åŒ–æ¯ä¸ªbagçš„æƒé‡$W^{(i)} = \\dfrac{1}{m\\times |\\mathcal{Y}|}$ å¯¹äºè¿­ä»£æ¬¡æ•°$t=1,2,\u0026hellip;,m\\times |\\mathcal{Y}|$:  ä»¤$W_j^{(i)} = W^{(i)}/n_i$, å°†bag çš„label $\\Psi(X^{(i)},y^{(i)})$èµ‹ç»™å…¶ä¸­çš„instance $(x_j^{(i)}, y^{(i)})$ï¼Œ è®­ç»ƒä¸€ä¸ª instance-levelçš„å­¦ä¹ å™¨ $h_t[(x_j^{(i)}, y^{(i)})]$. å¯¹ç¬¬ $i$ä¸ªbag, è®¡ç®—å…¶é”™è¯¯ç‡, $e^{(i)} = \\dfrac{\\sum_{i=1}^{n_i}\\llbracket{h_t[(x_j^{(i)}, y^{(i)})]\\neq\\Psi(X^{(i)},y^{(i)})}\\rrbracket}{n_i}$ è‹¥ $e^{(i)} \u0026lt; 0.5$ å¯¹æ‰€æœ‰ $i\\in [1,2,\u0026hellip;,m\\times|\\mathcal{Y}| ]$, åˆ™è·³å‡ºå¾ªç¯ è®¡ç®—$c_t=\\argmin_{c_t}\\sum_{i=1}^{m\\times|\\mathcal{Y}}W^{(i)}\\exp[(2e^{(i)}-1)c_t]$ å¦‚æœ$c_t \u0026lt; 0$, åˆ™è·³å‡ºå¾ªç¯ ä»¤ $W^{(i)} = W^{(i)}\\exp[(2e^{(i)}-1)c_t]$, é‡æ–°æ­£åˆ™åŒ–ä½¿å¾— $0\\leq W^{(i)}\\leq 1$ä¸”$\\sum_{i=1}^{m\\times|\\mathcal{Y}| }W^{(i)} = 1$   è¿”å› $Y^* = {y|\\arg_{y\\in\\mathcal{Y}}sign(\\sum_j\\sum_tc_th_t[(x_j^,Y)])=+1}$ ($x_j^$æ˜¯$X^*$çš„instance) P.S. æŸå¤±å‡½æ•°ä¸º: $$\\begin{aligned} E_{\\mathcal{B}}E_{\\mathcal{G}|\\mathcal{B}}[\\exp(-g\\mathcal{F}(B)+c(-g\\mathcal{F}(B)))] \u0026amp;= \\sum_i W^{(i)}\\exp[c\\left(-\\dfrac{g^{(i)}\\sum_jh(\\mathbf{b}_j^{(i)})}{n_i}\\right)]\\ \u0026amp;= \\sum_iW^{(i)}\\exp[(2e^{(i)}-1)c] \\end{aligned}$$   MINISVM  å¯¹MIMLæ ·æœ¬ $(X_u, Y_u), \\Gamma = {X_u|u=1,2,\u0026hellip;,m}$ ä»$\\Gamma$ä¸­éšå³é€‰æ‹©$k$ä¸ªå…ƒç´ åˆå§‹åŒ–$M_t$, é‡å¤ä»¥ä¸‹ç›´åˆ°$M_t$ä¸å†å˜åŒ–:  $\\Gamma_t = {M_t}$ (t=1,2,\u0026hellip;,k) å¯¹äºæ¯ä¸€ä¸ª $X_u\\in (\\Gamma-{M_t|t=1,2,\u0026hellip;,k})$:  $index = \\argmin_{t\\in{1,\u0026hellip;,k}}d_H(X_u, M_t), \\Gamma_{index} = \\Gamma_{index} \\cup {X_u}$   $M_t = \\argmin_{A\\in\\Gamma_t}\\sum_{B\\in\\Gamma_t}d_H(A, B)$ (t=1,2,\u0026hellip;,k) å°† $(X_u, Y_u)$ è½¬åŒ–ä¸º multi-instance æ ·æœ¬ $(z_u, Y_u), z_u = (z_{u1}, z_{u2}, \u0026hellip;, z_{uk}) = (d_H(X_u, M1), d_H(X_u, M_2),\u0026hellip;,d_H(X_u,M_k))$ å¯¹äºæ¯ä¸ª$y\\in\\mathcal{Y}$, ç”Ÿæˆä¸€ä¸ªæ•°æ®é›† $\\mathcal{D} = {(z_u,\\Phi(z_u,y))|u=1,2,\u0026hellip;,m}$, ç„¶åè®­ç»ƒä¸€ä¸ªSVM $h_y = SVMTrain(\\mathcal{D}_y)$ è¿”å› $Y^* = {\\argmax_{y\\in\\mathcal{Y}}h_y(z^)}\\cup{y|h_y(z^) \\geq 0, y\\in\\mathcal{Y}}$ P.S. Hausdorff distance: $d_H(A,B) = \\max{\\max_{a\\in A}\\min_{b\\in B}|\\mathbf{a} - \\mathbf{b}|, \\max_{b\\in B}\\min_{a\\in A}|\\mathbf{b} - \\mathbf{a}|}$      On Multi-Class Cost-Sensitive Learning \u0026ndash; Zhihua Zhou, Xuying Liu, AAAI 2006  rescale classes é’ˆå¯¹äºŒåˆ†ç±»é—®é¢˜æœ‰å¾ˆå¥½çš„æ•ˆæœ, ä½†æ˜¯é’ˆå¯¹å¤šåˆ†ç±»ä»£ä»·æ•æ„Ÿé—®é¢˜æ•ˆæœä¸å¥½. åŸç†: rescaleè®©ä»£ä»·ä¸æ•æ„Ÿçš„ç®—æ³•å˜å¾—ä»£ä»·æ•æ„Ÿ, å¯¹äºäºŒåˆ†ç±»é—®é¢˜, $p = P(class=1|\\mathbf{x})$, ä½œå‡ºæœ€ä¼˜é€‰æ‹©çš„é˜ˆå€¼$p^$æ»¡è¶³ $$p^\\times\\epsilon_{11}+(1-p^)\\times\\epsilon_{21} = p^\\times\\epsilon_{12}+(1-p^*)\\times\\epsilon_{22}$$ Elkan Theorem: å¯¹åº”åŸå…ˆçš„æ¦‚ç‡é˜ˆå€¼$p_0$è®¾ç½®æ–°çš„é˜ˆå€¼$p^$, åˆ™ç¬¬äºŒç±»çš„ä¸ªæ•°åº”ä¹˜ä»¥$\\dfrac{p^}{1-p^*}\\dfrac{1-p_0}{p_0}$. æ¨å¹¿åˆ°å¤šåˆ†ç±», rescaleæ—¶$i$ç±»ç›¸æ¯”äº$j$çš„æ¯”ä¾‹æ˜¯$\\tau_{opt}(i,j)=\\dfrac{\\epsilon_{ij}}{\\epsilon_{ji}}, \\epsilon_i = \\sum_{j=1}^c\\epsilon_{ij}, w_i = \\dfrac{(n\\times\\epsilon_i)}{\\sum_{k=1}^cn_k\\times\\epsilon_k}, w_i$æ˜¯èµ‹ç»™æ¯ä¸ªç±»æ ·æœ¬ä¸ªæ•°çš„æƒé‡. ä¼ ç»Ÿrescaleæ–¹æ³•èµ‹çš„æ¯”ä¾‹$\\tau(i,j) = \\dfrac{w_i}{w_j} = \\dfrac{\\epsilon_i}{\\epsilon_j}$, å’Œä¸Šä¸€æ¡ç›¸æ¯”åªåœ¨$c=2$æ—¶ä¸€æ ·, å…¶ä½™æƒ…å†µä¸ç­‰, å› æ­¤ä¼ ç»Ÿrescaleæ–¹æ³•é’ˆå¯¹å¤šåˆ†ç±»æƒ…å†µä¼šä¸é€‚ç”¨. The $RESCALE_{new}$ Approach $$\\begin{aligned} \\dfrac{w_1}{w_2} = \\dfrac{\\epsilon_{12}}{\\epsilon_{21}}, \\dfrac{w_1}{w_3} = \\dfrac{\\epsilon_{13}}{\\epsilon_{31}}, \u0026hellip;, \\dfrac{w_1}{w_c} \u0026amp;= \\dfrac{\\epsilon_{1c}}{\\epsilon_{c1}}\\ \\dfrac{w_2}{w_3} = \\dfrac{\\epsilon_{23}}{\\epsilon_{32}}, \u0026hellip;, \\dfrac{w_2}{w_c} \u0026amp;= \\dfrac{\\epsilon_{2c}}{\\epsilon_{c2}}\\ \u0026hellip;\\ \\ \\ \\ \\ \\ \\ \\ \u0026hellip;\\ \\ \\ \\ \\ \\ \\ \u0026amp;\\ \u0026hellip;\\ \\dfrac{w_{c-1}}{w_c} \u0026amp;= \\dfrac{\\epsilon_{c-1,c}}{\\epsilon_{c,c-1}} \\end{aligned}$$ å¯ä»¥æ„é€ å‡ºä¸€ä¸ª$c$å…ƒä¸€æ¬¡æ–¹ç¨‹ç»„.  Where am I: Place instance and category recognition using spatial $\\tt{PACT}$ \u0026ndash; Jianxin Wu, James M.Rehg, 2008, CVPR  \u0026ldquo;Where am I\u0026rdquo; é—®é¢˜: ç»å…¸çš„æœºå™¨äººé—®é¢˜, recognize instances and categories of places or scenes. åœ¨è§†è§‰ä¸­å¸¸è¢«å¤„ç†ä¸ºåœºæ™¯è¯†åˆ«é—®é¢˜ (åœºæ™¯çš„ç±»åˆ«è€Œä¸æ˜¯å…·ä½“ä½ç½®) PACT: Principal component Analysis of Census Transform histograms.  CT: ä¸€ä¸ªæ²¡æœ‰å‚æ•°çš„å±€éƒ¨è½¬æ¢æ–¹æ³•, å¯ä»¥å»ºç«‹å±€éƒ¨çš„ç›¸å…³æ€§. å°†å›¾ç‰‡è½¬æ¢æˆç°åº¦å›¾, æ¯ä¹ä¸ªåƒç´ ($3\\times 3$), å°†ä¸­é—´çš„åƒç´ ç°åº¦å’Œå‘¨å›´8ä¸ªæ¯”è¾ƒ(æ¯”è¾ƒç»“æœç”¨0/1)è¡¨ç¤º, æ„æˆä¸€ä¸ª8ä½çš„äºŒè¿›åˆ¶æ•°, è½¬æ¢æˆåè¿›åˆ¶æ•°åä½œä¸ºCTä»£æ›¿åŸæ¥çš„åƒç´ . CTçš„ä¼ é€’æ€§ä¿è¯äº†ç›¸è·å¾ˆè¿œçš„åƒç´ å¯¹åº”çš„CTä¹Ÿæœ‰ç›¸å…³æ€§. å†æ„é€ ç›´æ–¹å›¾ç»Ÿè®¡ä¸åŒCTå€¼çš„æ•°é‡ æœ€åä½¿ç”¨PCAè®¡ç®—å¯¹åº”ç‰¹å¾å€¼æœ€å¤§çš„ç‰¹å¾å‘é‡, å¯¹åº”ä¸»æˆåˆ†, å³å›¾ç‰‡ä¸­æœ€ä¸»è¦çš„å½¢çŠ¶.   Spatial PACT: å°†å›¾ç‰‡åˆ†æˆå°ä»½, åœ¨åŒºåŸŸä¸­å…¶ä¸­è®¡ç®—ç›¸å…³æ€§. æ„é€  \u0026ldquo;spatial pyramid\u0026rdquo;, 0,1,2-levelå¯¹åº”ä¸åŒçš„åˆ’åˆ†æ–¹å¼.  Semi-Supervised Learning with Very Few Labeled Training Examples  ä¼ ç»ŸåŠç›‘ç£å­¦ä¹ ç®—æ³•éœ€è¦å…ˆé€šè¿‡å°‘é‡æ ‡æ³¨æ•°æ®è®­ç»ƒä¸€ä¸ªåˆå§‹å¼±å­¦ä¹ å™¨$h$, å†åˆ©ç”¨$h$å’Œæ— æ ‡æ³¨æ•°æ®. ä½†æ˜¯åœ¨ç°å®ä¸­å¾ˆå¤šä»»åŠ¡èƒ½è·å¾—çš„æ ‡æ³¨æ•°æ®å¾ˆå°‘, æ¯”å¦‚åŸºäºå†…å®¹çš„å›¾åƒæ£€ç´¢(æœç´¢ç›¸ä¼¼å›¾ç‰‡)æˆ–åœ¨çº¿ç½‘é¡µæ¨è(åªæ‹¥æœ‰ä¸€ä¸ªç”¨æˆ·æ„Ÿå…´è¶£çš„é¡µé¢) OLTV (learning with One Labeled example and Two Views):  CCV (Canonical correlation analysis): å®šä¹‰ä¸¤ä¸ªè§†è§’é—´çš„ç›¸å…³æŠ•å½±. $X=(x_0,x_1,\u0026hellip;,x_{l-1}), Y=(y_0,y_1,\u0026hellip;,y_{l-1})$, å¯»æ‰¾ä¸¤ä¸ªåŸºå‘é‡é›†åˆ, ä»¥æœ€å¤§åŒ–ä¸¤ä¸ªè§†è§’å‘é‡åˆ†åˆ«åœ¨åŸºå‘é‡$w_x,w_y$ä¸ŠæŠ•å½±çš„ç›¸å…³æ€§: $$\\mathop{\\arg\\min}\\limits_{w_x,w_y} \\left(\\dfrac{w^T_xC_{xy}w^T_y}{\\sqrt{w^T_xC_{xx}w^T_x\\cdot w^T_yC_{yy}w^T_y}}\\right)\\ w^T_xC_{xx}w^T_x=1, w^T_yC_{yy}w^T_y=1$$ æœ€ç»ˆå¯ä»¥å¾—åˆ°ä¸€ä¸ªçº¿æ€§å…³ç³»$C_{xy}C_{yy}^{-1}C){yx}w_x=\\lambda^2C_{xx}w_x$. ä¹Ÿå¯ç”¨æ ¸å‡½æ•°æ˜ å°„åˆ°é«˜ç»´, ç›®æ ‡å‡½æ•°å˜ä¸º $$\\mathop{\\arg\\min}\\limits_{\\alpha, \\beta}\\dfrac{\\alpha^TS_xS_x^TS_yS_y^T\\beta}{\\sqrt{\\alpha^TS_xS_x^TS_xS_x^T\\alpha\\cdot\\beta^TS_yS_y^TS_yS_y^T\\beta}}$$ ä¸¤ä¸ªæ ¸çŸ©é˜µä¸º$K_x=S_xS_x^T, K_y=S_yS_y^T$, $\\alpha, \\beta$å¯ä»¥ç”±$(K_x+\\kappa I)^{-1}K_y(K_y+\\kappa I)^{-1}K_x\\alpha = \\lambda^2\\alpha, \\beta = \\dfrac{1}{\\lambda}(K_y+\\kappa I)^{-1}K_x\\alpha$ç®—å‡º. å› æ­¤å¯¹äºæ‰€æœ‰çš„$(x^,y^)$, æŠ•å½±å¯ä»¥åˆ©ç”¨$\\alpha, \\beta$ç®—å‡º. ç»§è€Œç®—å‡ºæ–°æ ·æœ¬å’Œæ—§æ ·æœ¬çš„æŠ•å½±çš„ç›¸ä¼¼åº¦, å°†æœ€åƒçš„ä½œä¸ºæ–°çš„æ­£ä¾‹, æœ€ä¸åƒçš„ä½œä¸ºè´Ÿä¾‹.    ","permalink":"https://michelia-zhx.github.io/posts/2021-07-22-pros_paper_notes/","summary":"Multi-Instance Multi-Label Learning with Application to Scene Classification \u0026ndash; Zhi-Hua Zhou, Min-Ling Zhang, NIPS 2006 Multi-instance: ä¸€ä¸ªexampleåŒ…å«å¤šä¸ªinstance, exampleåªå¯¹åº”1ä¸ªlabel; Multi-label: ä¸€ä¸ªexampleå¯¹åº”å¤šä¸ª","title":"Paper Notes - Week 2"},{"content":"1 å¿«æ’ 1.1 å¿«æ’ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1000010; int a[N]; void quicksort(int a[], int l, int r){ if (l \u0026gt;= r) return; int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1]; while (i \u0026lt; j){ do i ++ ; while (a[i] \u0026lt; x); do j -- ; while (a[j] \u0026gt; x); if (i \u0026lt; j) swap(a[i], a[j]); } quicksort(a, l, j); quicksort(a, j+1, r); } int main(){ int n; cin \u0026gt;\u0026gt; n; for(int i=0; i\u0026lt;n; i++) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); } quicksort(a, 0, n-1); for(int i=0; i\u0026lt;n; i++) printf(\u0026#34;%d \u0026#34;, a[i]); return 0; }   1.2 ç¬¬kå¤§çš„æ•°  æ‰§è¡Œå®Œpartitionæ“ä½œåï¼Œè®¾å·¦è¾¹çš„é•¿åº¦ä¸ºL , æ¢è½´ç‚¹çš„ä½ç½®ä¸ºp (å°†æ¢è½´å…ƒç´ è§†ä¸ºå±äºå·¦éƒ¨å³p==L)  è‹¥L == kï¼Œåˆ™æ¢è½´ç‚¹v å³ä¸ºç¬¬k å¤§æ•° è‹¥k \u0026lt; L, åˆ™åœ¨å·¦éƒ¨æ±‚å…¶ç¬¬kå¤§æ•° è‹¥k \u0026gt; L, åˆ™åœ¨å³éƒ¨æ±‚å…¶ç¬¬k âˆ’ Lå¤§æ•°    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1000010; int a[N]; int quicksort(int a[], int l, int r, int k){ if (l \u0026gt;= r){ return a[l]; } int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1]; while (i \u0026lt; j) { do i ++ ; while (a[i] \u0026lt; x); do j -- ; while (a[j] \u0026gt; x); if (i \u0026lt; j) swap(a[i], a[j]); } if (j - l + 1 \u0026gt;= k){ return quicksort(a, l, j, k); } else return quicksort(a, j + 1, r, k - (j - l + 1)); } int main(){ int n, k; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; for(int i=0; i\u0026lt;n; i++) cin \u0026gt;\u0026gt; a[i]; cout \u0026lt;\u0026lt; quicksort(a, 0, n-1, k) \u0026lt;\u0026lt; endl; return 0; }   2 å½’å¹¶æ’åº 2.1 å½’å¹¶æ’åº 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1e6 + 10; int a[N], tmp[N]; void merge_sort(int q[], int l, int r){ if (l \u0026gt;= r) return; int mid = l + r \u0026gt;\u0026gt; 1; merge_sort(q, l, mid), merge_sort(q, mid + 1, r); int k = 0, i = l, j = mid + 1; while (i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= r) if (q[i] \u0026lt;= q[j]) tmp[k ++ ] = q[i ++ ]; else tmp[k ++ ] = q[j ++ ]; while (i \u0026lt;= mid) tmp[k ++ ] = q[i ++ ]; while (j \u0026lt;= r) tmp[k ++ ] = q[j ++ ]; for (i = l, j = 0; i \u0026lt;= r; i ++, j ++ ) q[i] = tmp[j]; } int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 0; i \u0026lt; n; i ++ ) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); merge_sort(a, 0, n - 1); for (int i = 0; i \u0026lt; n; i ++ ) printf(\u0026#34;%d \u0026#34;, a[i]); return 0; }   2.2 é€†åºå¯¹çš„æ•°é‡ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;using namespace std; const int N = 100010; int a[N]; int nums; unsigned long result = 0; void merge_sort(int a[], int l, int r){ if (l \u0026gt;= r) return; int mid = (l+r) \u0026gt;\u0026gt; 1; merge_sort(a, l, mid); merge_sort(a, mid + 1, r); int temp[r - l + 1]; int lptr = l; int rptr = mid + 1; int tempptr = 0; while(lptr \u0026lt;= mid \u0026amp;\u0026amp; rptr \u0026lt;= r){ if(a[lptr] \u0026lt;= a[rptr]) temp[tempptr++] = a[lptr++]; else { temp[tempptr++] = a[rptr++]; result += (mid - lptr + 1); } } while (lptr \u0026lt;= mid) temp[tempptr++] = a[lptr++]; while (rptr \u0026lt;= r) temp[tempptr++] = a[rptr++]; for (int i = l, j = 0; i \u0026lt;= r; i ++, j ++){ a[i] = temp[j]; } } int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;nums); for(int i = 0; i \u0026lt; nums; i++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); merge_sort(a, 0, nums-1); cout \u0026lt;\u0026lt; result; return 0; }   3 äºŒåˆ† 3.1 æ•°çš„èŒƒå›´  å¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼Œè¿”å›ä¸€ä¸ªå…ƒç´  k çš„èµ·å§‹ä½ç½®å’Œç»ˆæ­¢ä½ç½®  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #include \u0026lt;iostream\u0026gt;using namespace std; const int maxn = 100005; int n, q, x, a[maxn]; int main() { scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n, \u0026amp;q); for (int i = 0; i \u0026lt; n; i++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;a[i]); while (q--) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); int l = 0, r = n - 1; while (l \u0026lt; r) { int mid = l + r \u0026gt;\u0026gt; 1; if (a[mid] \u0026lt; x) l = mid + 1; else r = mid; } if (a[l] != x) { printf(\u0026#34;-1 -1\\n\u0026#34;); continue; } int l1 = l, r1 = n; while (l1 + 1 \u0026lt; r1) { int mid = l1 + r1 \u0026gt;\u0026gt; 1; if (a[mid] \u0026lt;= x) l1 = mid; else r1 = mid; } printf(\u0026#34;%d %d\\n\u0026#34;, l, l1); } return 0; }   3.2 æ•°çš„ä¸‰æ¬¡æ–¹æ ¹ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #include\u0026lt;iostream\u0026gt;using namespace std; double n,l,r,mid; double q(double a){return a*a*a;} int main(){ cin \u0026gt;\u0026gt; n; l=-100, r=100; while(r - l \u0026gt;= 1e-7){ mid=(l+r)/2; if (q(mid)\u0026gt;=n) r=mid; else l=mid; } printf(\u0026#34;%.06f\u0026#34;, l); return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-01-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%951/","summary":"1 å¿«æ’ 1.1 å¿«æ’ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 1000010; int a[N]; void quicksort(int a[], int l, int r){ if (l \u0026gt;= r) return; int i = l-1, j = r+1, x = a[(l+r)\u0026gt;\u0026gt;1];","title":"åŸºç¡€ç®—æ³•(ä¸€)"},{"content":"1 åŒæŒ‡é’ˆ  ä¸¤ä¸ªæŒ‡é’ˆæŒ‡å‘ä¸¤ä¸ªåºåˆ—: å½’å¹¶æ’åº ä¸¤ä¸ªæŒ‡é’ˆæŒ‡å‘ä¸€ä¸ªåºåˆ—: å¿«æ’  1.1 æœ€é•¿ä¸é‡å¤å­åºåˆ— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include\u0026lt;iostream\u0026gt;#include\u0026lt;vector\u0026gt;#include\u0026lt;algorithm\u0026gt;using namespace std; const int maxn = 1e6+10; int a[maxn], q[maxn]; int n, ans=0; int main(){ cin \u0026gt;\u0026gt; n; for(int i=0;i\u0026lt;n;i++) cin \u0026gt;\u0026gt; a[i]; for(int i=0,j=0;i\u0026lt;n;i++){ q[a[i]]++; while(q[a[i]]\u0026gt;1){ q[a[j]]--; j++; } ans=max(ans,i-j+1); } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; endl; }   2 ä½è¿ç®—  nçš„äºŒè¿›åˆ¶è¡¨ç¤ºä¸­ç¬¬kä½æ˜¯å‡   æŠŠç¬¬kä½ç§»åˆ°æœ€åä¸€ä½ n \u0026raquo; k   lowbit(x): è¿”å›xäºŒè¿›åˆ¶çš„æœ€åä¸€ä½1 (è¿”å›å€¼æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶æ•°)  x \u0026amp; (-x) = x \u0026amp; (~x+1) x = 1010\u0026hellip;10000, ~x=0101\u0026hellip;01111, ~x+1=0101\u0026hellip;10000   nçš„äºŒè¿›åˆ¶è¡¨ç¤ºä¸­æœ‰å¤šå°‘ä¸ª1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include\u0026lt;iostream\u0026gt;using namespace std; int lowbit(int x){ return x \u0026amp; -x; // è¿”å›xäºŒè¿›åˆ¶çš„æœ€åä¸€ä½1 (è¿”å›å€¼æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶æ•°) } int main(){ int n; cin \u0026gt;\u0026gt; n; // å¤šæ¬¡æµ‹è¯•  while(n--){ int x; cin \u0026gt;\u0026gt; x; int res = 0; while (x){ x -= lowbit(x); // æŠŠæœ€åä¸€ä½1å»æ‰  res ++; } printf(\u0026#34;%d \u0026#34;, res); } return 0; }   3 ç¦»æ•£åŒ– 3.1 ç¦»æ•£åŒ–  æ•´æ•°åºåˆ—a, å€¼åŸŸå¤§ä½†ä¸ªæ•°å°‘, å°†ä»–ä»¬æ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°  aä¸­å¯èƒ½æœ‰é‡å¤å…ƒç´ (å»é‡) å¦‚ä½•ç®—å‡ºa[i]æ˜ å°„(ç¦»æ•£åŒ–å)çš„æ•°(äºŒåˆ†)    1 2 3 4 5 6 7 8 9 10 11 12 13  vector\u0026lt;int\u0026gt; alls; sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); int find(int x){ int i=0, r=alls.size()-1; while (l \u0026lt; r){ int mid = (l+r)/2; if (alls[mid] \u0026gt;= x) r = mid; else l = mid+1; } return r+1; // }   3.2 åŒºé—´å’Œ  é¦–å…ˆè¿›è¡Œ n æ¬¡æ“ä½œ, æ¯æ¬¡æ“ä½œå°†æŸä¸€ä½ç½® x ä¸Šçš„æ•°åŠ  c. æ¥ä¸‹æ¥, è¿›è¡Œ m æ¬¡è¯¢é—®, æ¯ä¸ªè¯¢é—®åŒ…å«ä¸¤ä¸ªæ•´æ•° l å’Œ r, ä½ éœ€è¦æ±‚å‡ºåœ¨åŒºé—´ [l,r] ä¹‹é—´çš„æ‰€æœ‰æ•°çš„å’Œ.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  #include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;algorithm\u0026gt;using namespace std; typedef pair\u0026lt;int, int\u0026gt; PII; const int N=300100; int a[N], s[N]; vector\u0026lt;int\u0026gt; alls; vector\u0026lt;PII\u0026gt; add, query; int find(int x){ int l=0, r=alls.size()-1; while (l \u0026lt; r){ int mid = (l+r)/2; if (alls[mid] \u0026gt;= x) r = mid; else l = mid+1; } return r+1; // } int main(){ int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; while(n--){ int x, c; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; c; add.push_back({x, c}); alls.push_back(x); } while(m--){ int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; query.push_back({l, r}); alls.push_back(l); alls.push_back(r); } //å»é‡  sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); for (auto item: add){ int x = find(item.first); a[x] += item.second; } for(int i = 1; i \u0026lt;= alls.size(); i ++ ) s[i] = s[i - 1] + a[i]; for (auto item: query){ int l = find(item.first), r = find(item.second); cout \u0026lt;\u0026lt; s[r] - s[l-1] \u0026lt;\u0026lt; endl; } return 0; }   4 åŒºé—´åˆå¹¶  è¿”å›nä¸ªåŒºé—´åˆå¹¶ä¹‹åçš„ä¸ªæ•°  æŒ‰å·¦ç«¯ç‚¹æ’åº åˆ†æ–°åŒºé—´ä¸å‰ä¸€ä¸ªåŒºé—´å…³ç³»çš„ä¸‰ç§æƒ…å†µæ›´æ–°åŒºé—´åˆ—è¡¨    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  #include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;algorithm\u0026gt;using namespace std; typedef pair\u0026lt;int, int\u0026gt; PII; vector\u0026lt;PII\u0026gt; p1, p2; int main(){ int n; cin \u0026gt;\u0026gt; n; while(n--){ int l, r; cin \u0026gt;\u0026gt; l \u0026gt;\u0026gt; r; p1.push_back({l,r}); } sort(p1.begin(), p1.end()); int res = 1; for(auto item: p1){ if (p2.size() == 0) p2.push_back(p1[0]); int l = item.first, r = item.second; if (l \u0026lt;= p2[p2.size()-1].second){ if (r \u0026gt;= p2[p2.size()-1].second) p2[p2.size()-1].second = r; } else{ res += 1; p2.push_back({l,r}); } } cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; endl; return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-03-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%952/","summary":"1 åŒæŒ‡é’ˆ ä¸¤ä¸ªæŒ‡é’ˆæŒ‡å‘ä¸¤ä¸ªåºåˆ—: å½’å¹¶æ’åº ä¸¤ä¸ªæŒ‡é’ˆæŒ‡å‘ä¸€ä¸ªåºåˆ—: å¿«æ’ 1.1 æœ€é•¿ä¸é‡å¤å­åºåˆ— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include\u0026lt;iostream\u0026gt;#include\u0026lt;vector\u0026gt;#include\u0026lt;algorithm\u0026gt;using namespace std; const int maxn =","title":"åŸºç¡€ç®—æ³•(äºŒ)"},{"content":"1 é“¾è¡¨ä¸é‚»æ¥è¡¨ 1 2 3 4 5  struct Node{ int val; Node *next; } new Node();   1.1 ç”¨æ•°ç»„æ¨¡æ‹Ÿé“¾è¡¨  ç”¨æ•°ç»„æ¨¡æ‹Ÿå•é“¾è¡¨(é™æ€é“¾è¡¨): é‚»æ¥è¡¨(å­˜å‚¨å›¾å’Œæ ‘)  O(1)æ—¶é—´æ‰¾ä¸‹ä¸€ä¸ªç‚¹, O(n)æ—¶é—´æ‰¾ä¸Šä¸€ä¸ªç‚¹    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 100010; int head, e[N], ne[N], idx; void init(){ head = -1; } void add_to_head(int x){ ne[idx] = head; head = idx; e[idx] = x; idx ++; } void add(int k, int x){ e[idx] = x; ne[idx] = ne[k]; ne[k] = idx; idx ++; } void del(int k){ ne[k] = ne[ne[k]]; }   1.2 ç”¨æ•°ç»„æ¨¡æ‹ŸåŒé“¾è¡¨  æ¯ä¸ªèŠ‚ç‚¹æœ‰ä¸¤ä¸ªæŒ‡é’ˆ, æŒ‡å‘å‰å l[N], r[N], 0å¯¹åº”head, 1å¯¹åº”tail, ä¸¤ä¸ªè¾¹ç•Œä¸åŒ…å«å®è´¨å†…å®¹  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 100010; int head, e[N], l[N], r[N], idx; void init(){ r[0] = 1; l[1] = 0; idx = 2; } void add(int k, int x){ e[idx] = x; r[idx] = r[k]; l[idx] = k; l[r[k]] = idx; r[k] = idx; idx ++; } void del(int k){ r[l[k]] = r[k]; l[r[k]] = l[k]; }   2 æ ˆä¸é˜Ÿåˆ— 2.1 æ¨¡æ‹Ÿæ ˆ 1 2 3 4 5 6 7  int stk[N], tt; // æ’å…¥ stk[ ++t] = x; // å¼¹å‡º t -- ; //åˆ¤æ–­æ˜¯å¦ä¸ºç©º return t \u0026gt; 0; // ä¸ç©º   2.2 å•è°ƒæ ˆ  æ±‚æ¯ä¸€ä¸ªæ•°å·¦è¾¹ç¦»å®ƒæœ€è¿‘ä¸”æ¯”å®ƒå°çš„æ•°, æ²¡æœ‰è¿”å›-1 äº‹å®ä¸Šå¯¹äºè¿™æ ·çš„è¦æ±‚, åªéœ€è¦ä¿ç•™å•è°ƒä¸Šå‡åºåˆ—å³å¯, é€†åºå¯¹çš„ç¬¬ä¸€ä¸ªå…ƒç´ åˆ æ‰  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 100010; int n; int stk[N], tt; int main(){ ios::sync_with_stdio(false); scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i\u0026lt;n; i++){ int x; scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); while (tt \u0026amp;\u0026amp; stk[tt] \u0026gt;= x) t --; if (tt) printf(\u0026#34;%d \u0026#34;, stk[tt]); else printf(\u0026#34;-1 \u0026#34;); stk[ ++t] = x; } return 0; }   2.3 æ»‘åŠ¨çª—å£é‡Œçš„æœ€å¤§å€¼å’Œæœ€å°å€¼ (å•è°ƒé˜Ÿåˆ—)  åœ¨æ‰¾æœ€å°å€¼æ—¶, åªè¦å‰é¢çš„å…ƒç´ æ¯”åé¢çš„å…ƒç´ å¤§, é‚£ä¹ˆå¤§çš„å…ƒç´ ä¸€å®šæ²¡æœ‰ç”¨ æ‰¾æœ€å¤§å€¼æ˜¯å®Œå…¨å¯¹ç§°çš„å†™æ³•  3 kmp  æš´åŠ›æ€ä¹ˆåš å¦‚ä½•ä¼˜åŒ–  next[i] = jè¡¨ç¤ºp[1,\u0026hellip;,j] = p[i-j+1,\u0026hellip;,i]    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #include \u0026lt;iostream\u0026gt;using namespace std; const int N = 10010, M=100010; int n, m; int p[N], s[M]; int ne[N]; int main(){ cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; p + 1 \u0026gt;\u0026gt; m \u0026gt;\u0026gt; s + 1; // æ±‚nextè¿‡ç¨‹  for (int i=2, j=0; i\u0026lt;=m; i++){ while (j \u0026amp;\u0026amp; p[i] != p[j+1]) j = ne[j]; if (p[i] == p[j+1]) j ++; ne[i] = j; } // åŒ¹é…è¿‡ç¨‹  for (int i=1, j=0; i\u0026lt;=m; i++){ while (j \u0026amp;\u0026amp; s[i] != p[j+1]) j = ne[j]; if (s[i] != p[j+1]) j ++; if (j == n){ // åŒ¹é…æˆåŠŸ  } } }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-07-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841/","summary":"1 é“¾è¡¨ä¸é‚»æ¥è¡¨ 1 2 3 4 5 struct Node{ int val; Node *next; } new Node(); 1.1 ç”¨æ•°ç»„æ¨¡æ‹Ÿé“¾è¡¨ ç”¨æ•°ç»„æ¨¡æ‹Ÿå•é“¾è¡¨(é™æ€é“¾è¡¨): é‚»æ¥è¡¨(å­˜å‚¨å›¾å’Œæ ‘) O(1)æ—¶é—´æ‰¾ä¸‹ä¸€ä¸ªç‚¹, O(n)","title":"æ•°æ®ç»“æ„(ä¸€)"},{"content":"ç³»ç»Ÿä¸ºæŸä¸€ç¨‹åºåˆ†é…ç©ºé—´æ‰€éœ€æ—¶é—´ï¼Œä¸ç©ºé—´å¤§å°æ— å…³ï¼Œä¸ç”³è¯·æ¬¡æ•°æœ‰å…³ï¼\nå“ˆå¸Œ  å­˜å‚¨ç»“æ„ï¼ˆä¸€èˆ¬åªæœ‰æ·»åŠ å’ŒæŸ¥æ‰¾æ“ä½œï¼Œå¦‚æœè¦åˆ ï¼Œåœ¨ç‚¹ä¸Šæ‰“ä¸€ä¸ªæ ‡è®°å³å¯ï¼‰  æ‹‰é“¾æ³•ï¼ˆæ§½+é“¾è¡¨ï¼‰ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  #include \u0026lt;iostream\u0026gt;using namespace std; const int N=100003; int h[N], e[N], ne[N], idx; void insert(int x){ int k = (x % N + N) % N; // è®©ä½™æ•°ä¸ºæ­£  e[idx] = x; //æ–°ç‚¹çš„çœŸæ­£å­˜å‚¨ä½ç½®  ne[idx] = h[k]; //æ–°ç‚¹çš„nextæŒ‡é’ˆæŒ‡å‘h[k](æŒ‡å‘çš„ç‚¹)  h[k] = idx ++; //h[k]æŒ‡å‘æ–°ç‚¹ } bool find(int x){ int k = (x % N + N) % N; for (int i = h[k]; i != -1; i = ne[i]){ if (e[i] == x) return true; } return false; } int main(){ bool flag = true; for (int i = 100000;; i++){ for (int j=2; j*j\u0026lt;=i; j++){ if (i % j == 0){ flag = false; break; } } if (flag == false) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; break; } } int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); memset(h, -1, sizeof h ); while(n--){ char op[2]; int x; int scanf(\u0026#34;%s%d\u0026#34;, op, \u0026amp;x); if (*op == \u0026#39;I\u0026#39;) insert(x); else{ if (find(x)) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } } return 0; }    å¼€æ”¾å¯»å€æ³•ï¼ˆä¸€èˆ¬åˆå§‹åŒ–ä¸¤å€å¤§å°çš„æ•°ç»„ï¼‰ï¼šæ’å…¥â€”â€”ä»ç¬¬kä¸ªå‘ä½å¼€å§‹ï¼Œå¦‚æœæœ‰äººï¼Œåˆ™å¾€åï¼Œé‡åˆ°æ²¡äººçš„æ’å…¥ï¼›æŸ¥æ‰¾â€”â€”ä»ç¬¬kä¸ªå‘ä½å¼€å§‹ï¼Œå¦‚æœæœ‰äººï¼Œå°±åˆ¤æ–­ï¼›å¦‚æœæ²¡äººï¼Œè¯´æ˜æŸ¥æ‰¾å¤±è´¥ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \u0026lt;iostream\u0026gt;using namespace std; const int N=200003, null = 0x3f3f3f3f; int h[N]; int find(int x){ int k = (x % N + N) % N; while(h[k] != null \u0026amp;\u0026amp; h[k] != x){ k ++; if (k == N) k = 0; } return k; } int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); memset(h, 0x3f, sizeof h); while(n--){ char op[2]; int x; scanf(\u0026#34;%s%d\u0026#34;, op, \u0026amp;x); int k = find(x); if (*op == \u0026#39;I\u0026#39;) h[k] = x; else{ if (h[k] != null) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } } }      å­—ç¬¦ä¸²å‰ç¼€å“ˆå¸Œæ³•  é¢„å¤„ç†å‡ºæ‰€æœ‰å‰ç¼€çš„å“ˆå¸Œï¼Œh[i]=str(0:i-1)çš„å“ˆå¸Œå€¼ è®²å­—ç¬¦ä¸²è¡¨ç¤ºæˆä¸€ä¸ªpè¿›åˆ¶çš„æ•° \u0026ldquo;ABDC\u0026quot;â†’(1,2,3,4)_p = (p^3+2p^2+3p+4) mod Q ä¸€èˆ¬ä¸æŠŠå­—æ¯æ˜ å°„æˆ0 $$ \\text{Låˆ°Rè¿™æ®µçš„å“ˆå¸Œå€¼: }h[R]-h[L-1]*p^{R-L+1} $$ O(1)æ—¶é—´æ±‚æŸä¸€æ®µçš„å“ˆå¸Œå€¼    C++ STL  vectorï¼Œå˜é•¿æ•°ç»„ï¼Œå€å¢çš„æ€æƒ³  size(), empty(), clear() front(), back() push_back(), a.pop_back() begin(), end()   pair\u0026lt;int, int\u0026gt;  first(), second(), æ”¯æŒæ¯”è¾ƒè¿ç®—ï¼Œä»¥firstä¸ºç¬¬ä¸€å…³é”®å­—ï¼Œsecondä¸ºç¬¬äºŒå…³é”®å­—   stringï¼Œå­—ç¬¦ä¸²ï¼Œsubstr(), c_str()  size(), length(), empty(), clear()   queueï¼Œé˜Ÿåˆ—  push(), front(), back(), pop() size(), empty()   qriority_queueï¼Œä¼˜å…ˆé˜Ÿåˆ—ï¼ˆé»˜è®¤æ˜¯å¤§æ ¹å †ï¼‰  push(), top(), pop(), clear() å®šä¹‰å°æ ¹å † priority_queue\u0026lt;int, vector, greater\u0026gt; heap;   stackï¼Œæ ˆ  push(), top(), pop() size(), empty()   dequeï¼ŒåŒç«¯é˜Ÿåˆ—ï¼ˆæ•ˆç‡å¾ˆä½ï¼Œæ¯”æ•°ç»„æ…¢ï¼‰  size(), empty(), clear(); front(), back() push_back(), pop_back() push_front(), pop_front() begin(), end()   set, map, multiset, multimapï¼ŒåŸºäºå¹³è¡¡äºŒå‰æ ‘ï¼ˆçº¢é»‘æ ‘ï¼‰åŠ¨æ€ç»´æŠ¤æœ‰åºåºåˆ—  size(), empty(), clear() set/multiset:  insert(), find(), count(), erase() â€” è¾“å…¥æ˜¯ä¸€ä¸ªæ•°x, åˆ é™¤æ‰€æœ‰x O(k+lgn)ï¼›è¾“å…¥æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œåˆ é™¤è¿™ä¸ªè¿­ä»£å™¨ lower_bounder(x) â€” è¿”å›å¤§äºç­‰äºxçš„æœ€å°çš„æ•°çš„è¿­ä»£å™¨ upper_bound(x) â€” è¿”å›å¤§äºxçš„æœ€å°çš„æ•°çš„è¿­ä»£å™¨   mapmultimap  insert() â€” æ’å…¥çš„æ˜¯pair, erase() â€” è¾“å…¥æ˜¯pairæˆ–è€…è¿­ä»£å™¨     unordered_set, unordered_map, unordered_multiset, unordered_multimapï¼Œå“ˆå¸Œè¡¨  å’Œä¸Šé¢ç±»ä¼¼ï¼Œå¢åˆ æ”¹æŸ¥æ—¶é—´å¤æ‚åº¦ä¸ºO(1) ä¸æ”¯æŒlower_bounderå’Œupper_boundï¼Œä»¥åŠè¿­ä»£å™¨çš„++/- -   bitsetï¼Œå‹ä½  å¯ä»¥çœ8å€ç©ºé—´ bitset\u0026lt;10000\u0026gt; s â€” \u0026lt;\u0026gt;é‡Œæ˜¯é•¿åº¦ ~, \u0026amp;, |, ^ \u0026laquo;, \u0026raquo; ==, ! = count() â€” è¿”å›æœ‰å¤šå°‘ä¸ª1 any() â€” åˆ¤æ–­æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ª1, none() â€” åˆ¤æ–­æ˜¯å¦å…¨ä¸º0 set() â€” æŠŠæ‰€æœ‰ä½ç½®æˆ1, set(k, v) reset() â€” æŠŠæ‰€æœ‰ä½ç½®æˆ0 flip() â€” æ‰€æœ‰ä½å–å    ","permalink":"https://michelia-zhx.github.io/posts/2021-05-12-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%843/","summary":"ç³»ç»Ÿä¸ºæŸä¸€ç¨‹åºåˆ†é…ç©ºé—´æ‰€éœ€æ—¶é—´ï¼Œä¸ç©ºé—´å¤§å°æ— å…³ï¼Œä¸ç”³è¯·æ¬¡æ•°æœ‰å…³ï¼ å“ˆå¸Œ å­˜å‚¨ç»“æ„ï¼ˆä¸€èˆ¬åªæœ‰æ·»åŠ å’ŒæŸ¥æ‰¾æ“ä½œï¼Œå¦‚æœè¦åˆ ï¼Œåœ¨ç‚¹ä¸Šæ‰“ä¸€ä¸ªæ ‡è®°å³å¯ï¼‰ æ‹‰é“¾æ³•","title":"æ•°æ®ç»“æ„(ä¸‰)"},{"content":"1 Trieæ ‘  é«˜æ•ˆå­˜å‚¨å­—ç¬¦ä¸²é›†åˆ å­˜å‚¨ï¼šæŒ‰å­—ç¬¦ä¸²å†…å®¹ï¼Œä»å·¦åˆ°å³ä¾æ¬¡è®¾ç½®ç»“ç‚¹ï¼Œå¹¶æ ‡è®°ç»“æŸä½ç½® æŸ¥æ‰¾ï¼šæŸ¥æ‰¾å­—ç¬¦ä¸²æ˜¯å¦å­˜åœ¨ \u0026amp; å‡ºç°å‡ æ¬¡  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  const int N=100010; int son[N][26], cnt[N], idx; void insert(char str[]){ int p = 0; for (int i=0; str[i]; i++){ int u = str[i] - \u0026#39;a\u0026#39;; if (!son[p][u]) son[p][u] = ++idx; p = son[p][u]; } cnt[p] ++ ; } int query(char str[]){ int p = 0; for (int i=0; str[i]; i++){ int u = str[i] - \u0026#39;a\u0026#39;; if (!son[p][u]) return 0; p = son[p][u]; } return cnt[p]; }   2 å¹¶æŸ¥é›† å¿«é€Ÿåœ°å¤„ç†ï¼ˆè¿‘ä¹O(1)ï¼‰ï¼š\n åˆå¹¶ä¸¤ä¸ªé›†åˆ è¯¢é—®ä¸¤ä¸ªå…ƒç´ æ˜¯å¦åœ¨ä¸€ä¸ªé›†åˆä¸­  åŸºæœ¬åŸç†ï¼š\n æ¯ä¸ªé›†åˆç”¨ä¸€æ£µæ ‘æ¥è¡¨ç¤ºï¼Œæ ‘æ ¹çš„ç¼–å·æ˜¯é›†åˆçš„ç¼–å·ï¼› æ¯ä¸ªç»“ç‚¹å­˜å‚¨çˆ¶ç»“ç‚¹ï¼Œpè¡¨ç¤ºå…¶çˆ¶ç»“ç‚¹  é—®é¢˜1ï¼šå¦‚ä½•åˆ¤æ–­æ ‘æ ¹ï¼šif (p[x] == x);\né—®é¢˜2ï¼šå¦‚ä½•æ‰¾åˆ°æ‰€å±é›†åˆæ ¹ç»“ç‚¹ï¼šwhile(p[x] â‰  x) x = p[x]; ï¼ˆè·¯å¾„å‹ç¼©ï¼‰\né—®é¢˜3ï¼šå¦‚ä½•åˆå¹¶ä¸¤ä¸ªé›†åˆï¼ˆpxæ˜¯xçš„é›†åˆç¼–å·ï¼Œpyæ˜¯yçš„é›†åˆç¼–å·ï¼‰ï¼šp[x] = py;\né—®é¢˜4ï¼šç»´æŠ¤é›†åˆå…ƒç´ çš„ä¸ªæ•° size()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  const int N=100010; int n, m; int p[N]; int find(int x){ if (p[x] != x) p[x] = find(p[x]); return p[x]; } int main(){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n ,\u0026amp;m); for (int i=1; i\u0026lt;= n; i++){ p[i] =i; size[p[i]] = 1; } while (m--){ char op[2]; int a, b; scanf(\u0026#34;%s\u0026#34;, \u0026amp;op); if (op[0] == \u0026#39;C\u0026#39;){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;a, \u0026amp;b); p[find(a)] = find(b); size[find(b)] += size[find(a)]; } else if (op[0] == \u0026#39;1\u0026#39;){ scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;a, \u0026amp;b); if (find(a) == find(b)) puts(\u0026#34;Yes\u0026#34;); else puts(\u0026#34;No\u0026#34;); } else{ scanf(\u0026#34;%d\u0026#34;, \u0026amp;a); printf(\u0026#34;%d\\n\u0026#34;, size[find(a)]); } } return 0; }   3 å †   å°æ ¹å †ï¼šæ¯ä¸ªç‚¹éƒ½å°äºç­‰äºå·¦å³å„¿å­\n æ’å…¥ä¸€ä¸ªæ•°ï¼ˆSTLï¼‰ 1  heap[++size] = x; up(size);    æ±‚é›†åˆå½“ä¸­æœ€å°å€¼ï¼ˆSTLï¼‰ 1  heap[1];    åˆ é™¤æœ€å°å€¼ï¼ˆSTLï¼‰ 1  heap[1] = heap[size]; size--; down(1);    åˆ é™¤ä»»æ„ä¸€ä¸ªå…ƒç´  1  heap[k] = heap[size]; size--; down(k); up(k);    ä¿®æ”¹ä»»æ„ä¸€ä¸ªå…ƒç´  1  heap[k] = x; down(k); up(k);       å †æ˜¯å®Œå…¨äºŒå‰æ ‘ï¼Œç”¨æ•°ç»„å­˜å‚¨ï¼Œ1å·ä½æ˜¯æ ¹ç»“ç‚¹\n  ä¸€ä¸ªç»“ç‚¹æ˜¯xï¼Œå·¦å­©å­æ˜¯2xï¼Œå³å­©å­æ˜¯2x+1\n  äº”ä¸ªæ“ä½œå®Œå…¨å¯ä»¥ç”¨ä»¥ä¸‹ä¸¤ä¸ªå‡½æ•°ç»„åˆèµ·æ¥æ“ä½œ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  void down(int u){ int t = u; if (u * 2 \u0026lt;= size \u0026amp;\u0026amp; h[u * 2] \u0026lt; h[u]) t = u * 2; if (u * 2 + 1 \u0026lt;= size \u0026amp;\u0026amp; h[u * 2 + 1] \u0026lt; h[u]) t = u * 2 + 1; if (u != t){ swap(h[u], h[t]); down(t); } } void up(u){ while (u / 2 \u0026gt;= 1 \u0026amp;\u0026amp; h[u / 2] \u0026gt; h[u]){ swap(h[u / 2], h[u]); u /= 2; } }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-09-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%842/","summary":"1 Trieæ ‘ é«˜æ•ˆå­˜å‚¨å­—ç¬¦ä¸²é›†åˆ å­˜å‚¨ï¼šæŒ‰å­—ç¬¦ä¸²å†…å®¹ï¼Œä»å·¦åˆ°å³ä¾æ¬¡è®¾ç½®ç»“ç‚¹ï¼Œå¹¶æ ‡è®°ç»“æŸä½ç½® æŸ¥æ‰¾ï¼šæŸ¥æ‰¾å­—ç¬¦ä¸²æ˜¯å¦å­˜åœ¨ \u0026amp; å‡ºç°å‡ æ¬¡ 1 2 3 4 5 6 7 8 9 10","title":"æ•°æ®ç»“æ„(äºŒ)"},{"content":"çŸ­è§†çš„è¡Œä¸º\nåŒºé—´é€‰ç‚¹ ç»™Nä¸ªé—­åŒºé—´, è¦åœ¨æ•°è½´ä¸Šé€‰å°½é‡å°‘çš„ç‚¹, ä½¿æ¯ä¸ªåŒºé—´è‡³å°‘åŒ…å«ä¸€ä¸ªé€‰å‡ºçš„ç‚¹ (answer â‰¤ count)\nåŒºé—´è´ªå¿ƒé—®é¢˜, è¦ä¹ˆæŒ‰å·¦ç«¯ç‚¹æ’åº, è¦ä¹ˆæŒ‰å³ç«¯ç‚¹, è¦ä¹ˆåŒå…³é”®å­—\n æ¯ä¸ªåŒºé—´æŒ‰å³ç«¯ç‚¹ä»å°åˆ°å¤§æ’åº ä»å‰å¾€åä¾æ¬¡æšä¸¾æ¯ä¸ªåŒºé—´   å¦‚æœå·²ç»åŒ…å«ç‚¹, pass å¦åˆ™é€‰å³ç«¯ç‚¹ (answer â‰¥ count) =\u0026gt; answer = count  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return r \u0026lt; W.r; } }range[N]; int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); int res = 0, ed = -2e9; for (int i=0; i \u0026lt; n; i++){ if (range[i].l \u0026gt; ed){ res ++; ed = range[i].r; } } printf(\u0026#34;%d\u0026#34;, res); return 0; }   æ’è¯¾ (ä¸å†²çªæ’å°½é‡å¤šçš„è¯¾) å’Œä¸Šé¢˜ä¸€æ ·, ä¸Šé¢˜é€‰ç‚¹çš„ä¸ªæ•°(æœ‰ç‚¹çš„åŒºé—´ç›¸äº’æ²¡æœ‰ç›¸äº¤) = è¿™é¢˜çš„è¯¾ç¨‹æ•°\nåŒºé—´åˆ†ç»„ åŒºé—´åˆ†ç»„, æ¯ç»„å†…çš„åŒºé—´ä¸¤ä¸¤æ²¡æœ‰äº¤é›†, è¦æ±‚ç»„æ•°å°½é‡å°. (åˆæ³•çš„ â†’ answer â‰¤ count)\n æ¯ä¸ªåŒºé—´æŒ‰å·¦ç«¯ç‚¹ä»å°åˆ°å¤§æ’åº ä»å‰å¾€åä¾æ¬¡æšä¸¾æ¯ä¸ªåŒºé—´   åˆ¤æ–­èƒ½å¦æ”¾å…¥ç°æœ‰çš„ç»„ä¸­ L[i] \u0026gt; max_r  ä¸èƒ½, æ–°å»ºä¸€ä¸ªç»„ å­˜åœ¨, éšä¾¿æŒ‘ä¸€ä¸ª, å°†å…¶æ”¾å…¥, æ›´æ–°å½“å‰ç»„çš„max_r æœ€ååˆ†å‡ºçš„countä¸ªç»„, æ¯ä¸ªéƒ½æ˜¯ç›¸äº¤çš„, å› æ­¤å¿…é¡»åˆ†å¼€ â†’ answer â‰¥ count =\u0026gt; answer = count    åŒºé—´max_rå¯ä»¥ç”¨å°æ ¹å †å­˜å‚¨  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;queue\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return r \u0026lt; W.r; } }range[N]; int main(){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;int\u0026gt; \u0026gt; heap; for (int i=0; i\u0026lt;n; i++){ auto r = range[i]; if (heap.empty() || heap.top() \u0026gt;= r.l) heap.push(r.r); else{ int t = heap.top(); heap.pop(); heap.push(r.r); } } printf(\u0026#34;%d\u0026#34;, heap.size()); return 0; }   åŒºé—´è¦†ç›– ç»™å®šä¸€äº›å°åŒºé—´å’Œä¸€ä¸ªå¤§åŒºé—´[start, end], è¦æ±‚ç”¨å°½é‡å°‘çš„å°åŒºé—´å»å®Œå…¨è¦†ç›–å¤§åŒºé—´.\n å°†æ‰€æœ‰åŒºé—´æŒ‰å·¦ç«¯ç‚¹ä»å°åˆ°å¤§æ’åº ä»å‰å¾€åä¾æ¬¡æšä¸¾æ¯ä¸ªåŒºé—´, åœ¨æ‰€æœ‰èƒ½è¦†ç›–startçš„åŒºé—´ä¸­, é€‰æ‹©å³ç«¯ç‚¹æœ€å¤§çš„é‚£ä¸€ä¸ª, å°†startæ›´æ–°æˆè¿™ä¸ªå³ç«¯ç‚¹  åˆæ³•çš„ â†’ answer â‰¤ count. åè¯æ³• â†’ answer â‰¥ count. å‡è®¾answer \u0026lt; count, æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸ä¸€æ ·çš„, æ›¿æ¢æ‰, ä¿æŒä¸ªæ•°ä¸å˜ä¸”ä¾ç„¶åˆæ³•. answer = count.    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;queue\u0026gt; using namespace std; const int N=100010; int n; struct Range{ int l, r; bool operator\u0026lt; (const Range \u0026amp;W)const{ return l \u0026lt; W.l; } }range[N]; int main(){ int st, ed; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;st, \u0026amp;ed); scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i=0; i \u0026lt; n; i++){ int l, r; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;l, \u0026amp;r); range[i] = {l, r}; } sort(range, range+n); int res = 0; bool success = false; for (int i=0; i \u0026lt; n; i++){ int j=i, r=-2e9; while (j\u0026lt;n \u0026amp;\u0026amp; range[j].l \u0026lt;= st){ r = max(r, range[j].r); j++; } if (r \u0026lt; st){ res = -1; break; } res ++; if (r \u0026gt;= ed) { success = true; break; } st = r; i = j-1; } if (!success) res = -1; printf(\u0026#34;%d\u0026#34;, res); return 0; }   å“ˆå¤«æ›¼æ ‘  åˆå¹¶æœå­ - æ¶ˆè€—ä½“åŠ›æœ€å°‘  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;queue\u0026gt; using namespace std; int main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;int\u0026gt;\u0026gt; heap; while(n--){ int x; scanf(\u0026#34;%d\u0026#34;, \u0026amp;x); heap.push(x); } int res = 0; while (heap.size() \u0026gt; 1){ int a = heap.top(); heap.pop(); int b = heap.top(); heap.pop(); res += a + b; heap.push(a+b); } printf(\u0026#34;%d\u0026#34;, res); return 0; }   ","permalink":"https://michelia-zhx.github.io/posts/2021-05-27-%E8%B4%AA%E5%BF%83/","summary":"çŸ­è§†çš„è¡Œä¸º åŒºé—´é€‰ç‚¹ ç»™Nä¸ªé—­åŒºé—´, è¦åœ¨æ•°è½´ä¸Šé€‰å°½é‡å°‘çš„ç‚¹, ä½¿æ¯ä¸ªåŒºé—´è‡³å°‘åŒ…å«ä¸€ä¸ªé€‰å‡ºçš„ç‚¹ (answer â‰¤ count) åŒºé—´è´ªå¿ƒé—®é¢˜, è¦ä¹ˆæŒ‰å·¦ç«¯ç‚¹æ’åº, è¦ä¹ˆæŒ‰å³ç«¯ç‚¹, è¦","title":"è´ªå¿ƒ"}]