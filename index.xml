<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Michelia&#39;Log</title>
    <link>https://michelia-zhx.github.io/</link>
    <description>Recent content on Michelia&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="https://michelia-zhx.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>https://michelia-zhx.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/about/</guid>
      <description>about</description>
    </item>
    
    
    <item>
      <title>Bias-Variance Decomposition</title>
      <link>https://michelia-zhx.github.io/posts/2022-03-03-bias-variance-decomposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-03-03-bias-variance-decomposition/</guid>
      <description>Bias–variance tradeoff The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: - The</description>
    </item>
    
    <item>
      <title>Hugo 博客搭建 | PaperMod 主题</title>
      <link>https://michelia-zhx.github.io/posts/2022-03-01-%E5%BB%BA%E7%AB%99/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-03-01-%E5%BB%BA%E7%AB%99/</guid>
      <description>1. 安装 Hugo 苹果用户有安装 HomeBrew 工具的话可以直接输入 brew install hugo 进行安装. 下载完成后检查是否安装成功，输入: hugo version, 若出现版本信息则表示安装成功. 2. 新建站点 输</description>
    </item>
    
    <item>
      <title>Notes - Computer Networking -- Chap 1</title>
      <link>https://michelia-zhx.github.io/posts/2022-02-26-computer_network_chap1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-02-26-computer_network_chap1/</guid>
      <description>Chap 1 计算机网络和因特网 1.1 什么是因特网 1.1.1 因特网的具体构成描述 各种各样的设备都被连接到因特网,这些设备都被称为主机或端系统. 端系统通过通信链路和</description>
    </item>
    
    <item>
      <title>Notes - Computer Networking -- Chap 2</title>
      <link>https://michelia-zhx.github.io/posts/2022-03-08-computer_network_chep2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-03-08-computer_network_chep2/</guid>
      <description>Chap 2 应用层 2.1 应用层原理 网络应用的体系结构: 客户-服务器 (C/S) 模式 (可扩展性差, 性能随用户数量增加会有断崖式下降) 对等体 (P2P) 体系结构 混合体 进程通信:</description>
    </item>
    
    <item>
      <title>Notes - Computer Networking -- Chap 3</title>
      <link>https://michelia-zhx.github.io/posts/2022-03-28-computer_network_chep3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-03-28-computer_network_chep3/</guid>
      <description>Chap 3 传输层 3.1 概述和传输层服务 3.2 多路复用与解复用 3.3 无连接传输: UDP 3.4 可靠数据传输的原理 3.5 面向连接的传输: TCP 3.6 拥塞控制原理 3.7 TCP 拥塞机制</description>
    </item>
    
    <item>
      <title>Paper Notes - Active Learning</title>
      <link>https://michelia-zhx.github.io/posts/2021-08-31-active_learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-08-31-active_learning/</guid>
      <description>Active Learning 也称为查询学习或者最优实验设计. 主动学习通过设计合理的查询函数, 不断从未标注的数据中挑选出数据标注后放入训练集. 有效的主动学习数据选择策</description>
    </item>
    
    <item>
      <title>Paper Notes - Attention</title>
      <link>https://michelia-zhx.github.io/posts/2022-04-02-attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-04-02-attention/</guid>
      <description>1. Attention 机制 1a. 背景知识 我们最为熟悉的NMT模型便是经典的Seq2Seq, 这篇文章从一个Seq2Seq模型开始介绍, 然后进一步看如何将Attent</description>
    </item>
    
    <item>
      <title>Paper Notes - Multi-Teacher Knowledge Distillation - 1</title>
      <link>https://michelia-zhx.github.io/posts/2022-02-23-multi_teacher_knowledge_distillation-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-02-23-multi_teacher_knowledge_distillation-1/</guid>
      <description>Learning from Multiple Teacher Networks http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf loss: teachers的softmax输出取平均和student的交叉熵 中间层表示的相对相异度(仅适用于MTKD), 三元组$(q_i</description>
    </item>
    
    <item>
      <title>Paper Notes - Multi-Teacher Knowledge Distillation - 2</title>
      <link>https://michelia-zhx.github.io/posts/2022-02-24-multi_teacher_knowledge_distillation-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-02-24-multi_teacher_knowledge_distillation-2/</guid>
      <description>Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks https://arxiv.org/pdf/2004.05937.pdf Learning from Multiple Teacher Networks, KDD 2017 Efficient knowledge distillation from an ensemble of teachers. Interspeech 2017: 对teacher的logits取加权平均, 加权平均和student的logit</description>
    </item>
    
    <item>
      <title>Paper Notes - Object Detection</title>
      <link>https://michelia-zhx.github.io/posts/2021-09-12-object_detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-09-12-object_detection/</guid>
      <description>Definition Image Classification: 输入图片, 输出图中目标物体的类别. Object Localization: 输入图片, 输出图中物体的 bounding box. Object Detection: 输入图片, 输出图中物体的 bounding box 和类别. R-CNN Model Family 采用region proposal methods, 首</description>
    </item>
    
    <item>
      <title>Paper Notes - Self-Attention</title>
      <link>https://michelia-zhx.github.io/posts/2022-04-03-self_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-04-03-self_attention/</guid>
      <description>Self-Attention 机制和代码 BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, SciBERT, BioBERT, MobileBERT, TinyBERT 和 CamemBERT 的共同点是 self-attention 机制. Self-attention 机制不仅是使某种架构被称为&amp;quot;BERT&amp;quot;的原因, 更准确地, 是基于</description>
    </item>
    
    <item>
      <title>Paper Notes - Self-Supervised Learning</title>
      <link>https://michelia-zhx.github.io/posts/2021-09-05-self_supervised_learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-09-05-self_supervised_learning/</guid>
      <description>Why self-supervised learning: 主要的问题在于获取数据及其标注部分. Definition: Self-supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one: &amp;ldquo;Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?&amp;rdquo; Replace</description>
    </item>
    
    <item>
      <title>Paper Notes - Vision Transformer</title>
      <link>https://michelia-zhx.github.io/posts/2022-04-04-vit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2022-04-04-vit/</guid>
      <description>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale 在整体的实现上, 原文完全使用原始bert的transformer结构, 主要是对图片转换成类似token的处理, 原文引</description>
    </item>
    
    <item>
      <title>Paper Notes - Week 1</title>
      <link>https://michelia-zhx.github.io/posts/2021-07-15-pros_paper_notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-07-15-pros_paper_notes/</guid>
      <description>Mining Typhoon Knowledge with Neural Networks &amp;ndash; Zhi-Hua Zhou, Shi-Fu Chen, Zhao-Qian Chen - 1999 需解决的问题: 神经网络的两个缺点 &amp;ndash; 数据量大, 训练时间长; 神经网络对知识的学习果不能直接用于决策. Fast neural model - FTART (Firld Theory</description>
    </item>
    
    <item>
      <title>Paper Notes - Week 2</title>
      <link>https://michelia-zhx.github.io/posts/2021-07-22-pros_paper_notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-07-22-pros_paper_notes/</guid>
      <description>Multi-Instance Multi-Label Learning with Application to Scene Classification &amp;ndash; Zhi-Hua Zhou, Min-Ling Zhang, NIPS 2006 Multi-instance: 一个example包含多个instance, example只对应1个label; Multi-label: 一个example对应多个</description>
    </item>
    
    
    <item>
      <title>基础算法(一)</title>
      <link>https://michelia-zhx.github.io/posts/2021-05-01-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%951/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-05-01-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%951/</guid>
      <description>1 快排 1.1 快排 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include &amp;lt;iostream&amp;gt;using namespace std; const int N = 1000010; int a[N]; void quicksort(int a[], int l, int r){ if (l &amp;gt;= r) return; int i = l-1, j = r+1, x = a[(l+r)&amp;gt;&amp;gt;1];</description>
    </item>
    
    <item>
      <title>基础算法(二)</title>
      <link>https://michelia-zhx.github.io/posts/2021-05-03-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%952/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-05-03-%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%952/</guid>
      <description>My first blog</description>
    </item>
    
    <item>
      <title>数据结构(一)</title>
      <link>https://michelia-zhx.github.io/posts/2021-05-07-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-05-07-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%841/</guid>
      <description>My first blog</description>
    </item>
    
    <item>
      <title>数据结构(三)</title>
      <link>https://michelia-zhx.github.io/posts/2021-05-12-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%843/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-05-12-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%843/</guid>
      <description>系统为某一程序分配空间所需时间，与空间大小无关，与申请次数有关！ 哈希 存储结构（一般只有添加和查找操作，如果要删，在点上打一个标记即可） 拉链法</description>
    </item>
    
    <item>
      <title>数据结构(二)</title>
      <link>https://michelia-zhx.github.io/posts/2021-05-09-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%842/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-05-09-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%842/</guid>
      <description>1 Trie树 高效存储字符串集合 存储：按字符串内容，从左到右依次设置结点，并标记结束位置 查找：查找字符串是否存在 &amp;amp; 出现几次 1 2 3 4 5 6 7 8 9 10</description>
    </item>
    
    <item>
      <title>贪心</title>
      <link>https://michelia-zhx.github.io/posts/2021-05-27-%E8%B4%AA%E5%BF%83/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://michelia-zhx.github.io/posts/2021-05-27-%E8%B4%AA%E5%BF%83/</guid>
      <description>短视的行为 区间选点 给N个闭区间, 要在数轴上选尽量少的点, 使每个区间至少包含一个选出的点 (answer ≤ count) 区间贪心问题, 要么按左端点排序, 要么按右端点, 要</description>
    </item>
    
  </channel>
</rss>
